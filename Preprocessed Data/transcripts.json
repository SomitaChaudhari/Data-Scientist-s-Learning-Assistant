{
    "Machine Learning & Data Science Project - 4 _ Outlier Removal (Real Estate Price Prediction Project).wav": " In this video we are going to do outlier detection and removal. Outliers are the data points which are data errors or sometimes they are not data errors but they just represent the extreme variation in your data set. So although they are valid, it makes sense to remove them otherwise they can create some issues later on. We can apply now different techniques to detect the outliers and remove them. And these techniques are you either use standard deviation for example or you can use simple domain knowledge. One of the things in real estate domain is that when you have a two bedroom apartment, it cannot be 500 square feet in total area. Usually look at all these guys, here is 1056 square foot but it is 2 BHK. So if you just divide this by 2 BHK you get like 500 for example. Here also 2600 you divide by 4 and you get around 600 to 700. So we want to first look at any data rows where the square foot per bedroom is less than some threshold. So as a data scientist now you go to your business manager who is a real estate expert and you ask him this question. What is a typical square foot per bedroom? And he will tell you that it is around 300. So if you have any case, for example you have let's say, let's say you have 600 square foot home. And the total number of bedrooms are 6. Then it means it's 100. This seems very unusual. He tells you that 300 square foot per bedroom is a typical threshold. So using that criteria now you will examine your data set and try to find out on the properties where this threshold is not met. And the way you do that is you will divide a total square foot by BHK and you will see if it is less than 300 show me the value. So you will print this data frame these data points and you will go to your business manager and show it to him. He will look at this and he will say, hmm 1000 square foot home and 6 bedroom hall kitchen that is unusual. So he will ask you to remove that. Similarly you have 600 square foot home but the total number of beds are 8. So these are like clearly data errors or anomalies or outliers, whatever you want to call it and you can safely remove them. So we are going to remove all these data points. So first let's see how many rows we have in our data frame. So we have around 13,000 and now I am going to remove this. So the way you remove it is you create a new data frame called DF6. And here I want to negate this. So if you want to filter all these rows you do negate on your criteria. And then DF6.shape when you run it you see you remove some outliers. Now your number of rows are 1200 502. So this is one of the ways of removing outliers. Now we can have even more outliers. For example, let's check price per square foot. So I am going to check price per square foot. I want to look at all the properties where price per square foot is either very, very high or very, very low. And first I will do is you can say df6.price.squarefoot.describe. Describe method gives you some basic statistics on that particular column. When you run it you find that the minimum value is 267 rupees per square foot. Although in Bangalore to get a property with 267 rupees per square foot is very, very low. It's very unlikely. And similarly you have this property where the price per square foot is extremely high. Now this is possible if the property is in a very prime area. But as we are going to build a generic model it makes sense to remove this kind of extreme cases. Okay. So we are going to write a function that can remove these extreme cases based on standard deviation. Now if you know about mean and standard deviation the if your data set has a normal distribution which we are assuming that our dataset should have a normal distribution. Then most of the data points around I think 68% data points should lie between mean and one standard deviation. Okay. So we are going to filter out anything which is beyond one standard deviation. Okay. Now if you don't know the statistical theory about standard deviation then you can pause the video and just follow some basic tutorials on what standard deviation is etc. It's a pretty simple concept. Okay. So I will write now a function which can remove price per square foot outliers per location. Now you have to do this per location because some locations will have high price some location will have less price. So what you want to do is per location you want to find mean and standard deviation and then filter out any data points which are beyond one standard deviation. Okay. And that function will look something like this. Here what I'm doing is I'm taking data frame as an input and I am grouping them by location first. And for location I get this sub data frame for which I am calculating mean M is mean and ST standard deviation and then I'm filtering all those data points which are beyond this standard deviation which means anything above mean minus one standard deviation and anything below mean plus 10 one standard deviation. I will keep it in my reduced data frame and then I will keep on appending those data frames per location and this will give me the output data frame. Okay. So let's call this function on my DF7. DF6 so when I call it on DF6 I get DF7 as an output. And when I run it what we did is we removed price per square foot outlier from DF6 and now we have a new data frame DF7 which has 10,000 data points. Okay. The previous one had 12,502 so we removed close to 2000 outliers from our data set. Now one more thing we want to check in our data set is whether the property prices for three bedroom apartments are more than the property price of two bedroom apartment or not for the same square foot area. For example here we are looking at two properties same square foot around like 1200 square foot but you see that the three bedroom price is 81 lakh whereas the two bedroom price is 1 crore 27 lakh. So although the square foot is same the less number of bedrooms you have the property price is higher. Now this could be due to many reasons we don't know right it could be because of the properties in some location where there are spatial amenities or anything else right. We don't know what the reason is but this is the behavior we see in our data set and if you want to do some visualization of how many such cases we have okay for which I am going to write a function which will use a scatter plot to give me this visualization okay. And that function I have already created I am just going to copy paste here so just let me walk you through this function what this function is doing is it is drawing a scatter plot on which it will plot two bedroom and three bedroom apartment okay let me just run this function okay and then we'll go over this function so this function takes location and a data frame as input. So I have the F7 and I'm going to run this function for Rajaji Nagar location and when I do that what I find is this scatter plot so here blue points are two bedroom apartments and green markers are three bedroom hall kitchen properties. This X axis has total square foot area and the Y axis has price per square feet okay I can have price per square foot or maybe price let me do price. When I plot this what I find is I have in a same so if you look at a vertical line for same square foot area for example look at this particular line it around 1700 square foot area the two bedroom apartment prices are higher than three bedroom see I have these four data points where these are like two bedroom apartments whose values are higher than three bedrooms okay here also you see some cases so I want to remove some of these outliers okay and okay let's go over this function so this function creates two different data frames where for same location you will have data points for two bedroom and three bedroom apartments and then we'll plot a scatter plot using these two lines okay and that's it all right now you can run the same function for a different location okay I'm going to run it in heaven location as well and there also I find many cases see here like all these blue data points have higher value than green so which means two bedroom prices higher than three okay so I want to do some data clean up in this area for which I have again return of function for removing the outliers so this function what it will do is it will create a poor bedroom homes it will create some statistics okay so the kind of statistics it will create is I will create a dictionary like this where this is one bedroom apartment homes okay for which I will calculate a mean standard deviation and count and then what I will do is I will filter out all the two bedroom apartments whose mean value like whose value not mean value whose value is less than let's say mean because you would think that your two bedroom apartment home for the same square foot area should have a little higher value than one bedroom okay so essentially that's what I'm trying to do in this particular function so here first I'm doing location group I going through every location data frame and for every location data frame I'm again creating new data frames based on BHK and per BHK data frame I'm computing means standard deviation and count okay and once that for loop is over I'm running the same for loop again and try trying to exclude those data points which lies beyond whose value of price per square foot area of price per square foot is less than the mean of the previous BHK for example for two bedroom apartment I will try to filter all those property values whose price per square foot is less than the mean of this one bedroom mean okay so we can do this and we can create DF8 where I will call this remove function and then say DF8.ship so when I run it it's gonna take some time but see it removed so many data points and now I have 7,329 left after this is done I can again plot the same scatter plot to see what kind of improvement it has done and when I do that I will have to supply DF8 here when I do that you will notice that see all this green data points that I had in this area those data points are gone now so majority of the data points for three bedroom apartment this is green is three has higher value than two I mean I still have some data points here green it is very kind of difficult to exactly remove those data points so this kind of abnormalities are going to be present and it is I think it is fine to have such abnormalities once this outliers are removed now I want to plot a histogram and I want to see how many apartments or how many properties I have in poor per square foot area okay and I run this code what I find is majority of the property so here on the x axis I have price per square foot the y axis is a histogram so it shows number of data points in that category so like from zero to 10,000 rupees per square foot range I have majority of my data points you can see this is normal distribution kind of like a Gaussian curve right it is like a bell co here and I see that my data set has normal distribution it looks good at this point now let us explore the bathroom feature okay so we have this feature called bathrooms and I want to see the unique values here and you can see that there are properties which has even like 13 bathrooms okay and I am just naturally curious like in Bangalore you have these homes which with greater than like 10 bathrooms and some of these are kind of okay so see if you have 12,000 square foot home which is pretty big then having 10 bedroom then having 12 bathroom is okay all right so then I again go to my business manager and I ask him is there criteria you have to remove the bathroom specific outliers basically he will tell you that anytime you have number of bathrooms which are greater than number of bedroom hall kitchen usually when you have two bedroom apartment you have two bathrooms you can sometimes you can have even three but you can it is very unusual to have like four so let's say you have two bedroom apartment and you have four bathroom that is kind of unusual right so then after discussion with your business manager you decide that anytime you have number of bathrooms greater than your bedrooms plus two you are going to remove those as an outlier okay so before we do that let's plot a histogram here again so histogram you can plot by calling PLT dot HIST and R width is the width of the bar okay so when you run it you see the most of the properties are in two properties are having two bedrooms and then fours etc and there are few outliers you know you see here which has more number of bathrooms okay so here now let's go back to our original criteria that our business manager told us which is any time you have bathroom greater than number of bedrooms plus two you can mark them as an outlier so after I come back from a meeting with business manager I will run this criteria on my data set and see what's going on so here I see that I have this four bedroom property having seven bathrooms another one is three bedroom property having six bathrooms so all of these are outliers which I can safely remove so I'm going to remove them by running this and now I get a new data frame I still have around 7000 data points in that now my data frame looks pretty much neat and clean so I can now start preparing it for machine learning training and for that I have to drop some unnecessary feature so the price per square foot and the size feature at this point are unnecessary because for size we already have BHK feature so this one can be dropped price per square foot can also be dropped because this we used just for outlier protection right it did not have any other use so I will create a new data frame where I will drop these two features and my new data frame looks something like this all right that's all I had for this tutorial in the next tutorial we are going to create a machine learning model and we'll train it will use grid search CV to come up with the best model",
    "Outlier detection and removal using percentile _ Feature engineering tutorial python # 2.wav": " Outliers are unusual data points which are very different from rest of your observation. For example, you are analyzing a data set which has people's age in it. Now you might see up to 90 or 100 years of age but if you see a data point that has 1000 years then that's an outlier that clearly indicates an error in data collection. These outliers can happen just because there is a nature of variation in your data set. For example, you see a data point with 120 years age that couldn't be an error. Maybe it's a legitimate valid data point but since it is very different from rest of the data points, it can skew the statistical power of the data analysis process. For that reason often if not all the time, it makes sense that you detect outliers and remove that. Now, there are many different ways of detecting and removing outliers. There are statistical techniques such as percentile, C score, standard deviation. You can also use visualization using box plot or scatter plot to detect the outliers. In this particular tutorial, we are going to look at percentile way of detecting and removing the outliers. We will write simple Python code on a simple data set initially. Then we will look into some complex data set and we will remove outliers from that using percentile. In the end, we will have an interesting exercise for you to work on. This tutorial series is going to be awesome because I will be producing different videos for each of the outlier detection techniques. Let's get started. Let's first understand what exactly is percentile. If you know percentile, then you can skip this section. I have the timeline of this video in the video description used so you can easily skip to the next section. But you might have noticed that in some of the test score techniques, they use a relative scores. Here in this Excel file, I have the test score of different people out of 100. Now if you use your usual percent score, then this will be your percent score because these are the numbers from out of 100. So the percentage is same as that. But sometimes people use a relative scoring technique where 69 is the highest score. Hence they will say, okay, this person achieved 100%. And 27 is the lowest score. Hence we will say this person achieved 0. So basically he is at the bottom and this guy is at the top. Now the definition of percentile is, this is a percentile rank by the way. So here it is 50%, which means that 50% of the samples are below value 56. So let's count it. So there are 4 samples 1, 2, 3 and 4. So 4 are 4 out of 8. So if you don't count this data sample, then there are total 8 without, after excluding 56, 8 samples out of 8, 4 are below 56, which means this is 50% percentile. This is 100% percentile because all the data points are below 69. So that's some basics on percentile rank. Now in this tutorial, we are going to examine a person's height data set. So just assume that there is an app or clothing company who wants to perform data analysis on people's height so that they can design the clothes of relevant land accordingly. Again, there are some dummy people's name and then I have listed all the heights here. The data set is very simple so by visual examination also you can spot outlier easily. But the idea is in real life your data set will be much more bigger and you need to use statistical techniques. So I'm going to load this data in my Jupyter notebook. So I have loaded it here in my pandas data frame. And then I will use a percentile feature of pandas data frame. So you all know that if you want to access the height column, then you can access the height column by doing this. And that will return you the numpy array on that you can call quantile. So quantile will give you the percentile value. And if you want the data samples which are about 95 percent quantile, then you will get this value. Now what this means is 9.68 is 95 percent quantile. Anything above this is something we can consider as an outlier. Okay now what do you want to set your threshold to? It really depends on situation. So there is no like fixed skyline, but here I'm just using 95 percent. So let me store this in a variable called maximum threshold. And the maximum threshold value is this. Now in your data frame. So this is how you identify the outliers. So see here the person's height is 14 feet. That cannot be true. You know like it's hard to find a person whose height is 14 feet. So we just detected an outlier. We can also detect the outlier on the minimum and by doing this. So we can say my minimum threshold is quantile 0.05. So give me anything which is less than 5 percent. So then I get this value. And when you do less than that minimum threshold. You will also show you some outliers. See here your set height is 1.2. Assume that this is the data set for adults. 1.2 feet height seems to be really less and it's most likely a data error and outlier which we can remove easily. Now if you have a domain knowledge or you can actually use your domain knowledge. For example for people's height we know that the max height could be around 7 feet or maybe maybe 7.5 feet. So even if I don't want to do quantile I can directly say okay if the height is greater than 7.5 then that's an outlier. But unfortunately when we deal with data sets in real life we don't have that domain knowledge and features are very very complex. So it becomes very hard to come up with a fixed threshold and at that time using quantile can be very useful because what you're doing is you are removing the samples on the far ends on the left far as well as the right far. So for that reason quantile is one of the techniques that you can use. Now here in our example if you want to remove these outliers what you can do is this. In your data frame you can say if the height is less than max threshold and if the height is greater than min threshold then only keep this example. So you will get all these examples and you see that there is no yourself with 1.2 height here and also we removed this particular sample which had 14.5 feet height. Now let's look at little complex data set. I have a data set of Bangalore property prices which I got from Kegel. I have pre-processed it a little bit and I'm going to load this CSV file into my data frame. Now you can see that this has around 13,000 rows. And here these are the some of the very basic features for property prices. I have loaded them in my notebook here and this is how it is looking. So now let's start analyzing this data. So first thing I will do is I will just confirm how many rows and columns so 13,200 rows, seven columns. You can also use a describe function just to get a quick feeling on your data set. So here this 25% 50% values that you are seeing are percentile. What this means is quickly is for example for total square feet column. 75% of samples have total square feet less than this value. This is the mean value. This is the max value. Okay. Similarly price post square foot. Let's look at that. Here 75% of your data sample have value less than 7,317 rupees per square feet. Now if you're looking in a Bangalore you will get a feel that okay this is probably about right. But then look at the maximum value. The maximum value is 1.2 and e raise to 7. So this value is really high. Now this could be either a data error or it could be a legitimate property but these type of outliers will really hurt the performance of our model. So we need to tackle them. Okay. So let's first find out the min and max threshold by using the quantile. So here you can also supply an array in your quantile function and it will return you min and max threshold. Okay. And this I'm doing on price post square feet. I want to basically remove outlier based on price post square feet feature that I have in my data set. My minimum threshold is 13,6600 rupees post square feet maximum is 59,959 rupees post square feet. Now if you're living again in Bangalore you would know that. If you're getting a home which has a value price post square feet value of less than 1366 is most likely not true. You know it cannot be true. Bangalore is pretty expensive. So let's see what data points are how very less value. So you can do something like this where in your data fam you can say my price post square foot is less than minimal threshold and give me all those data points. So I got all these data points. Here the price post square feet is ridiculously low. In Bangalore I mean come on you can't get a home for 371 rupees post square feet. So these are most likely errors in our data set. Look at this guy. 9 bedroom hall kitchen apartment. 40,000 square feet area and you are getting this is in lack you are getting in one crore 75 legs. This cannot be true. So these are clearly outliers. Now I used 1% and 99.9% you can use different ones. So based on the situation based on your intuition you can use different thresholds. Also if you have a domain knowledge let's say you are working for mccan.com or some property or real estate website and if you have a talented business manager who comes to you and directly says you know what I know based on my knowledge of real estate in the city that the price post square foot should be restricted in these two range. Then it's fine then you don't need to use quantile. But as I mentioned before many times you don't have that domain knowledge many times features are complex. That's when quantile can be really useful. Okay now let's look at our data points which has okay price post square foot price post square foot is greater than max threshold. Okay now we are looking at the data sample on the higher end. Here check this property the price post square foot is ridiculously high. All these are like very very high prices. Now I'm not saying these are data errors maybe some of them are valid but if you keep these in your model while you're building model if you keep these data points they are really going to hurt the performance. So to remove these outliers what you can do is you can create a new data frame. All right how do you create a new data frame like this. So here what I did is remove anything whose price post square foot is less than max threshold. Yeah so basically keep anything that is that whose price is less than max threshold and greater than min threshold. So this will automatically remove your outliers and now my new data frame which is DF2 has 13172 rows. Initially it was 13200 so we removed few outliers here and if you do DF2 dot sample by the way you can use sample just randomly sample some rows from your data frame and if you print 10 random rows they show these values and you see that these values look pretty decent. There's all I had for this tutorial now the most interesting part of this tutorial is an exercise. You have to use Airbnb New York data set from Kegel so if you right click on this link you will find this data set here on Kegel and you can click on this button to download the data set. If you don't want to do that then you can actually go to my GitHub and on my GitHub under this outlier there is an exercise folder in that exercise folder you'll find this CSV file. Now this Python not book is actually a solution for the problem so don't look at the solution until you have tried it out yourself. So what you have to do is you have to analyze this data set and remove the outliers based on the price per night. The Airbnb data set shows various properties and hotels and there is a price per night per apartment or whatever and you have to use your intuition to determine your percentile range and then remove it. Then you can verify your solution with my solution. Now I used different type of percentile range so your solution doesn't need to accidentally match with mine but this is a very simple exercise by the way you'll be doing pretty much the same thing as what I did in this tutorial. So go try it out once you try it out you will your understanding from whatever you have learned in this tutorial will become more solid. I'm going to provide a link of this notebook in the video description so go check it out and towards the end of the notebook there is an exercise description. Now many times people ask me how can we download the CSV file etc. So what you can do is you can go to the root directory which is pi and you can click on clone or download button and then you can go to ML directory under ML I have different ML tutorials and here is a location where I'm going to host all my feature engineering code. Thank you very much for watching. Bye.",
    "Machine Learning Tutorial Python - 14_ Naive Bayes Classifier Algorithm Part 1.wav": " In this two part tutorial we are going to learn about naive base. In the first part we will cover some theory and then predict Titanic survival rate using naive base. In the second one we will build email spam detector. And here is the list of topics along with the timelines. If you want to skip to a specific topic you can do that. Let's start with some basic probability theory. You all know that when you flip a coin the probability of getting head or tail is 1 by 2 because there are two possible outcomes and the chance of getting head or tail is 50%. Similarly when you pick a random card the probability of that being a queen is how much? Just think about it. It's very simple. There are total 4 queens, 52 cards. Hence the probability of getting queen is 4 by 52 which is 1 by 13. If you know that the card that you have picked is a diamond then what's the probability of getting a queen? Well this is called a conditional probability where you know that even A is occurred and then now you are trying to predict the probability of B. So total diamonds are 13, queen is 1. We all know by just simple intuition that the probability here is 1 by 13 but this is how the conditional probability is represented where you say P of queen slash diamond where you know that even diamond has already occurred and the probability of getting a queen is something you are calculating. So that's called a conditional probability and the way that is represented is P of A slash B where even B has occurred already which is that you know that the card is a diamond and you are trying to find the probability of event A which is whether the card is queen or not. Thomas Bayes gave this famous equation of finding a conditional probability where you can find probability of A given the event B has occurred by knowing some parameters which is the individual probabilities of A and B and knowing the probability of B given that A has occurred. So let's look at it in the context of our queen and diamond problem. So here this is the same equation I have represented for our specific use case and you know this individual probabilities and if you put them into our equation you can easily find this conditional probability. Now this is a very powerful theorem where you know probability of certain events but you don't know the probability of some other events and using those certain events you can find other probabilities. You all know about Titanic crash there was a movie that was made and it was a super hit but that event actually occurred and it was said that so many people died. We have a data set of this Titanic crash where there are name of the people along with certain features which is your fair ticket cabin etc. based on that we are trying to find out the survival rate and here we can use base theorem where we are trying to find the probability of survival based on the features such as if the person was male the class, age, cabin, fair and so on. The reason is called naive base is because we make a naive assumption that the features which is male, class, age, cabin and so on are independent of each other. In reality some of these features might be dependent such as the fair is fair and cabin are kind of related but we assume for simplicity purpose that these are not related hence it is called naive based and it is a simple assumption which reduces our calculation and makes the algorithm simple yet effective. If you want to go in math a little bit you can watch this video. This video is by Luis Serrano and he has really explained it well. I have a link of this video in the video description below so go watch it it's going to be very useful if you want to know the details. naive base is used in email spam detection and rigid character recognition, the weather prediction, face detection and news article categorization. Now let's go straight into the coding. From the KGLE website I have downloaded the Titanic dataset and it's available locally here in form of CSV. You can see all the passenger names, their features and whether they survive or not. I have downloaded this file locally. I have loaded the same CSV file into a pandas data frame in my Jupyter notebook. Now we are going to do some data exploration first. We can see that some of the features are probably not relevant so I am just going to assume that they don't have an impact on my target variable. For example name, right? Like name doesn't matter like what was the name, it doesn't have an impact on the survival rate. I am going to drop those variables and make my data frame a little simpler. I dropped all these variables and now I have this data frame. One thing I noticed was there is this target variable which I want to separate out into a different series. I will say target is equal to DF dot survived and inputs which is my dependent variables. For that variable I will drop survived column and just put a rest of it. So this way I have my independent variables and dependent variables into two separate entities. We can see that the sex column is text and we want to convert it into a dummy columns because we all know that the machine learning models can't handle text so we have to convert them into numbers and I have one hot encoding or the dummy's tutorial and there I have explained why it's needed but here that's what I am going to do. So the dummy's will basically convert the sex column into two different columns and it will just put zero and one values. We are going to append these dummy's columns to our inputs data frame and the way you do that is by using the pandas concat function where you concat these two data frames on columns and when you check the data frame which came as a result of concat operation you will find that now I have these two columns. Now I need to also drop the original sex column because now I have dummy columns so I don't need that particular text column. So now my data frame looks like this it is much simpler with all the numbers there is no text. I also want to find out whether there are any numbers in any of the columns and the way you do that is by using this. When you run it it tells me the age column has some any values and I am just curious to find out what those values are. So I am just going to print like the first 10 values and I find that some of these values are n and n. Now I have to turn on how to handle n and n values in pandas so you can refer to that but the popular approach here is you can take a mean of the entire column and then feel those n values with the mean value. And the way you do that is by doing this you can say inputs dot age dot fill n a with the mean and when you do that let's print first five the fifth one had I guess the n value and you will see that it filled it with the mean value. I think fifth or the sixth okay the sixth value had it. So you see that this was the mean value. I know point 69 probably doesn't make much sense so you can just average it out you can just make it like a whole number but yeah. Now we are going to use sklons train test split method to split our data set into training and test sample this is what we usually do before training our model. So that the model is not biased when we are trying to calculate the score and this is how you train test split. This is something we have done in all the tutorials so this is pretty simple there is no rocket science to it. I am going to divide my test and training sample into 20 80% ratio all right so when I do control enter it's going to divide it into training test samples. I can just quickly check the length of x train and the x train x test to see the distribution so you can see that and the total is length of inputs so the total was 891 and 80% of that is 712 20% is 179 you can also print these individually if you are interested for example if you want to check what is your x train you can print like that. All right now it's the time to create the naive base model. Now there are couple of naive base classes we are going to use Gaussian naive base the Gaussian naive base is something that you will use when the your data distribution is normal or it is like a Gaussian distribution this is a concept in statistics you can learn about it it is also called a bell curl okay so we can use that model here okay so I have created that model and now it is time to train the model you all know that you have to call fit method whenever you want to train the model it and you train it on x train and y train and x train and y train is something that we got here so when you execute this it will train it now the training here is very fast because our samples are very less when you have so many samples it might take time and you have to use parallelization GPUs and so on. The first thing I do after my model is train is I always measure the score to find the accuracy and I found that it was 77% I know it's not very high but it actually depends on the samples as well so if I run this again the score is going to vary so you can see it is now 78% now let's look at our x-taste samples so x-taste I will check the first 10 samples and I will see this is what it is I will also check my y-taste sample to see what are their values so y-taste is this oh this is probably not a good taste set okay let me just run it once more so when I do control enter it again creates some random samples and the score is a little better this time you can see there are a couple of ones here and you can now use model dot reddit for our first 10 samples and you can now compare that to y-taste so you can see that it was 0 1 here is 0 1 0 0 0 so 0 0 this sample it didn't get right you see it says it's 0 so our score is like 81% so it is expected it will make mistakes sometime and that's fine you can also use predict probability function to figure out what are the probabilities of each class like in our case each class is whether a person survived or not survived when you run this this sample says that 97% probability is the person didn't survive and this much probability that actually this much probability that person survived and this much probability that he didn't survive right that's why it makes a decision that it is 0 in the second one the person survived it is 1 so that's why this probability is like 93% this all I had for part 1 in part 2 we will cover email spam detection using naive base and we'll also have exercise at the end of that part 2 tutorial thank you",
    "Machine Learning & Data Science Project - 5 _ Model Building (Real Estate Price Prediction Project).wav": " In last tutorial, we cleaned up our data frame and made it ready for model building. In this tutorial, we are going to build a machine learning model and then use K-fold cross validation and grid search CV to come up with the best algorithm as well as the best parameters. Here in this data frame, we have a location and as you all know that machine learning model cannot interpret tax data. This one is a tax column. We have to convert this into a numeric column and one of the ways of converting tax column, which is a categorical information, into numerical information is to use one hot encoding. It is also called dummies. We are going to use pandas dummies method here and the way you do that is you take your data frame and the column that you want to hot encode. Then call pd.getdummies function on it. When you run it, what it will do is for each of the locations, it will create a new column. For example, first few locations are first block jiannagar. For that, create a new column and set the value to 1 and remaining all the values are 0. When you have a location as first face jiannagar, the value c here, the value is 1 and remaining values are 0. It is a pretty straightforward encoding method. If you want to know more about this, I have a separate video just to cover one hot encoding. Now let's say you have this dummy columns here and I'm going to store them into a separate data frame. It looks something like this and then I will append that into my main data frame. I will create a new data frame called df11 and I will concatenate df10 and these dummies. So df10, so the way you do that is df.conkit. I want to concatenate two data frames, df1 and df2 on columns. Now which are the two data frames that I want to concatenate? One is df10, the second one is dummies. Now we learned in our other one hot encoding tutorial that to award a dummy variable trap, you should have one less dummy column. Let's say if you drop first block jiannagar column, then to represent this column, we can use zeros in all other columns and that will means it's first block jiannagar. So we can live with one less column and for that reason, I'm going to drop the last column here and the last column is other. So this is the column I'm going to drop. And now my data frame looks something like this. So it has our necessary features such as total square foot, bar, price, etc. and then all the location columns are dummy encoded. So they are represented as the numbers. So of course, now I can drop this location column because I have already covered that column into those dummy columns. So here I will just drop that column and create a new data frame called it df12. Wow, we are already at number 12. So this shows how long over data processing pipeline has become. This is like a pipeline. And df1 is one stage, df2 is third to second stage and df12 is probably the 12th stage. We are now all set to start building our model. First let me examine the shape of the data frame. So we still have 7,000 rows, 245 columns. I will create an X variable because X variables should contain only independent variables. My dependent variable is price. So I need to have price dropped from my data frame. So I will say drop the price on x is equal to columns, which means drop the price column and that will give me my x which I need for my model training. So X is all independent variables, total square foot bath, BHK and remaining all the columns are representing the location. And Y is df12.price. And when you do Y dot head. Alright, so my X and Y are ready. Now you all know from my other train test plate tutorial that we always divide our data set into training and test data set. Then we use training data set for the model training and to evaluate the model performance we use the test data set. So we are going to import train test split method from a scale model selection and my test size is 0.2, which means I want 20% of my samples to be test samples and remaining 80% I am going to use for model training. Now here what I have done is I have created a linear regression model. I have a separate tutorial on linear regression. So if you want to look into the math behind it, you can follow that tutorial for this tutorial. We are just going to call fit method on X train and Y train. And once the model is trained, which is in this tab, the next step is to evaluate the score of our model, which will tell you how good our model is. When you execute this, it will do the training on X train and Y train. And then it is giving me the score. So the score is 84% which is pretty decent. Typically a data scientist would try couple of models with couple of different parameters to come up with the best optimal model. So that's what we are going to do. We are going to first use a K fold cross validation again for K fold cross validation I have a separate tutorial. So if you have any curiosity in knowing the details, you can go and watch that tutorial. So here I have imported some imported needed methods. And then I am creating a shuffle split for my cross validation. Shuffle split will randomize my samples so that each of the fold have equal distribution of my data samples. So it's not just targeted into one area. And when I use cross validation, I'm getting these scores. So you can see I'm kind of getting more than 80% score all the time. I mean, here I got 77% but majority of the time I am getting more than 80% okay. I don't want to go into detail of how cross weld score works because that's a big topic. And if you don't know about that, I recommend that you watch my K fold cross validation tutorial. Now next step is we figured that for linear regression, even if you run five fold cross validation, we are getting score more than 80%. But how about trying a few other regression techniques? There are like lasso regression, there is a decision tree regression, there are various regression algorithms available. So as a data scientist, I want to try those different algorithms and figure out which one gives me the best score. For that, we use a method called grid search CV. For grid search CV also, I have a separate tutorial and I'm going to link the tutorial in this video at top right corner as well as in video description. That's a very good API that a scalar provides which can run your model on different regressors and different parameters and it can tell you the base score. Let's import grid search CV. Other than linear regression, of course, I want to try lasso and decision tree regression. For grid search CV, I'm going to write a function. So this is how function is going to look like. So I will say, okay, find my best model using grid search CV. I will supply X and Y as an input and this should tell me which algorithm is the best. Now in this configuration, I have specified the algorithm as well as the parameters. So grisers CV will not only do the best algorithm selection. For that particular algorithm, it will also tell you the best parameter. This is called hyper parameter tuning. To save some time, of course, I'm just copying and pasting the code. So scores, I'm going to store in this course list. I have this cross validation, Suffolz split, which will just randomly shuffle my sample so that I can get more better result. And then what I'm doing is I'm going through this Python dictionary. This is just a dictionary. I'm just going through that and initializing grid search CV object with this model as well as these parameters. Okay. This is the parameter grid that it will use for a cross validation. Of course, I'm using this CV object with five old cross validation. And then I will call a fit method. Once this method is called, I will append the scores into this scores list. Okay. And this GS dot base score and base parameters will tell me the best parameters and base code for that particular run. Okay. And then I will return the resultant scores into a data frame. All right. So once I have this method defined, I can call the method on my x and y. I paused my video because the training might take some time depending upon your computer. And after running the training for some time, it came up with the base score. So here you can see that the winner is linear regression. It has the maximum score. You see, LASO has 68 person. This is entry has 72 person. And for linear regression, these are the best parameters, which is normalized false. So I can conclude that my linear regression model is the best one. So whatever allows CLF classifier, I have created here. I will just use that. It is already trained with 84 person score. So I'm just going to use that to make property price prediction for couple of sample, just to kind of test it out. And for that, I will write a predict price function. Okay. So let me write that function. So the predict price function takes location, square foot, bath and BHK as an input. And it will return you the price estimated price. Now in your x array, the 0th column, the first column is square foot. The second one is bath, third one is BHK. And for location, you know, we have some 240 location columns. So for location, I am locating the appropriate column. So let me just show you. For example, if you do x dot columns, you find all of these columns. Okay. If I want to know the location of the second phase judicial layout, then I can simply do this. Here, I can say my location is second phase judicial layout. And when you run it, it gives you the column index as five. You can see that this is 0, 1, 2, 3, 4, 5. Okay. So that's how this method works internally. And once you have location index, you can set that particular index value to be one here. And that's exactly what I'm doing. And by doing this, this will give you the predictor price. So let's execute this. And now let's make price prediction for first phase JP Nugger, where the property square foot area is 1000 square foot is two bedroom and two bathroom apartment. So you get 83 lakh rupees as an estimated price. And if the same thing is, let's say, three bedroom, three baths, then you get 86 lakh, which kind of makes sense, you know, let's look at some other location, some high price location. We know Indra Nagar in Bangalore is little costly. So when you run this, you get, so in first phase JP Nugger, the price would be 83 lakh versus Indra Nagar is one crore 81 lakh rupees. And the same home for three bedroom, three bath is 184 lakh. Now one observation you will make here is a second column, third column is bath, the fourth one is BHK. So let's say for the same bath and if I have more BHK and if I run this, I'm getting little less price. You see 83 versus 81. You would think that the price should be higher if my BHK goes higher, okay? But that's not what our data is telling us. If you look at our data, we have many samples where for a given location, three bedroom apartments for same like square foot area, they cost less compared to two bedroom apartment. Now there could be different reasons. Like sometimes if you're a thousand square foot home and two bedroom apartment and the two bed, each, the size of each bedroom will be big enough versus if you're three bedroom, you're getting three bedroom, but each of the bedrooms are very small and compact. So someone might not like that. That's one reason. Other reason is because of our data is distributed. Sometimes we don't have enough information on why three bedroom apartments would cost or less compared to two. For example, check this. Here in Rajajinagar, I have two bedroom and three bedroom apartment. The square foot is little higher, but look at the price. Price of two bedroom apartment is 42 lakh more than this one. And if you observe our data properly, you will find many such examples. And that's the reason why the model is giving this behavior. All right. Now it is the time to export the model to a pickle file. At the start of this tutorial series, we say that we will export this model to a pickle file and then it will be used by our Python flash server. So now that our model building procedure is done, we are exporting all the artifacts which are needed by our Python flash server to different files. Okay. So as a data scientist, you worked with your business manager. You went through various iterations. You tried different models, grid search, CVK full cross validation. You clean your data. You removed outliers and you came up with this awesome model which is ready to be used in production. In real life, people also use AB testing just to test previous model versus this new model. But we are not going to cover AB testing, etc. We will just look into how to export this model and then use it on our website. For exporting the model, also I have a separate tutorial. But it is very simple code actually. What you do is you import a pickle file and then you just say pickle.dump on and then you pass your model, your classifier as an argument. And when you execute this, it will export this file. When I open my directory, I see that I have this file. You see, this is the model that it got exported. The size is 5kB. You would think like why the size is so small? Because this linear regression model is just storing the coefficients and intercept and all those parameters. It doesn't have your actual data. That's why the file size is very small. Other than the model, we also need the columns information. For example, here in my predict price function, I had this x.collums. These columns, the way they are structured and their index into the list is important for making a predict prediction. So I will export that information into a JSON file. And I have imported JSON and then all those columns, I am converting into lowercase because you know, like there are like upper and lowercase combinations. It's better if everything is in the lowercase and then I am dumping all of them into a JSON file. Here is my JSON file. If I add it with not pad, it looks something like this. So it has all the columns. So this JSON file and this pickle model are going to be used in my Python Flas server. So this is all I have for this tutorial. In the next tutorial, we are going to build Python Flas server, which will use these two artifacts. Thank you for watching. I will see you next tutorial.",
    "Machine Learning Tutorial Python - 3_ Linear Regression Multiple Variables.wav": " We are going to look into linear regression with multiple variables also known as multi-variate regression. Using this we are going to predict the home prices in monotone ship, New Jersey. Here in this table I have various metrics available such as area, bedroom, age, and these are the factors that the ultimate price depends on. In the previous tutorial we looked at simple linear regression with one variable where price was dependent only on area, but now we are making our problem little more complex by adding bedrooms and age because as we know in normal life the home price depends on multiple factors not just the square foot area. After we build a model we are going to predict prices of these two homes. Now before we tackle any machine learning problem the first thing that needs to be done is we need to carefully analyze our data set or our training data. When I look at this data set the first thing I notice is there is a data point missing here. So we have to do something to handle this missing data point. Another thing that I notice is there is a linear relationship between each of these factors and our target variable which is price. For example as the home gets older the price tends to go down. Here I have 3200 square feet home with 18 years of age and the price is more than 600 000 whereas I have a little bigger home here 3600 square feet but since the age is more the price is less than the one whose square foot area is less. Similarly as the area and number of bedroom goes up the price also tends to go up. So overall by analysis we can say that for this data set we can use linear regression safely. If we go into a math a little bit then our linear equation will look something like this where price is dependent on three factors area bedroom age and these three factors are called independent variables or features. Features is a term that you will hear often while going through machine learning material. It is nothing but an independent variable and the price is a dependent variable. M1m2m3 are called coefficients and b is an intercept. It can be generalized into this equation where you can have n number of independent variables. I just have three here but you can have more than three. Now the topics that we are going to cover during this tutorial is first we'll look into how to handle that missing data point and then I will build a model in a Python code. All right now let's go back to our Jupyter notebook. So Jupyter notebook is something I'm using to write my code. You can use p y charm or any other idea of your choice. I have a CSV file here where I have all my home prices along with these factors. So the first thing I have done here is imported a necessary modules and then I'm going to load my data into pandas data frame. Home prices dot CSV. If you don't know about pandas data frame then I have a separate pandas tutorial. Pandas is extremely useful while doing machine learning. So I highly recommend that if you don't know about it already. Okay so now I have my data frame ready and I can see this null data point and I need to handle it. So the way I'm thinking I will handle this is I will take a median of this entire column and put it here. Since number of bedrooms are missing, taking median seems like a safe assumption. All right so first thing I will do here is calculate the median and how do you calculate the median of bedrooms. So DF dot bedrooms will give you pandas series and when you do this it gives you a median. Now you have three bedrooms appearing two times. That's why it's giving the average of it which is 3.5. I just want to keep it integer the whole number and for that I will just import a math model. And I will say median bed rooms is equal to mat dot floor and the medium bedrooms would be three. Okay and the way I fill this column's any values is using fill and a function. So fill and a function is available on pandas series. So DF dot bedrooms will give you one column which is nothing but a pandas series. On that I call fill and a function and I want to fill all and a values with this median number. So you can see that I got a new series where this nn value is now replaced with a median number. I need to assign this back to original series so that my data frame gets updated and when I print my data frame I can see my data frame looks much better. So our data pre-processing step is over. So again to summarize before applying any machine learning model you need to pre-process your data. You need to clean your data because data is always messy. There are problems with it. So you need to fix the errors kind of prepare your data and then apply your actual machine learning model using that data you train the model. Okay so now my data frame looks good. I am all set to train my model. So the first thing I am going to do is create a linear regression object. So I have imported linear model here and using this I will create linear regression class object. The object got created fine. Now I will call fit method. So you will use this fit method often and this method is used to train your model using your training set. Here my training set is a data frame. So if you want to create a data frame using your existing data frame you can use the syntax of using two brackets. Okay and my independent variables are area bedrooms and age. And my target variable is price. When I execute this my model is now ready. Once it is ready it's a good idea to take a look at the coefficients. So the coefficients I got was this just to summarize what these coefficients are in this equation m1m2 and m3 are coefficients. So this is m1 this is m2 and this is m3. Now you will wonder where is my intercept. Intercept is stored in another variable called intercept. So this number is this b. So once we have m1m2m3 you have area bedrooms age which are your independent variable. Now you can calculate the price. So here I can say predict. Okay. So I can predict a home price of a home whose dimensions are this. So 300 3000 square foot 3 bedroom and 40 year old. Okay. So 3340 that's what I'm going to supply here. And if okay so 3000 square foot 3 bedroom 40 year old and I found find the price to be this much. All right so this home is you can see that the price of the 3000 square foot home here is 565 thousand but here got it you got it much less and the reason is I think mainly age. So here it like 40 year old versus this home is 15 year old. Okay. So if I make it 15 year I will get around it is little bit skewed but you get the ballpark number. All right. So the price is this and now I want to know how it can get this price. So it is coefficient. Okay. Coefficient multiplied by your square foot plus another coefficient multiplied by your bedrooms. The third coefficient multiplied by the age. Here it is missing an intercept so we need to add intercept also plus intercept and when you do that you get that price right here because of rounding factor is not showing the full value but it is essentially the same price. Similarly you can predict the price of the home where the values of those factors is 2500 4 and 5 2500 4 bedroom only 5 year old. This home is very new so I expect the price to be little higher than average. But here you can see that 5888000 dollar is the price. For this home which was 2600 the price was 550 but this home's price is more and it is because this home was 20 year old whereas this home is only 5 year old and it has one more bedroom. See this guy had only three bedrooms. So you can see how different factors play a significant role. Okay. That's all I had. Now we are going to do an exercise. So the exercise is for you guys to practice what you have learned in this tutorial. I have this CSV file which contains a hiring data for a form. Here based on the experience the written test score and the personal interview score candidate salaries decided and I have some past data. Using this data you have to build a model for your HR department where they can feed in experience and various scores and they can get some idea on what kind of salary they need to offer to a candidate. Now in this data set you will find few interesting things. First of all in experience these two cells don't have value so you can just assume them to be zero. Here also that is one data point missing. Here you shouldn't assume it to be zero maybe take a median here. All right and also usually linear regression models work on numbers whereas here I have a string. So that experience again is the number is just that our data set contains word for that particular number. For this you can use Python's word to number model to convert that string into a number. All right so just do peep install and use the model. All right and using this what you're going to do is figure out the salary for two candidates who has these statistics. So one candidate has two years experience scored nine and six in two tests. Second candidate an excellent guy 12 years experience scored 10 out of 10 in both of it and you have to find out recommended salary for these two candidates. If you go to my GitHub page let me go to my GitHub page. I have all the tutorials available in my GitHub so you should go and download it. So here this notebook is whatever I just went through and in the exercise folder I have a CSV file which you should download and try to practice on your own and then you can compare your answer or to this notebook where I have provided the entire solution but don't get tempted too much and don't go and just start looking at the solution. First you should try to solve on your own. All right so that's all I had for this tutorial. Thank you very much.",
    "Machine Learning Tutorial Python - 15_  Naive Bayes Classifier Algorithm Part 2.wav": " Here is a list of topics we are going to cover in this video in the end we have an exercise for you so watch till the end. I have this file with spam emails where I have the text body of the email and the first column says whether it's ham or spam. Ham means it's a good email it is not a spam okay and you can see that whenever you have spam you heard terms like free message or free entry or winner you know this just by reading it it definitely looks like a spam. I have loaded this particular CSV file in Apanda's data frame as usual and I'm going to now do some data exploration to see what's going on with this data. The first thing I'm doing is just grouping it by category and describing it so that I know that there are 4,825 ham and 744 47 spam okay so that much I know there are good amount of spam in our dataset. The spam column is text I want to convert it to a numbers because we all know that machine learning models they understand numbers they don't understand text so we have to convert the category and message both the columns into numbers somehow okay. The first one is very easy category ham and spam we can use one and zero so that's what we are going to do and the way to do that is by using this apply function. Here when I say DF category it takes the category column and applies this particular namda function on it. The namda function takes each individual values and checks if it is spam then it will put a return a value of one otherwise zero and we will create a new column called spam here and when you execute this you can see that all the ham are zero the spam are one here. Once that is done we can import the train template method from SK learn as usual here I have imported it and I'm gonna keep my test size to be 25% and when you run it it's it is splitting the samples into train and test dataset okay. Once this is done we still have a message column which is text so that text column we definitely want to convert into into numbers. Now the way we'll do this is using count vectorizer technique. In count vectorizer technique let's say you have these four documents or the email bodies with all this text. One of the ways to convert this into matrix or a vector is you find out the unique words in each of these documents and you'll find that there are nine unique words combined in all of these documents. Now you can take each of these documents and you can treat these nine unique words as features or kind of like a column and you can build this kind of matrix okay. Here this is the first document second document and so on the first document you say and is zero the occurrence of and is zero times you see that there's no zero document appeared once first appeared once. Similarly in the second document the document appeared too so you say document here document here. So this is a simple technique of representing words as count okay and we can use the individual columns as features for our problem. I took this example from a scalar documentation and here is a code snippet from a scalar documentation it explains the same thing but with the a scalar API and these are the APIs we are going to use. For our data set I used count vectorizer and created the metrics which I showed you in the picture. So it created probably many features that's why you see dot dot dot and these features are equal to number of unique words in our corpus corpus is basically all the unique words that you see in this huge data sets. So you realize it will be many columns in our metric. Now naive base have three kind of classifiers like Bernoulli multi-nomial and Gaussian. In the last tutorial we used Gaussian naive bias. Here is the core answer on what's the difference between the three and this guy actually really gave a good answer where Bernoulli is basically when you feature. So it features not the target variable when your features are zero or one. Their binary nature that's when you use this. Multinomial is when you have discrete data for example you're moving ranging from one to five. Gaussian naive base is when you have normal distribution or a bell curve in your features. We are going to use multinomial naive base here for our problem and the way you do that is by writing this code. Just to save the typing time I'm just copying and pasting the code but you kind of get it you run model dot fit function on your x-trained count and y-trained. Remember the x-trained count is basically the text which is the emails converted into a number metric. Once the model is trained it is ready to make a prediction. So let's have two emails. Okay so we have these two emails. Now the first one looks like a good email where a friend is asking another friend to go for a football game and the other one clearly looks like spam and when you run it you see it detected the second email as one which is it's a spam. Alright now let's measure the accuracy or the score and the way you do that is first x-taste you need to convert it into count because our model is designed such that it works only on numbers and then you can feed it to model for prediction and you find that the model performs really well with 98% probability. So you can see that for spam non-spam type of problem the naive is model works really great. Now you found that the converting it into a metric create a little bit of inconvenience in terms of when I was supplying x-taste count I had to perform this transform method also when I was trying this test emails I had to call this transform method before giving it to my model. Eskilon has a nice feature called pipeline where you can define a pipeline of your transformation. Here what we are doing is on our raw data we are applying some sort of transformation before feeding it into our model. Right now we use only count vectorizer some people use more than word one transformation people use like TFI DF and so on. So in that case if you have Eskilon pipeline it is super useful and convenient and I'm going to show you how to use the pipeline. So the first thing you do is you import the pipeline like this and then you create the pipeline using your pipeline steps. So my first step is count vectorizer. So just to remind you on what I'm doing here is I'm trying to simplify the same code base. So the code worked till here okay our model is ready it's code 98% fine but since we had to perform this transformation steps I am writing the same code using a simple API and here I created a pipeline with two steps. First step is convert my text into the vector of count vectorizer and then apply the multinomial naive base and when I have my classifier created what I'm doing is I'm going to train it. Now this time when I train it I can train directly on X train. So remember what is X train? X train has this text okay in the previous example we use X train count we converted text into count and then train the model here we can directly feed the text into our model because internally this pipeline will convert to a vector first and then it will apply naive base on that. So when you run it you can see this works okay and again you can check the performance of our classifier it is same 98% okay and just to verify if the emails prediction works okay you can run this and you can see the first email is not spam the second one is spam and those are these two emails. Alright now comes the most interesting part of my tutorial which is the exercise. I always give this example that if you want to learn swimming by watching swimming videos you're not going to learn swimming okay what do you do you have to move your butt and jump in the swimming pool. Similarly if you want to know coding you have to code and I have prepared these exercises with so much effort for you so why don't you take your laptop and just work on this naive base exercise all you have to do is load ascalon data sets, wind dataset and classify those wines into one of the three categories using naive base. You can use Gaussian and multinomial classifier and tell me which one performs the best in the comments below. I have the link of this file, exercise file in the video description below. Also the tutorial code that was shown in this video that code link is also available in the video description below so all this code is available so you can just go ahead and try it. There is a solution link by the way but a good student will not click on the link without trying it first on his own. So I assume you all are beautiful good students you will try this thing on your own and then only look at the solution to match your answer. Thank you very much if you like this video give it a thumbs up share this with your friends subscribe to my channel and please post a feedback in the comments below it really helps me it helps me improve my content thank you bye",
    "Machine Learning Tutorial Python - 11  Random Forest.wav": " Random Forest algorithm is another popular machine learning technique used in regression and classification both. Now why is it called random forest? Well the forest has trees and a tree in machine learning world means a decision tree. Now if you haven't seen my decision tree tutorial then you should click on that post button right now. Go watch decision tree tutorial and then come back here. Assuming you watch this what we try to do in that tutorial is predict employees salary based on certain features and we build a decision tree like this. Now this decision tree looks very complicated so can you represent it in a very simple image? Well yeah there you have it. You have green samples, red samples and based on that you build your decision tree. Now how can you build multiple decision trees out of this single data set? Well one approach is you take your data set then you divide it into batch of random data sets then you build decision tree for each of them. So since we did random sampling here is called random forest and now we have multiple trees so you see a forest being formed already. Once it is strained you give the thing that you want to predict and they will all come up with different decision. You just may take a majority of what out of it and get a decision. So this is the basic crux behind the random forest algorithm. And I recently used similar approach to make a decision in my real life. I wanted to buy NIST thermostat and I called bunch of my friends and yeah by the way that's me in the picture. And those smart dudes give me different opinions. One guy told me you know it's going to save you so much money. I saved like $500 last year so you should buy it. The other friend say oh it's freaking $200. It's a waste of money don't buy it. Thor guy told me oh you can control your temperature remotely anywhere from outside. So definitely buy it. All I did is just took a majority of what and decided to buy it. See making decisions is so simple in life just call your friends and take a majority of what all right it doesn't work all the time. But at least in this case it worked and I had a decision and I'm not making this up I just install it yesterday. All right now we are going to use SKLON's digits dataset to make a classification using random forest. So that dataset basically contains the images of handwritten characters and all you're trying to do is just classify into one of this 10 categories whether it's a digit from 0 to 10. All right so that's what we will do in our Python coding today and in the end we will have an interesting exercise for you to solve. As usual I have launched my Jupyter Notebook and loaded the digits dataset from SKLON datasets. All right and if I look at the properties of this dataset it has the real data and the target. All right now I'm going to use Matplotlib to visualize my data and when I run this it looks like this. So I have basically handwritten characters which is like 8 by 8 pixel array it's like a multi-dimensional array and when I use Matplotlib it looks like this you can see this is character 0, 1, 2, 3 and so on. Okay now I will create a pandas data frame from this dataset. So the data basically data is if you see if you want to see it just to make it simple you will see that it is a two dimensional array of numbers. So let's look at first five. See first five like each element is just a two dimensional. It's actually one dimensional array but it's like 8 by 8 matrix so the length of this array is 64. Okay and you can visualize this in a better way if you are using data frame. Each sample is nothing but an array of 64 integers and they map to the target wave also if you do digits dot target. It is showing the target variable and I'm going to append that into my data frame. This is how you create a new column in pandas data frame. This is nothing but digits dot target and when I look at my df.head it says that this 64 samples they map to 0. So for example this picture here is actually 0. So the target is showing you the truth. All right now what I'm going to do is use train test split. I will do from skln dot model selection import train test split. Once import that you need x train x test y train y test. Okay and train test split. Now my x is just a set of independent variables. Now my df already has target so I need to drop target from that. Okay so I'm going to drop target and x is this columns and my y is digits dot target and you need to specify how you want to split your training and test data. When I say test size equal to 0.2 it means 20% of my sample is test data and 80% are training. Okay when I run this it created all these four variables and when you look at size of your x train it is 4 in 37 whereas x test is this. So this means you're 20% of your total samples. So now I have nice training and test samples. I can use now the random forest classifier or to train my model. All right so from skln dot ensemble. Now why is it called ensemble? Ansible is a term used when you're using multiple algorithms to predict the outcome. All right that's what we are doing to here. We are building multiple decision trees and taking a majority of what to come up with our final outcome. So that's why it's called ensemble from that import random forest classifier. Okay and then just call model.fit. This is your training step. After this step your model will be trained and it trained the model and you can see bunch of parameters here. So the genie and entropic criteria is something that you studied in the decision tree tutorial already. So that's what this is. You also have estimators. So here 10 means it used 10 random trees. Okay and we are going to see the performance using the 10 random trees. So let's call model.score on x test and y test. And we can see that the accuracy is 91%. Okay when we use 10 random trees the accuracy was 91%. Now let me try to tune this model and by giving 20 random trees. So I train it using 20 trees. You can see my score is increasing. All right I had more trees and my score increase. If I do 30. Wow it's even better. Okay so looks like now it's not changing much. But it's a 40 or 50 trees you can get a base score. So you can fine tune your model by taking this parameter. It also depends on your samples as well. So if I execute this line once again then now my training set has changed and it might give me a different result. So you see it performed even better now. Your model is trained and you can give any sample input which is nothing but this kind of handwritten character image and it will tell you what number that is. All right now I'm going to plot a confusion matrix to kind of see the distribution of errors and where my model performs well versus where it performs poorly. And for that I need x predicted. So here I can use model.predict on my x-taste dataset and I get y predicted. This is predicted by model. This is the truth. Okay and confusion matrix allows you to plot truth on one axis and the prediction on the other axis. From skln.matrix. Import. Confusion matrix. And I'm just going to call it cm. Here you first supply your truth and then you supply your prediction. And when you plot this is going to plot two dimensional array. All right now this is probably not a better way to visualize it. So I'm going to use a seabond visualization library. I'm just going to copy paste code just to say some typing. So it's the same confusion matrix but it's visually more pleasing. Okay so here you have your truth data. You have your prediction. And what this means is I had 46.0 and 46 times it predicted them to be 0. So for 0 it performed like really well. Now what does this two means? This two means I had two times the truth was 8 but my model predicted it to be 1. So this is where it made an error. 30 times the truth was 8 and it predicted it to be 8. All right so this is a nice visualization which allows you to gauge the performance of a model and you should use it more often. All right now the most interesting part which is an exercise. We are going to use our famous iris dataset from skln.dataset to predict the iris pieces and for that we'll use a random forest classifier. So I want you to train your model using the default and estimator parameters and then try to tweak it and tell me what base core you can achieve by using how many trees. All right you should do this exercise if you are not going to do this exercise and you're going to make me very very angry all right so just be afraid of me and just do it.",
    "Machine Learning Tutorial Python - 5_ Save Model Using Joblib And Pickle.wav": " We will look into two different approaches of saving a train model to a file, which you can use later on to load the model from a file into a memory and use that to make actual predictions. Solving a problem using machine learning consists of two steps typically. The first step is training a model using your training dataset and the second step is to ask your questions to the train model which short looks like a human brain and that will give you the answers. Often the size of the training dataset is pretty huge because as the size increases your model becomes more accurate. It is like if you are doing a football training and if you train yourself more and more you become more and more batter at your football game and when your training dataset is so huge often it is in like gigabytes the training step becomes more time consuming. If you save the train model to a file you can later on use that same model to make the actual predictions so you don't need to train it every time you want to ask these questions. If you have it saved to a file now I don't have a training step here and I can directly ask a question so that's what we are going to look into today we'll write a python code to save that model to a file. Here I have a Jupyter notebook which I used in my first tutorial of linear regression of predicting home prizes. The code here is pretty straightforward I am loading home prizes from my CSV file and then using linear regression to make the actual prediction and here is saying that 5000 square feet home is going to cost me $859,000. So let's use pythons pickle module now you guys might be aware about pickle module already it allows you to serialize your python object into a file so here I will use the file so I'll first save with open model pickle and I'm going to write binary data hence I'm using WB mode in the file so first I'm opening a file and then what I will do is I will say pickle.dump my model into this file when I run this what actually happens is in my working directory it created this model pickle file which if I open in my not pad looks like this this is some gibberish and it is expected to be gibberish because it's a binary file okay you actually don't need to care about the content here but what you need to know is your model is saved into a file now now you can use the same model here so what I can do is here I can say model is equal to pickle dump load the file okay so again I have to open the file pointer it's the same file but this time I'm using it in a read mode and it's a binary file hence I have supplied B here now I have my model I'll just say MP so now I have my model loaded from a file into a memory and MP is the object if I use now MP object to make the prediction okay I want to ask what is the price of my 5000 square feet home then you can see that it will give me the same answer as I got it here at this tab so this is beautiful because now I can supply this model file to a friend of mine and I can say okay here is my train model or a train brain go use it for your actual problem all right so you can ask the questions to this model and it will give the answers there is a second approach of saving model to a file which is using SK learns joblib so if you google SK learn model persistent you will find this link where SK learns document documentation shows how you can use joblib to do essentially the same thing so then if it is doing the same thing then what's the difference between pickle and joblib as per the documentation if your model consists of large numpy arrays then using joblib might be more efficient now I have not done any profiling myself but you can go ahead and do it on your own and figure out which one you want to use but usually people say that when you have a lot of numpy arrays joblib tends to be more efficient but essentially gives you the same functionality so I will first import joblib in juby to not book you can hit tab and it will show you the auto complete so here there is external modules from that I will import joblib now the difference between joblib api and pickle api is that joblib can take the file them directly so I have my model and I want to save that model to a file I will say model joblib when execute this it saved this model to this particular file and when I go to my working directory I will find this file here it is just updated right now 631 is the timestamp when I open that file into not paired I will again see some gibberish because this is also a binary file again you don't care about the content here what you need to know is your model is successfully saved and you can load that model using joblib.load give the file name in return you get your model object back and that model object you can use to make actual prediction and it gives you the same answer here what it is saving inside that binary file is different things such as for example if you look at coefficient the coefficient is same as what I got it here so it's saving all these essential pieces for your model okay that's all I had for this tutorial I don't have any exercise today but you can go ahead and save your model using joblib and pickle into a file and I have gone through linear regression models today but you can pretty much save any other kind of machine learning models using these two awesome models",
    "Data Science & Machine Learning Project - Part 3 Data Cleaning _ Image Classification (1).wav": " In last video, we looked at data collection for our image processing project. In this video, we are going to talk about data cleaning. In any data science project, majority of the time is gone in data cleaning process. And you will see that in this project also, we will be spending significant amount of time in cleaning our images. Because when we download our images from internet, the images might have a lot of issues. Now when you want to detect a person from that image, just think about it. Like when I show a photo of a person, how would you go about detecting that this person is X or Y? Majority of the time, you will be using the face of a person. Now using the hide hands and legs, you can tell about a person to some extent, but your final decision of who that person is is mostly based on the face. And we are going to use the same concept. All the images that we downloaded from Google, we will first try to detect the face of the person. Sometimes face might be obstructed. So we want to detect if the face is clearly visible or not. Now how do you detect that? So you will try to detect two eyes as well. So if in a photo, you can find a face with two eyes clearly, then you will keep that image, otherwise you will discard it. For face detection and detecting the eyes, we will be using open CV, which is a famous image processing library in Python. And for the specific detection, we will be using a technique called hard cascade. That's a famous technique on how you can detect the face and the two eyes. And we'll see how exactly you can do it. It's pretty straightforward. At the end of this tutorial, you will have a clean data set that you can do for the feature engineering. All here are some sample images from our data set. You see a lot of variety here. For example, here, Serena Williams face is not clearly visible. In these two pictures, along with Virat Kohli, there are two other people here is Anuska, his wife and Amestoni. In this picture, Massie's face is visible, but it is only a side face. So his two eyes are not visible. Now how do you handle these different images? Let's look at a very simple example. So this is the image of Maria Sarapova. Here the face and two eyes are clearly visible. So we'll detect this using open CV. And once you detect face and two eyes clearly, you keep that image. In this photo, there are two faces. So first we'll detect those two faces. And using open CV, you cannot say that this is Virat Kohli versus this is Dhoni. Once we can only tell you that there are two people. And here are the regions where they have their faces and two eyes visible. So once you get these two cropped faces, we'll have to run a process of manual verification. 80 to 90% of our data cleaning is happening through our Python code in automated way. But there is 10% where you have to spend manual effort in cleaning your data. So we'll delete image of MS Dhoni in the manual verification step. In this case, Serena Williams face is not visible properly. Hence we will not use this image in our classification. Because if we do, then our classifier might make mistakes. So we want to make sure our classifiers accuracy stays high. In this image, although we know messy from the side face for computer, it might be hard if two eyes are not visible. And hence we will also discard this image. So just to go over the overall data cleaning process, you had raw images. You created cropped faces. You detected basically the faces out of all these images. You also discarded some images where faces were obstructed, such as masses and Serena Williams pictures are not visible here. Then you run a manual data cleaning process where you delete unwanted images. In our case, we are doing a sports celebrity classification only for five players, which is Kohli, Roger Federer, Serena Williams, Lionel Messi and Maria Saraboa. And here there is an image of Anus Kasharma and Dhoni who are not our classification classes. Hence we are just deleting those images. In the next video, we will be looking at wavelet transform and how you can do feature engineering to extract the features from the cropped image in an effective way. We'll then use this wavelet transform images as well as the raw images. We will do vertical stacking of raw and wavelet transform image and train our model and then hypertune it. Once that is done, we will save our model to a file and we have already looked into this architecture where we'll write Python's Flassover around it, which will be serving to our website. Let's jump into coding now. In C code dry, I have created sportsperson classifier folder where I'm going to host all my code for this project. I have already created three different folders. In the model folder, we'll do a model building and the training. The server will host Python Flassover code and UI will host UI of course. It's obvious from the name. If you go to model, I have placed my data set, the Google images that are downloaded. Into this data set folder. You can see that there are five people here. If you look at Maria Sarapova, all her images will be in this folder and same applies to other players. You have already done that. The other folder I have is OpenCv. I will talk about that folder a little later and I'm going to provide this folder on my GitHub so you check it in a video description below and you will be able to download this folder. I also have Requirement.txt which contains all the modules that you need to install. The way you install it is you go to your command prompt and see here right now I am in this folder so I can just go to my model folder and just say, PAPE install minus R, Requirement.txt and this is how you install all your modules. Now I have already installed this module so it's going to say these modules are already installed. If you get this kind of error where it says access is denied, what you can do is you can start Git Bash as an administrator. So if you run it as an administrator, you will be able to install things okay. I assume that you already have Anaconda installed so you install Anaconda and on top of that you have to install these three modules. I'm going to provide Requirement.txt file which is this in my GitHub so again you have to just download it and just run PAPE install minus R on Requirement.txt. I also have this test images folder guys where I have a couple of test images to try a few things and with that now we can start our Python coding. So I went, I opened Jupyter Notebook, I created a new notebook by going here, I call it Sports Person Classifier Model and imported a couple of important modules here. This is a CV2 module which is open CV basically and it will be helping us a lot throughout this project. So the very first basic thing you can do in CV2 is read an image. So here from the test images folder that I have, I'm reading an image of Maria Sarapova. So this is a beautiful looking image. I'm just downloading it and when you read this image in CV2 you will realize that the shape has three dimensions. So this is X and Y, X and Y coordinates and the third dimension is the RGB channels. You know that any color can be represented using RGB values and therefore you have this third dimension for your RGB values. Now if you want to quickly show that image you can use PLT which is Metplotlib basically and PLT has this method call IM.Show and that will show you this image. Now when you look at this image it's a colorful image basically which has RGB values and if you want to change it to a gray image you can do something like this where you can see that it is removing that third dimension that you have. Alright and the gray values, I mean ultimately they are all numbers. It's an dimensional array with numbers from 0 to 255 and when you plot a gray image using again a Metplotlib. Metplotlib has this IM show function you can use CMAP gray, the gray image looks something like this. Now where you are going to detect the face from this image and also the eyes. Now if you look at open CV documentation they have this nice article on how you can detect face and eyes using hard casket. We are not going to go too much into detail on what is hard casket, how it works because it requires a long discussion. But just to give you brief idea you have this line and edge features and it will use a moving window of this edge features to detect where is your nose and where is your eyes. For example in this image when you have eyes the area of eye tend to be more darker than the area below. Similarly when you have nose the area of eyes tends to be darker and the tip of the nose will be little brighter. So you can use all this mask to detect these areas. Open CV documentation contains this ready made API which you can use to detect the face and image. So if you don't want to bother too much about the hard casket and inner workings of it just assume that there is this cool technique called hard casket which helps you detect face and images and your result will look something like this. So I am going to try the same code here on our image of Maria Sarapova. Now going back to our folder structure let's see here we have this open CV folder which I downloaded from open CV GitHub and it has all these hard caskets. So what are these hard caskets? They allow you to detect different features on the face. They allow you to detect face, eye, left eye, right eye. So these are the different XMLs or pre-trained classifiers that you can use for detecting various features. And I am going to first try face. So and I am just copying pasting the code from my other notebook so that it saves me time on typing because there is like lot of code that will be writing. So I loaded that XML file which is called front face default and also I loaded eye casket. I am not using eye casket for now. So when I load face casket and when I say detect multi-scale from this gray image what is this gray? There is nothing guys but this gray image of Maria Sarapova. On that you are saying now detect me faces and what it return is an array of faces. So if you had two faces it would return two faces right now it return only one face and this is an array of four values. So what are these four values? So it is your x, y, width and height. See this image has this scale so 352. So 352 see you see 300 here so 352 will be somewhere in between this and 38 will be somewhere here. So see at this point the face starts and the width and height is 233 so you go 233 here 233 here and this will be your face. So let's draw that face so that we know how it looks. So since faces is a two dimensional array we are going to detect the first face we are going to store that first face in x, y, w and h values. Once you have this you can now crop that face not exactly crop but you can draw rectangle around that face using open CV. So in open CV you can say CV2.rectangle in my image so IMG what is IMG? Well IMG is my original image and in that IMG image I am saying draw rectangle with a red color. See this is RGB. R is 255. That's why this is going to draw red rectangle and the rectangle dimension will be it will start with x and y and then x plus w and y plus h. And I will store that into my face image and when I draw it you get something like this. So now my face is very clearly detected. Now I am going to draw the two eyes. So this is the code nothing fancy about it. The open CV documentation has this code so I have just done copy paste from there. So what we are doing is we are iterating through all the faces. In our case even if you don't have full loop it will work because we have just one face. And for each of the face we are first drawing face image. So see face image is nothing but this. So we are doing that and then we are applying eye cascade. So eye cascade will give you eyes and you might have multiple eyes. So you are going to run full loop on those eyes and again do the same rectangle but you see I am doing rectangle in now green color. So this is RGB. This is RGB. This was read before. Now I am doing that in green. This code is extremely simple believe me. Now you get these two eyes detected. In this code RY color was nothing but the rectangle region that read rectangle that you are seeing for the face. So if I just plot RY color you will notice that I get a cropped face and this is something we are interested in. I am calling it RY because it is region of interest. We are interested in the facial region of every image in our dataset. So we will be cropping the face region from all the images and we will store this cropped images into a different folder and use that for our model training. So now what I am going to do is write a function where I can input this image. For example I had this image right the origin of image. Let's say I have a function where I input this image and function returns me the cropped face if the face and two eyes are detected clearly. And that function I can run on all my images. So let's write that function. It's the same code. I am just creating a simple function out of it. So my image will be supplied using an image path as an input to this function and it will read the image. It will then convert it to gray and then detect the faces first then you go through all the faces and if the number of eyes that you get in your face is greater than equal to two then it returns you the region of interest basically. So let's try this function on Maria's image. Whatever we did previously we are now doing the same thing using the function. So see this is the original image I have and let's see what kind of image is returned by this function. So I am calling this function on that image. I am passing the path in that and that function is returning me the cropped image. When I plot that image it looks something like this. So this is pretty cool because this way I get a cropped image. Now if the face is not clear and if the two either not clearly visible we want this function to return nothing because we want to ignore that image. Now if you look at our test image folder so let's see. In our test images I have the second image of Maria where her face is actually obstructed because the two eyes are not clearly visible. So I don't want to use this image for my classification purpose. So let's see how our function behaves for this particular image. So first I loaded this image and I plotted it here so this is how it looks. Now I am going to call my function. See I am calling this function on this image, Sarapur, which is this image. And when I run this function I get nothing. So cropped image noise is now none which means the face is obstructed and I don't want to use this image in my model training. Alright now in my dataset folder I want to create a new folder called cropped which I will do programmatically. In that I want to store all the cropped images. So if you look at my original images they are original and they have a lot of things. And I want to generate a cropped folder. So how do I do that? The first thing I am going to do is initialize couple of variables. So dot slash means the current directory. So my current directory for this notebook is the C. Here is my notebook. This is my current directory and my dataset is in dataset folder which you can see here. And my cropped dataset. So CR means cropped dataset. I am going to store in the cropped folder. And first let me store the path of all the induces subfolders in a Python list. So I am using a Python OS model. And when you do OS dot scan directory what it will do is it will go through all the sub directories within my dataset folder. So my dataset folder has how many directories these five directories. Those names of those directories are going to be stored in this image directory variable. So if you print that variable it will look something like this. Now I have complete paths of induces of folders for each of these players. So now what I am going to do is if cropped folder doesn't exist then I am going to create it. So right now there is no cropped folder inside my dataset. Okay. So let's see. So this is my dataset folder. This doesn't have any cropped folder. You can see that. But this code will generate that folder if it doesn't exist. See now I got cropped folder. So what this code is doing very very simple code. What I am saying that is if the folder exists OS dot part dot exists means does this folder exist? No. Oh sorry. Here what it is doing is if the folder exists then I am removing it. So that if you are doing multiple run then if you have some old image you want to clean it. So first thing is if the folder exists remove it. Then this line will create that folder. So make directory will create that folder. So now I have this folder. Pretty cool is looking good guys. So far life is pretty good. We have no issues. Now what we are going to do is we are going to iterate through each of these image directories. So for image directory in image directories. Okay. So I am going to iterate through all these images first. And I am going to build let me just copy paste some variables here. So I will need these two variables. I will explain you the reason. But crop image directories nothing but the it's similar to image directories but it contains the cropped folder path for each of our five players. So when I go through this image directories first thing I want to do is what is my celebrity name. So I am just again going to copy paste. I am doing copy paste just to save time. So when you do this what happens is see what is my image director first of all image directories this. Okay. When I split this string by slash it will give me these two tokens data set and Lionel Messi. Okay. And these two tokens will be stored in a list. And you know in Python list when you do minus one it will give you the last element from the list. So here what's happening is I am splitting all these strings one by one and taking the last element which will be the name of the celebrity. So if you want to just verify really quickly and if you say celebrity name you see you are now getting celebrity name in this variable called celebrity name. All right. What is my second thing now? So my second thing is I want to now I trade to each of these folders and I trade through all those images. So let's see what is my folder. So my folder here is less a Lionel Messi. So now I am going to go through all these images one by one and use that get cropped if two eyes function to create a crop image. Okay. OS dot scan direct is a nice function. You supply the image directory it will tell you. It will give you the iterator which can help you go through each of the images or each of the files from that folder. Okay. So my entry. All right. So the entry dot path. Okay. So the third path will have the path of that image on this path I want to call my function. So when I call my function in RY color I will get the cropped face if the face and eyes are clearly visible if they're not what will be the value of RY color. We already saw that the function will return none. See if you look at this function it returns only if the eyes are clearly visible. Otherwise it returns nothing which means it's none. So now what we have to do is first we have to check if RY color is not none. If this is not none which means my face and two eyes are clearly visible. In that case you can store that image into a crop folder. So first you need to get individual folder for the celebrity. So what is path to see our data? So path to see our data is data says less cropped. Okay. So let's see. So it will be this. In this crop folder you want to create a sub folder for your player first. So who is a player? Well celebrity name. Celebrity name is the player. So you are going to create a cropped folder which will be path to CR plus celebrity name which will be data sets, slash crops, slash\u5229onal missy. If you print this name of the folder it will print that. And if that folder doesn't exist. So here see if this folder does not exist then your first thing is you create the folder. Python's OS module is pretty handy. You just call OS.make.dfs and it will create that folder for you. All right. So now I am going to just print this folder so that we know you know it's generating this folder. If you want to run this code just for fun you can run that. See right now it is doing cropping. We are not saving the images yet but I want to just show that you can see it's going through all those\u5229onal missy images creating cropped images and generating a folder is still working on it. Now see it went to my RSARAPOVA and it is saying generated a folder for my RSARAPOVA and so on. Okay. So I am going to stop this cell. So if you click on this button it will stop this execution because I wanted to just demo the code so far. If you look at your cropped folder see\u5229onal missy and my RSARAPOVA. You got two folders but they were empty. So I just delete it. I will finish rest of the coding and then we will run the same code block again. Okay. So I have now cropped folder. What I am going to do is now I have this cropped image directories. That is nothing but a list of your cropped image directories for each of the sports person. In that I am going to append the cropped folder. Okay. So this is just a helper variable. It will help us later on. So that is what it is. The code so far is extremely simple. Okay. I hope you are understanding it till this point. If you don't take a pause try to think about it. No rocket science guys. This is extremely simple project. All right. Now I am going to do one more thing which is once you create a folder outside that if log I want to generate the name of the file. So name of the file I will just call it like you know like Lionel Messi 1.png Lionel Messi 2.png and so on. I want to keep it that simple. So then I need a count for one two you know so that's where I have count here celebrity name count dot png. Now count I have not initialized here. So I realize I need to initialize that in here. Okay. And this will be the name of the file and this will be the full path of that file. Now what we are going to do is this ROI color we will save that as an image in this cropped file path. Very simple. Now how do you do that in open CV. CV 2 dot I am right. Very simple. What is the first argument your file path second argument your region of interest. Whatever you got back from your cropped get crop image if twice that's an awesome function we wrote guys. We made a big achievement by writing that function. Although that function is a big achievement there is one short coming I will tell you so that you don't complain later on. If you have two images is going to return only first image. Okay. So if you want to make this more robust I'm kind of feeling lazy but if you want to make it more robust you can return to ROI colors as an array and then save those two images. But you know sometimes I feel lazy and I don't care about little details. Because guys understand my schedule is very busy and I'm doing this YouTube thing on side and I don't have time to go too much into details. You know although I try my best but you understand my situation. Okay. Enough of a side talk. Now once you execute this line your cropped image is stored in cropped folder amazing. But we need to do one more thing. We need to store the name of all those image file paths into a dictionary. That dictionary will be useful later on. So this is that dictionary. Celebratory file names dictionary. So what is this? So the key in this dictionary will be the name of the celebrity and the value will be the list of file paths. So it will look something like this guys. See. You'll say Leo. Leo. Now let's see. And it will have the file path. You know like data says crop messy whatever messy one dot png. Messy to dot png. You know you want this kind of dictionary so that it helps you later on. We'll see how it helps. But you know like you get an idea this is very easy. You are creating a dictionary where you have the path of all your crop images stored here in this beautiful looking dictionary. See this is the dictionary I'm trying to create that is celebrity file names to dig. Now here I can append but I need to initialize the key of this dictionary. Somewhere here. So how do I do that? Well you can just do this. Okay so what is this? So when a dictionary is empty the first key is Leo and messy and you are creating a blank array. So blank array will be this value. And once you have blank array you can insert all these image paths one by one one by one friends that's what I'm doing here. Alright now looks like my code is ready for big execution. So I just want to see that I'm not doing any bluff. I have only crop folder it is blank and now I'm going to control execute. So now it's executing the code. It is going through all the images generating the crop images. I paused this video while this was going on because we were doing some heavy lifting based on your computer speed it might take you few minutes. But after this execution is complete for this cell let me show you how my crop folder looks like. So crop folder you see now it has five beautiful subfolders in each of these subfolders I see the crop images see. I see crop images guys guys and girls crop images my life is pretty cool. See okay. Now one issue you notice is we have cropped images but then we have an image of Venus Williams she's a sister of Serena Williams we have this image which looks pretty blurry then we have this image I think this person is Serena's husband I think if I'm not wrong see. So what is this person doing in Serena's folder so what happened was this image so this image see it has this face so it just return that face into a crop folder. So using Python you can crop this images but you can do the data cleaning to only certain extent. Most of the companies they will try to do data cleaning in as automated way as possible but you have to rely on humans or to do some manual remediation. So many companies what they do is they will hire this workforce in countries where the labor is cheap some companies also use crowdsourcing platform. For example in our case we can easily use a crowdsourcing platform and we can throw these images as a crowdsourcing job and we can ask people that does this image look like Serena Williams and people will easily detect this and they will say no this doesn't look like Serena Williams in that case you can delete the image okay. So companies have this workflows where they use crowdsourcing platform or many different tools and assign these micro task to people especially in the countries where the labor is cheap to do manual remediation or manual cleaning of these images. Now our data is small so we are just going to eyeball and clean these images okay. So what I am going to do is go through each of these folders see just quickly it's okay we are not for a case of this looks like Serena Williams or Venus Williams or sister this image is blur so I'm deleting it so I'm just manually deleting it so now I'm done with my automated way of data cleaning now I'm doing manual data cleaning see this was a background image which it caught. See this is an image of someone else okay so Serena Williams folder is clean. Similarly see I look at Lionel Messi and I see this boy's image maybe this is Lionel Messi's son this another boy so I'm going to delete that. Virat Kohli my favorite player Indian Cricketer and I see this blur image I see this another Virat Kohli see I want to delete that guy and then see Anuska Serena like her wife his wife I see three images see one two three oh I see Dippy Kasper okay delete all those images and once you delete all these images you have a clean data set that you can use it further okay so so that's all I have for this tutorial in the next tutorial we are going to look at Wavelet transform and how we can use that to generate the features and we'll use Wavelet transform as well as these raw images for our model training later on if you're liking this series so far please leave a comment below because your comments kind of helps me in designing the future content in a better way so if you like projects like this if you want me to build these kind of projects please give it a thumbs up or comment below so that I know that there is a demand for doing these kind of projects and I can maybe focus more on these projects in the future thank you",
    "Machine Learning Tutorial Python - 16_ Hyper parameter Tuning (GridSearchCV) (1).wav": " In this video we are going to talk about how to choose the best model for your given machine learning problem and how to do hyper parameter tuning. Here is a list of topics that we are going to cover in this video. Let's say you are trying to classify SK learns iris flower dataset where based on the pattern and sample width and length you are trying to predict what type of flower it is. Now the first question that arises is which model should I use? There are so many to choose from and let's say use figured out that SVM is the model I want to use. The problem doesn't end there. Now you have hyper parameters. What kind of kernel and C and gamma values should I be using? There are just so many values to choose from. The process of choosing the optimal parameter is called hypertuning. In my Jupyter Notebook I have loaded iris flower dataset here and it is being shown in a table format. The traditional approach that we can take to solve this problem is we use train test split method to split our dataset into training and test dataset here I am using 7030 partition and then let's say we first try SVM model. So first I am going to show you how to do hyper parameter tuning and then we will look into how to choose the model. So just assume that you are going to use SVM model and using SVM model you can train the model and you can calculate the score. Here I randomly initialize these parameters. I don't know what is the best parameter so I am just going with some value. The issue here is that based on your train and test set the score might vary. Right now my score is 95% but if I execute this again, X train X test samples are going to change so it will change from 95 to now it change to 1. I cannot rely on this method because the score is changing based on my samples. For that reason we use k-fold cork cross validation. I have a video on k-fold cross validation so if you want to pause here and take a detailed look at you can go there but I will just give you an overview. As shown in the diagram what we do in a k-fold cross validation is we divide our data samples into n number of false. Here I am showing 5 false and we take 5 iteration in each iteration. One fold is test set remaining our training set. We find the score for that iteration and we take these individual scores from each iteration and we make an average. This approach works very well because you are going across all the samples. We have a method called cross-well score which can tell you the score of each iteration. Here what I have done is tried cross-well score for 5 fold so CV is equal to 5 means 5 fold and tried this method on different values of kernels and C. Here kernel is linear, here it is RbF, C10 and C20. For each of these combinations I found the scores. You can see there are 5 values here and these are the scores from 5 iteration. You can take the average of these and find out what is your average score and based on that you can determine the optimal value for these parameters but you can see that this method is very manual and repetitive because there are so many values you can supply as a combination of kernel and C. C could be 1, 2, 3, 100. For how many times you are going to write this line? The other approach you can take is you can just run a for loop. I am doing the exact same thing but using a for loop. I have my possible values of kernel and then C and then I ran a for loop on both of these and I am supplying those values here. You can see here K well and C well and then find the average scores. When I execute this I get these scores. With RbF and 1 the score is this, RbF and 10 the score is this and so on. Just by looking at the values I can say that RbF1 and the value of C being either 1 or 10 or linear kernel and C being 1 will give me the base score. You can see that these scores are low. This way I can find out the optimal score using the hyper parameter tuning. You can see that this approach also has some issues which is if I have 4 parameters for example then I have to run like 4 loops and it will be too many iterations and it is just not convenient. Luckily, Escalon provides an API called Grid Search CV which will do the exact same thing. Grid Search CV is going to do exact same thing as shown in this code here in line number 14. I am going to do the same thing but you will not that we will be able to do that in a single line of code. The first thing you do is you import Grid Search CV from Escalon model selection and then we will define our classifier. The classifier is going to be Grid Search CV where the first thing is your model. My model is SVM.SVC. I am supplying gamma value to be auto if you want gamma to be in your parameters you can do that but for this example I am just keeping it static. Now the second parameter is very important. Second parameter is your parameter Grid. In parameter Grid you will see I want the value of C to be 110 and 20. These are like different values that you want to try. The second parameter is kernel and you want to try the kernel and you want the value of your kernel to be RBF and linear. These are the two values. There are other parameters in Grid Search CV for example how many cross validations you want to run. Grid Search CV is still using cross validation. It is just that we are making this particular code blog convenient and we are writing the same thing in one line of code. CV is this. There is another value call return train score. This is some parameter that this method returns which we don't need. That's why we are saying it is false. Once this is done you will do model training by saying iris.data and iris.target and once that is done we will print the cross validation results. When you execute this you get these results. Now if you look at these results you will notice that you got this mean test score. CV results are not easy to view but luckily SQL learn provides a way to download these results into a data frame. Here I have SQL and documentation and it says that this can be imported into pandas data frame. So that's the next thing I'm going to do and all you guys are I think exports into pandas by now. So you just create pandas data frame and supply CV results as an input and when I run this I get this nice tabular view. Here you can see that these are the C parameter values then kernel values and these are the scores from each individual split. We then five full cross validation. That's why you get split 0 to split 4 and then you have mean test score as well. Some of these columns in this grid might not be useful so I'm going to trim it down and just look at parameter values and means go. So you can see that these are the possible values of parameter c and then kernel and these are the scores I got based on this I can say that I can supply of first three values into my parameters to get the best performance. So we already did hyper tuning of these parameters. You see that this how this works right now you can have many many parameters all you have to do is supply them in parameter grid and this grid search CV will do permutation and combination of each of these parameters using k fold cross validation and it will show you all the results in this nice pandas data frame. I can do dIR on my classifier and see what other properties this object has and I see some of the properties such as based estimator, best perms and based score. So let me try a based score for so CLF dot based score and the base score it is saying 0.98 which is well it's a 0.98 is the base score. I can also do CLF dot best perms and it will tell me the base parameters. In our case there are multiple parameters which gives you optimal performance but you can see the point you just run grid search CV and then call based perms to find out the base parameters and these are the parameters you are going to use for your model. One issue that can happen with grid search CV is the computation cost. Our data set right now is very limited but just imagine you have millions of data points into your data set. And then for parameters you have so many values. Right now C values I randomly took them to be 1 to 10 but what if I just want to try a range let's say number 1 to 50. Then my computation cost will go very high because this will literally try permutation and combination for every value in each of these parameters. To tackle this computation problem SQL library comes up with another class called randomized search CV. Randomized search CV will not try every single permutation and combination of parameters but it will try random combination of these parameter values and you can choose what those iteration could be. So let me just show you how that works. Here I imported randomized search CV class from the skill and model selection and the API kind of looks same as grid search CV. I supplied my parameter grid, my cross validation value which is again five full cross validation and the most interesting parameter here is an iteration. I want to try only two combinations. Okay here we tried total six. You see 0 to 5. So here it will try only two combinations and then we'll call fit method and then we'll download the results in two data frame. When I run this you can see that it randomly tried C value to be 1 and 10 and then kernel value to be linear and RBF. When I run this again it changed the value of C to be 20 and 10. This way it just randomly tries the values of C and kernel and it gives you the base core. This works well in practical life because if you don't have too much computation power then you just want to try random values of parameters and just go with whatever comes out to be the best. Alright we looked into hyper parameter tuning. Now I want to show you how do you choose a based model for your given problem. For our iris data set I'm going to try these two classifiers. Okay SVM, random forest and the logistic regression and I want to figure out which one gives me the best performance. You have to define your parameter grid and I'm just defining them as a simple JSON object or simple Python dictionary where I'm saying I want to try SVM model with these parameters random forest with these other parameters. I want the three value of random forest to be 1, 5 and 10 and this n estimator is an argument in random forest classifier. Okay similarly the value C is an argument or a parameter in logistic regression classifier. Once I have initialized this dictionary I can write a simple for loop. So I'm just going to show you that for loop here and this for loop is doing nothing but it's just going through this dictionary values and for each of the values it will use grid search CV. So you can see that grid search CV the first argument is the classifier which is your model. So here you can see the model is classifier. So it is trying each of these classifiers one by one with the corresponding parameter grid that I have specified in this dictionary. You can see that parameter is the second object, second argument and then cross validation is five. I then run my training and then append the scores into this scores list. When I run this my scores list has all those values and all I'm going to do now is convert those results into pandas data frame. When I do that I see a nice table view which is telling me that for SVM model I'm going to get 98% score random forest is giving me 96 and logistic regression is giving little more than 96. So here I have my conclusion that the best model for my iris dataset problem is SVM it will give me 98 person score with these parameters. So not only we did hyper parameter tuning but we also selected the best model. Here I have used only three models for the demonstration you can use 100 models for example here. So this is more like trial and error approach but in practical lives this works really well and this is what people use to figure out the best model and the best parameters. Now comes the most interesting part of my tutorial which is the exercise you have to do this exercise guys just by watching video you are not going to learn anything. So please move you but work on this exercise here we are going to take SK learns handwritten digits dataset and then classify those digits using the listed classifiers and also you're going to find out the best parameters for it. Post your answer as a video comment below and if you want you can tell your answer with the solution I have provided now my solution is not the best one because I just tried only few parameters so you should try more parameters and I hope you can find better score than me. So don't click on the solution link until you have tried it yourself. Thank you very much for watching this video if you like the content please give it a thumbs up subscribe to my channel and share it with your friends. Thank you very much I will see you next tutorial.",
    "Machine Learning Tutorial Python - 7_ Training and Testing Data.wav": " In this video, we are going to look into how to split your data set into training and test using AscalandTrainTestSplitMethod. Usually when you have a data set like this, sometimes we train the model using the entire data set but that's not a good strategy. The good strategy is to split the data set into two parts where you use part of the sample for actual training and you use remaining samples for testing your model and the reason is you want to use those samples for testing the models that model has not seen before. So for example here if I use the first eight samples to train the model and then use remaining two tests then I will get a good idea on the accuracy of model because the model has not seen these two samples before. The data set that we are using for this exercise is the BMW car or prices data set. Here I have the mileage and the age of the car and the price that it was sold for. So these are all the sold BMW cars along with their mileage, age and sell price and here mileage and age are independent variables and selling price is the dependent variable. In my Jupyter Notebook I have loaded this CSV file into a data frame which looks like this and then I'm using some matte plot lib visualization to figure out the relationship between my dependent and independent variable. So here I have a plot of mileage versus the sell price and you can see a clear linear relationship here. We can draw a line that goes through all these data points. Similarly for car age and sell price I have plotted another scatter plot and here also you can short of apply a linear relationship. So we are going to use a linear regression model based on these visualization. I have prepared my X and Y here. So X again mileage and age and Y is the selling price. The first thing we'll do here is use train test split method from SK learn dot model selection. We are importing train test split method and then use this method supply X and Y as an input and also you need to supply the ratio by which you are splitting. So here I want my test dataset size to be 20 person and my training dataset size to be 80 person. So this is how you specify that. As a result what you get is a X train dataset then X test dataset Y train and Y test. You get four parameters back okay and if you look at the length of whatever you've got back you will see it is 80% of your total data size. Your total data size here is 20 and based on 80% ratio my training dataset is 16 samples. Look at test and it will be four as you would expect it to be so. If you check the actual content of your X train you will see that it shows the random samples is not selecting the first 80% of the samples just using random samples which is good. If you execute this method again and again you will see that the samples will change here. Now sometimes you want your sample to remain same and for that you can use this random state method. If you use random state remember it's going to use same samples okay. So for random states value of 10 it will always produce the same output you can see that my X train is not changing now right 22500. You will see these values are not changing when I execute this multiple times. Whereas if I didn't have this parameter it was changing all the time so if you do control enter you can see that right okay now let's use our linear regression model. So you can guess that I am going to import linear regression class and create my classifier which is nothing but an object of this class and then you are use a fit method to actually train your model. Okay so I need to use X train Y train and my model is train now and now I can call predict method on my actual test dataset. So my model is predicting that the values predicted values for my X test are these let's look at how my Y test looks. So my Y test looks like this so values are kind of in similar range but not exactly. So let's check the accuracy of the model by calling score method. So what score method will do is it will use the X test and predict these values and then compare it against this Y test value and tell you the accuracy. So here accuracy is 0.89% okay that's just because of the nature of my dataset. But that's all I had for this tutorial you learned that how train test split can be used to split your actual dataset. So you can change your percentage ratio between the two dataset by modifying this. If I do this then it will be 70-30 ratio okay. So that's all I had for this tutorial. I don't have any exercise but I have this notebook available in the video description below so feel free to download it and play around it. Thank you bye.",
    "Data Science & Machine Learning Project - Part 6 Flask Server _ Image Classification.wav": " In the last tutorial we trained our model and exported it to a pickle file. It's called saved and discord model.pickle. We also exported class dictionary.json and you can see those files here. Now I'm going to copy these files into my server folder. So in this tutorial we are going to build a Python Flash server for our project. Typically if you are working in a big company data scientist will build the model. They will hand over this model to an engineering team. An engineering team will take care of building Python Flash server. They will also build the website and so on. So as a data scientist you don't have to be very good in Python Flash or even a website building. But if you know these skills, they are always helpful. So I copied those two files into server directory. Server directory right now is empty. I'm creating a sub folder called artifacts. So the popular term to describe these objects is called artifact. And these artifacts I am saving here in this folder. I have opened Python where you can see this server. And here I'm going to create a server.py file. So this is a simple Python Flash server. If you don't know about Flash, it's a micro web server. It's like a very lightweight web server that you can write in Python. And I will just quickly show you a very simple server. So first you need to import all these libraries. And then you can have your main method. And the main method will run the Python Flash server on this particular port. So now you can right click and run the server. Here it is saying it's not found because see if you have sometimes two Python installation, this issue might arise. So I'm going to change my interpreter. I have separate Python installation other than the anaconda interpreter that I have. So I'm just changing it to be my anaconda interpreter. Because that has this Flash module in it. Now when you call this server, you will use localhost. 5000 is the port. And I'm going to write a simple API called Hello in my Python Flash server. So here how do you write that simple API? Well you will say addapp.routeHello. And here you can just simply return some string. And this should be a method called Hello. So here you are returning this string. And let's run this. Okay so this will run a server. Okay I have this thing. This will run a server on 5000 port. And it will support an API called Hello. And you are returning Hi. So localhost 5000 Hello. And it is returning Hi. So writing Python Flash server is very very straightforward. So this is your very very simple Python Flash server. Now what we are going to do is add a new file called Util.py. You know you want to modularize our code. And for that reason the server will have a method called ClassifyImage. Okay so this method will be your ClassifyImage. And it will have get and post method. You can read about HTTP raised APIs that are get post these kind of methods available. And I will call this function as ClassifyImage. And this function will be doing the image classification using our saved model in the artifacts directory. So let's see what we want to put in Util.py. So in Util.py, of course I will add like usually you add a new main method to just perform your test first. And nothing about how UI is going to pass an image to our backend. Okay so in terms of architecture we have a UI, we have the backend. Backend is nothing but the Python Flash server that we are writing. And this Python Flash server is doing the actual image classification. But on the UI you drag and drop the image. And when you send this image to backend, the image at the backend will use that image for the classification. So the question is how can UI send an image to backend? There are a couple of ways. One way is you upload your image to let's say Amazon has three bucket. And send a link of S3 bucket in the HTTP request that you are making. Or you can send base 64 encoded string. Now base 64 encoding for images are very very popular. And many applications use this approach where you have an image. It will be converted into base 64 encoded string. So base 64 encoding is nothing but it's a way to convert an image into a string. Very simple. I will show you how you can convert an image into base 64 encoded string. So first what I'm going to do is I will create a test image dictionary directory. So here is my server folder. Here I created a test image directory and I have some test images here. And let's say for testing purpose I want to use this Virat Kohli's image. So first thing I want to do is I want to convert this into base 64 string. Because when UI when you drag and drop this image on the UI, UI will convert it to base 64 string and it will send it to a backend. So now backend needs to have access of this string so that we can do our core development. So there is this website. So if you do like Google convert image to base 64 you will find this website. And if you now drag and drop this image here, see it converted it into base 64. So now when you say copy image and when you copy this image here, you see you got this base 64 encoded string. So this big gibberish looking string that you are seeing is actually Virat Kohli's image. It is just converted as a base 64 encoded string. So I am going to save it as B64.txt and then I will... So I save that and now I will copy it here. So you can check this B64.txt and when you check it, what you find is it's a... it is this string. So let's use this base 64 string for our image classification. So now I am back into my pyjama and what we need to do here is write this method call, get base 64 test image for Virat. And in python you can open this text file as like this and it will return you that image as a string. So then once I have a string, I will write this method called classify image. So we are performing image classification of this B64.txt first and seeing you know what it returns. In classify image function, so this is a blind function. I have my image base 64 data and the second string I have is I am also passing a file path. You know, just in case if you want to pass the file, the image directly, the image path, then you can use this particular variable. Now by default it's initialized to be none so that even if you don't pass this, it's going to work. So my end goal would be to do this. So I am saying classify my image. This is Virat Kohli and it should print the classification result. So far it looks pretty good. Now we will write that function called getCropImageIf2i. So this is the function we had while building the model. So I am using the same function with little change. I am copying pasting this function. I am not going to write everything because it will save some time. So I hope you understand that I am going to also import some important libraries here. We will also need this wavelet function. So I am going to create wavelet.py file. So wavelet.py has the same W2D function that we used in our notebook. So in our notebook, if you realize, see we use that function if you remember. So it is the same function. Now we have another function called getCV2ImageFromBase64String. So that function also I am going to just copy paste. This function resource I got it from StackAllFloes. So what this function is doing is it is taking a base64String and returning you CV2Image. So if you look at the StackAllFloes question, it has all this code. Basically you are taking base64String and using numpy and CV2imdCode function to convert it into openCVImage. Okay, so let's look at this function. So this function getCV2ImageIf2Is is similar to what we did in our model building. So if you are not seeing model building tutorial, go what that it is the exact same function. What this function is doing is if the image has two eyes and a clear face, it will return that face in cropped faces or a. So if it has two faces, it will return two faces. If it cannot detect face or two eyes properly, it will return you an empty array. Okay, so we are looking good till this point. Now let's move on and... So now here I am seeing images is equal to getCroppedIf2Is and the file path and image base64Data. Okay, and as we just said, if you have clear face and two eyes, then it will be returned into this array. So we will write a full look for image and images. Once we get this image, this will be a cropped image. So we can resize it and we can again use the same code that we used in our model building. So let's look at our code in model building. See, in this code, once we have an image, we resize it. Then we converted it into a wavelet transform. Then we resize wavelet transform image and we did a vertical stacking of those two images. And that was our X. So we are trying to do exactly the same thing here. All right, so now we have combined image and the length of an image array is this. Because the first image is 32 by 32 into 3. Second image is 32 by 32. Second image is black and white. That's why it doesn't have a third dimension. Here it is third dimension is RGB. Okay. I hope you are following some of these code is taken from the model building part. So watch model building part and you will kind of get an idea. And our final image for the training is reshaping this. So I am reshaping it and doing as type as float because some of the APIs that we are going to use later, they expect float data type and hence I am converting it to a float. Okay. So once I have my final image, now I need my model. So where is my model? My model is in saved model.pcult file. I need to load that model. And for that reason, we will write this function called loadartifact. So what this function is doing is it is, I need to create two variables first of all. Actually three variables at the top. So here this variable will store model in Python. So here we have the model. So here we have the model. So here we have the model. So here this variable will store model. In Python when you say underscore model is like a private variable to that particular file, that's a coding convention. And I am initializing it to be none. In load savedartifact function, if you look at this code snippet, I am opening this file and loading this using a joblib module. So using joblib module, I am loading it and saving it into this variable. So now my underscore model will have the train model. We also have class dictionary. So that is nothing but player's name and some number. So that we need to load as well. I am loading two variables, class name to number and number to name. So if you have like zero, it will turn Leon on messy one, Marias Arapoa. And so one dictionary is doing that and the second dictionary is doing the reverse, which is player name to number conversion and the second dictionary is number to player name conversion. And this method, I need to call here at the beginning. Okay, so I am calling this method. Alright, now back to our classification method. So once I have final method, I can use model. So model has this predict function and in this predict function, you can pass your final image. And once you pass this final image, generally predict function would expect two dimensional array. It is expecting actually number of images. But we want to just supply one image. And for that reason, I am just saying, I am reshaping it to this array. Okay. So hence, when you get the result, it will be an array and I want to just get the first result. So this will return an array and the zero th one will be the first result. Okay. So this will be the class. You know, so I am just appending the result and returning the result here. Okay. So let's run this method and see what it is doing with Virat Cole's image. So you can right click Run Util. One thing I think I forgot to do was, and this is the array you will get whenever you don't have the cascade. So you realize here I am using OpenCv folder and these face detection cascades. Again, we covered this in our model building part. So I need to have those XML files in my server directory. So from my model directory, I copied OpenCv folder here. It has hardcast scale folder and all the XML files. So now I am running my code again and it is going to do image classification and it returned 4 actually. So I did image classification for this particular string. I got that string from this website. It was Virat Cole's image basically. And now my classifier is returning 4. So 4 is nothing but Virat Cole. If you want more meaningful output, you can convert number 4 to Virat Cole. And the way you can convert it is by writing this method. You know you can write this method where you can convert it from class number to name. So this will return, take an input of class number and return you the name. So if you want to test this method really quickly, you know let's say if I give you number 4, what do you give me back? I want to test that. So this is saying gear because I need to load artifacts actually. I forgot to print it so let's print it. So see I give 4 and it is returning me Virat Cole. If I give you 2, it will return me Marasara Power because that's the number that we have. O2 is Roger Federer. I see it. 2 is Roger Federer. So this method is working okay. Now I want to use this method. By the way in Pyjama you can do control click. It will take you that method. I only use that method here. So instead of getting number in the result now I want to put name of the person. So see now it is saying that the image is of Virat Kohli. Pretty good. I want to also get the class probability. Class probability is like you know how much similarity it has with Virat Kohli's face and with Roger Federer's face and all that. You can do that. If you have followed my previous machine learning tutorials there is this method called model.predictprobability. Okay. And this method will return you the probability of the similar face similarity score basically. And since I am returning multiple things from my function I am thinking maybe I should just create a dictionary here. You know so the result that I am getting will be a dictionary and the dictionary will contain my class which will be this. It will also contain class probability. So what is class probability? Well model.predictprobability. Final. Okay. Let's see what it gives back. Okay now let's look at this array. In this array I got this percentage score. See here it is saying 95% it is matching with Virat Kohli's face. Now these numbers are not very readable. So just to get it more readable I will multiply it with 100% so that instead of 0.95 I get 95% something like that. Also I will do numpy.round. I will just round it to 2 digits. So this will round the decimal points after dot to 2 decimal points. And I will also convert it to a list. Okay. When you run this output will look little better. So now see class probability. The last element is Virat Kohli's, matching 95% the element before is Serena Williams. Serena Williams is matching 1% I don't know like somehow it matches. See with the relational message it is matching 1.26%. Okay. So far so looks good. Now I also want to return class dictionary back to the UI. Because you know UI needs this so that it can map which number maps to what name. Running it again. Okay. So my output looks pretty good now. Now let's write a method in my server.py function. Before going to server.py let's test this method on couple of more images. So we tested it on base64 string it looks pretty good. Now I want to test it on an image actually. So what I'm going to do is you know I will take some images. If you remember we created this test images folder where we have Fader 1, Fader 2. So see Fader 1 looks something like this. Fader 2 is this. Vr1, Vr2, Vr1 is same as that. Vr2 is this. Okay. And the first argument was a base64 string. So this time we are not passing this 64 string. We are passing the file path. Okay. So let's run this method and see how it behaves. Okay. So for the first image it return me a blank array which means it was not able to detect the eyes and face properly. Now here eyes and face are clearly visible but I think the eyes is having a shadow. And sometimes this hard cascade might not work the best for all the images and that's perfectly fine. But you realize that for second, third and fourth image it actually performed well. Okay. Now let me try one more image. Now this image is interesting because I have Vr2 coales phase and his wife phase. So there are two phases. So let me just command this. Okay. Now I got some interesting result. It is saying that there is Serena Williams in the image. And there is the second one is. So if you look at the second one, see. First one is that. You know what? I will just debug it. So let me debug it. So here you can put a break point like this and you can right click debug. In PyChem you need to know this debugging skills. They will help you a lot during core development process. So let's look at the result. Okay. So I am having two results. So the first result is Serena Williams and second one is Virat Kohli. So you realize that when I had this image there were two phases in it. Virat Kohli and Anushka Sarma. So the first phase is, so it turned two phases basically. Virat Kohli's phase is detected. Okay. You can see that the similarity score is somewhere around 93%. See the last one is for Virat Kohli. But in case of Anushka Sarma, see we don't have classification of Anushka Sarma. But among all our players, maybe she looks more like Mariya Sarapua. You know her facial features are more like that. But this is saying Serena Williams. By the way, our image classifier is not looking at color. Okay. So if you just discard the color, if you look at facial features, you can still say, you know, there will be some, because these are all girls, they will have some similarity between Mariya Sarapua, Anushka Sarma and Serena Williams. But if you look at the class probability, 38%, it matched with Mariya Sarapua and 35% Serena Williams. So actually it matches more with Mariya Sarapua. But because of some reason, which I don't know why it is behaving like that, it is saying, final class, it is saying Mariya Sarapua. I was doing Google and I found an issue in scikit-learn library. So I'm just want to quickly show you. So this issue, I think it is because of this issue where you have some inconsistency between predict function and predict probability function. Okay. But don't worry too much about it. I think our classifier is performing okay so far. Okay. So let's see if our server.py is ready to consume this function. So now in server.py I have this classify image function. When you get, when the UI calls this function, it will send the image data in a request object. And this request object I have imported from Flask module. And this image data will be based 64 and coded string. Okay. So what you want to do is, you know, utl.the method that we wrote, what was the name of this method, classify image and pass that image data. Now here I need to import utl. And once I get the result, that result I want to JSONify. JSONify will convert it into JSON basically. And JSONify something I have imported from Flask module. And I will just call it response and this response. So I will add access control, allow origin here and then return the response back. Okay. So that's all we have for this tutorial. We are kind of done writing our Python Flask server. We wrote this method. We tested utl.py method. It is working okay. We still have to test this classify image method. But we will test this image from UI directly. The other option you have is a postman in the Bangalore property.py prediction project that I did last time. I use postman. So you can of course use postman to test this if you want. Just, you know, pass base 64 and coded string and it will tell you the result. But I'm going to just test it directly from the UI. And by the way, when you start this server, of course, you need to first actually load the artifacts. So artifact will load the saved model into memory so that you can do prediction. Okay. So this line is extremely important. Don't forget that. And I will see you in the next tutorial with the UI code. And with that, we will complete our project and we will have end to end website ready for sports, sports and classifier. I hope you guys are enjoying. If you like this project, please give it a thumbs up. Share it with your friends. Subscribe to my channel. It really motivates me when people comment on my project. How they like the project. If there is something they want to add or you know, there is something they want more clarification on. Now I understand I went a little fast because you know, if I try to explain each and every method is going to take forever. So I'm assuming that you know some basics and I'm just continuing with that assumption. Thank you. Bye.",
    "Data Science & Machine Learning Project - Part 5 Training a Model _ Image Classification.wav": " Hello everyone, now we are all set to train our model. In the last tutorial, when we are going through cropped images, we couldn't find some of the images because we deleted them, we couldn't find them in our class dictionary. So to improve the code, what I did is, and I'm going to provide this notebook link in the video description below. And I went through cropped image directory and I scanned those images and rebuilt my dictionary. So these dictionary contains all the valid files now. And after that, when I reconstructed x and y, I got 168 images and each image is nothing but an array of 4,096 data points. So every image is represented as an array, like one dimensional list. So it is as simple as that. Now we are ready to train our model. So we will try support vector machine because support vector machine tends to perform good when it comes to when we are talking about classification. We'll just randomly try that, then we'll try couple of other models using grid search. So the goal of this tutorial is, we first try SVM, look at the performance, then we use grid search CV, try couple of models, do fine tuning, hyper parameter tuning. And in the end, once we have decided which model we are going to deploy to production, we will export that model in a file. So we'll cover all of that in this tutorial. So let's first import necessary modules. So I imported all these modules. And as usual, the first thing is to split x and y into x train, y train and so on. You all know about this. If you don't know about this, then follow my machine learning tutorial playlist. That playlist is a prerequisite for this project. This is a standard practice. We always divide x and y into training and test set. After that, I'm going to create a scale on pipeline. The reason I'm doing it is because I want to scale our data first. So this is nothing actually, it's very simple. You first, whatever x train you are giving, you are scaling it using this standard scalar. You can also use minmax scalar. And then in the second step, you are creating an SVM model with certain parameters. Now don't ask me how I came up with this parameters. I'm randomly choosing this parameter just to kind of get a feel of how my model is performing. I'm going to fine tune this parameters later on using grid search CV. And once you have the pipe created, you can just simply say pipe.fit. And what this fit will do is it will train machine learning model on x train and y train. And once it is train, I have this habit of checking the score on x and y taste. So x and y taste, you can check how good your model is performing. And when you execute it, what I find is I find 88% accuracy. So you can see that 88% accuracy you can get even if you are not using neural network. So this is kind of good. I understand we tested it on only a small sample of x taste and y taste. And you can check what is that sample size. So we tested 42 images basically. And on that it gave us 88% score, which is kind of okay. If you want to look at the details statistics, then SK learn has this classification report. So what this classification report will do is it will try to predict using x taste. So you get y predicted and it will compare it with y taste and it will give you different metrics. 0 to 4 are different classes that we have. Right? So what are our classes? So if you look at it, 0 is learn and messy. Maria Sarapu, wise one and so on. And we now have precision and recall. So if you don't know what is precision recall, you can say, okay. So precision and recall. Just search for f1 score and you will find nice Wikipedia article on it. And that article has information on what is precision and recall. Look if you are trying to learn data science machine learning, you need to know about all these terms. Precision and recall precision is like I'm predicting. Let's say there are 10 images and I'm saying these 10 images are of Lionel Messi. And out of that, let's say one image is not Lionel Messi. In that case, my precision is 90% because 90% of time, whatever I'm telling is true, but one person time I'm making mistake. And recall is something like, you know, this article, Wikipedia article gives a nice description like how many selected items are relevant out of the entire set, positive and negative. And how many relevant items were selected? Like for how many items were able to predict even. And then f1 score is just to score around this precision recall. And you can look at all this mathematical formula. So you can see for every class, you are showing this precision recall. For class and 1, 2, my precision is 1, which means what is my 1 and 2 class? So 1 and 2 is Maria Sarapua and Roger Federer. In these cases, it did not make any mistake. So if it says that it's Maria Sarapua, it is Maria Sarapua and so on. So you can read more about this classification report and all and you know, just get understanding on it. Now we'll do grid search CV. So grid search CV is used to hyper tune parameters. For example, here I use RBF and C equal to 10. How do I know that this parameter will give me best performance? There is no good way to know other than trial and error. So what we do is we train SVM model on RBF kernel, linear kernel on different values of C such as 1, 10, 100 and so on. And we rely on grid search CV to tell us which model is performing the best. And I have a grid search CV tutorial. I highly recommend that you watch that that tutorial is a prerequisite for this class. So that will give you a good understanding. So in grid search CV, what I'm going to do is I'm going to define different candidate models. What I mean by this is that I have, so I have this different model that I want to try SVM render forest and logistic regression. And I want to try these models with these parameters. So I want to try SVM first with the value of C being 1, 10, 100, 1000, then the kernel value being RBF and linear. Similarly I want to try random forest with number of estimator, which is like number of decision trees. The value of those would be 1, 5, 10. So this is just a dictionary that I create. And then I can actually run my random forest using this code, not my random forest, but my grid search CV using this code using this code. So what is this code? So this code try to I trade through this dictionary. So we go through each model first of all for every model. We try all these parameters. And what is the model? The model is this. And by the way, I'm also creating a pipeline because you need to scale your data first. So here I'm creating a scale learn pipeline. So pipeline is step one is scale the data, step two is use the model to train. And in grid search CV I'm using 5 cross folds. So what this will do is like if you have 100 samples 80 like 1 to 80 it will use for training and 20 it will use for test. In the second iteration it will use 1 to 60 and 80 to 100 for training and then 60 to 80 for test. It will do this for all 5 folds and then it will average out the score. I have another tutorial on cross validation. So go watch that tutorial again that tutorial is also a prerequisite. So you need to have all this concepts clear in order to work on this project. And if you don't have those concepts clear you can pause this video. Go to like search in YouTube or code basics machine learning tutorial playlist try to find cross validation tutorial look at it and you will get an idea. After I train using each model using grid search CV I am appending the scores here in a list and then I'm creating a data frame out of it. So my data frame is telling me this that SVM is giving me the highest score you can see 84% logistic regression is performing well random forces not that good. And within SVM the best score is given by these parameters. So if you have value of C being 1 value of kernel is linear then you get the best score. So now I have all this base estimator. So see let's see what is this base estimator. So if you look at my base estimator for SVM for linear regression and for logistic regression which is here I got this base estimator function a base estimator. Basically that estimator is a train model. And now see what we did if you think about this loop that we ran. You might have noticed we use X train by train here. So there is this concept of training data, test data and validation data. So here in this full loop when I have X train internally grid search CV will split that into training and validation set. And I'm using this validation set to decide the performance of these models so that I can hyper tune my parameter. So for hyper tuning parameter I'm using validation test and to test the performance eventually I'm going to use X test. So you can kind of get a difference between validation set and the test set here. Okay so now I have the X test see X test why it is I have not used yet. So I'm going to now use that. So when you do base estimator for example SVM it gives me the SVM model which gives me the base performance and it has the base hyper tune parameter. And when you do X test why test you get 90% score. So see this 84% score that you got was on validation set. Now I am testing on test set. Okay now let's try random forest as well. So that is giving me a horrible score actually. So of course I'm not going to use random forest and there is logistic regression which is giving me 92%. But right now you would tell me should I use logistic regression or should I use SVM? Because when we ran across full validation SVM gave us the base score. But when I'm running it on X test why test logistic regression is giving base score. Now there are there is no one answer to this. There are different practices that people follow. Some people say okay X test why test gives me ultimate idea on how my model is going to perform in the production. And therefore they will use logistic regression. There are other people who will use the grid search CV score along with the X test Y test score to get a feel of which model they should deploy. I will personally use SVM because SVM performs best when we did cross full validation on five different data sets. And also during X test why test is still performs good. You know it's like 90% 1992% is not very much difference. Different but you know here it gave me like 4% more accuracy. So I will use SVM you can use logistic regression as well. I mean there is no like one definitive answer to this question. Alright so now that we have decided we are going to use SVM I'm going to store that into my best classifier. So my best classifier is this it's a trained model. And now I'm going to use just to give you an idea. I will use a confusion matrix. So if you have followed my machine learning tutorial playlist I think some of these tutorials I talk about confusion matrix which you should be aware about if you are not again go once that tutorial playlist first. So confusion matrix will tell you basically like you know let me plot that. So this confusion matrix is okay but it's not visually very appealing. So we can use a Seaborn Python library to plot confusion matrix in a nice way. So it's the same grid by the way. It's just a fancy visualization. What this visualization is telling you is this. So let's print class dictionary so that you can get an idea on what this is trying to tell you. So Lionel mess is zero and Maria Sarah Poise one. So let's look at zero first. So the y axis is truth the axis is predicted. So if you look at this diagonal it will tell you on how many samples our model predicted correctly. So when you see seven here in zero zero so diagonal is zero zero. So seven times out of our 42 test samples seven times it was Lionel messy and it predicted it to be Lionel messy. And if you look at this column it's all zero which means for Lionel messy it did not make an mistake. Okay to good. In the case let's look at this case. In the truth was three. So when it was Serena Williams on one occasion so this one means on one occasion it said it's Maria Sarah Poise on that occasion it made a mistake. This 11 here means see 11 the x axis is four. Truth is four means it's Virat Kohli so 11 times it was Virat Kohli and it actually said that it is Virat Kohli. Okay so you kind of get an idea on the confusion matrix. Now it's time to save our model to a file. So we are going to use joblib. If you don't have joblib install if you write this line in your Jupyter Notebook if it is not installed it will install it. If you run this all you need to do is the base classifier you supply the first argument and the second argument will be your the name of the file. So I'm saying okay it's a pickle file say your model. And then I'm also going to save class dictionary because these numbers will be useful when we write Python Flask Server. So I do this and if I look at my folder now. So let's look at the folder. So see I got these two files. So class dictionary is nothing it's just a JSON object. Okay so class dictionary would be this. Okay and if you look at saved model it's like a binary encrypted file. So if you don't worry about it this model will use in Python Flask Server to make our actual prediction. So at this point the job of data scientist is over. Data scientist has built a very good model which performs around 90% accuracy or 90% it gives 90% score. And at this point data scientist will go and hand over this pickled model and this class dictionary dot JSON file to engineering team and say hey engineering team I have this model can you guys incorporate into your software application. And engineering team will then write Python Flask Server and the website which will do actual prediction. So in the next tutorial we are going to build the Python Flask Server. Thank you for watching.",
    "Machine Learning Tutorial Python 12 - K Fold Cross Validation.wav": " Sometimes we get into this dilemma of which machine learning model should I use to solve my problem. For example, we worked on this Iris Flower dataset problem. Now, you can classify those Iris flowers using SVM, random forest, logistic regression, decision tree. Which model out of these is the best? Cross validation is a technique which allows you to answer the exact same question. It basically allows you to evaluate a model performance. When you are looking at machine learning models such as classifying emails as spam or not spam, your typical procedure is you first train the model using whatever label dataset that you have. And once the model is built, the next step is to use a different dataset for testing your model. And your model will basically return the results back. And then those results you can compare it with the truth to measure the accuracy of a model. Now, there are several ways you can perform this training step. The first way is all the training dataset that you have available, you just feed that to your model. So you are using 100% of all your samples to train the model and then you use the same exit samples to test the model. This in real life can be compared to an example of preparing this cute kid for a mathematical test. So let's see you have 100 mathematical questions. You train this kid for those questions. And then when he goes for the exam, you ask the exit same questions. And then you try to measure his mathematical skills based on the score. Now, this is not a very good way of measuring someone's math skills because he has already seen those questions before. So what if he gets like 100 out of 100? There's no point. He has already seen them. So then the second option we have is we split the samples that we have available into training and test datasets. So for example, here I will out of 100, I will use 74 training and 34 testing. And we have been using this trained test split method in all of our supervised learning model tutorials. And you will see that we have used trained this split method in I think majority of our tutorials. So that's what this technique is. So again, going back to our student example, you give this person 70 math questions for preparation and the remaining 30 you will reserve for the test. So that way when he goes for the test, he has not seen these 30 questions, which is good. That way you can measure the skills in a good way. But there is one problem with this approach also. Let's say 70 math questions that you give this person, let's say they were all algebra. And now remaining 30 questions are from calculus. So now those questions he has not seen before or he doesn't have knowledge on that topic itself. Then he might not perform well. So this technique is also it kind of works okay, but it's not like very perfect. That's why we have K fold cross validation. Now in this technique, what we do is we divide our 100 samples into folds. So I have five folds here each containing 20 samples. And then you run multiple iteration in the first iteration. You use fold number two to five for training the model and the first fold for testing the model and you're not down the score. Second iteration, you use first fold and then three to five for training and the second one for testing again not down the score. You repeat the process till the last fold where you use fold number five for testing and remaining for training. And then once you have every these scores, you just average them out. This technique is very, very good because you are giving variety of samples to your models and then you are taking individual scores and then averaging them out. Here I have imported necessary libraries that I'm going to use in this tutorial. And the purpose of this coding exercise is to classify the digits data set, which is an ask a learn library. You probably know about this. These are like handwritten characters that we can classify into one of the 10 categories 0 to 9. We will use different algorithms, different models and then we'll evaluate performance of each of them using K fold cross validation. Here I imported my digits data set and the first thing I'm going to do here is split that data set into training and test data set. This is something you might be aware that using trained to split, you can split into our training and test data sets. Once you have that, we can use a different classifiers. For example, the first classifier I'm going to use is logistic regression and I'm going to measure the score. Here I created a logistic regression classifier. Classifier is basically your machine learning model, which is trying to classify your samples. I trained it using Xtrained Ytrained and then when I tested the performance using the score method, it returned me 0.959, which is pretty good. Now I use logistic regression, but I have many other models as well. For example, SVM. I want to try on SVM, how SVM performs. You can see SVM performs pretty poorly. The score is very low. Compared to logistic regression, I can try random forest as well. In random forest case, it is performing the best. This was a quick way of measuring the performance of these three models. Logistic regression, SVM and random forest classifier, we evaluated the performance and we found that the random forest classifier is performing the best. This works in a practical situation, but what happens is the distribution of samples in Xtrained and Xtaste is not uniform. For example, when I run this method one more time, now my samples change totally. Now when I execute this code using control, that is a shortcut in Jupyter Notebook to execute your cell, you will see that the score will change. The previous score was 9.959, when I execute it, 9.53. So I change a little bit. Here is 0.40, when I re-execute it, you can see it becomes 0.62. Now why did that happen? Because when I re-executed this cell, Xtrained, Xtaste, Ytrained and Ytaste got changed. The samples that we put into these four sets, they got changed. Hence you expect the performance of your models to change. When I execute this, this is still performing better, but you see that previously score was 0.97, now is 0.98. So you see the problem with TrainTestplid method that you can't run it only one time and say that particular model is better than the other model. You have to run this multiple times. So see if I run it multiple time, every time it is changing. Now let's try Kfold. First what I am going to do is use Kfold API to demonstrate what Xtrained is doing. So from SKLON.modelSelection, you can import Kfold. And KF is equal to Kfold. Here you can specify how many folds you want to create. So I want to create less than three folds, just as an example. So here it created that. And the way you use this Kfold on the data set is you will say something like from TrainDex, TasteIndexInKF.Split. Now my Kfold is ready. We know that it is going to make three splits. So here in the argument, you can supply the data set. So let's say for simplicity sake, I just want to supply number 1 to 9. And then I can print TrainIndex and TasteIndex. So when I run this, what exactly is happening is this will return an iterator. And that iterator will return TrainIndex for each of the iterations. So it divided this into three folds, three each. And the first iteration, it used one fold for testing, which is this. And remaining two folds for training, which is this. In the second iteration, it moved this fold into training. So you can see 0, 1, 2 is in a training set. And then this fold into testing. And it repeats the procedure like that. So you can hear supply 10 folds also and it should work accordingly. Now we are going to use Kfold for our digits example. So to simplify the things, I'm going to write a generic method called Gatskord, which can take a model as an input, then X TrainX TasteY Train and Y Taste. And I'll tell you what's the purpose of this method. So this method calls model.fit, which means I want to train my model using X Train and Y Train. And once the training is done, it will return the model score using the TasteSample that you are supplying as an argument to this method. Now this method is pretty powerful. We could have used this method to measure performance of these guys as well. So for example, let me just quickly show you. If instead of doing, you know, repeating all these three lines for each of the model, I could have just call the Gatskord method here on X TrainX TasteY TrainY Taste. And it should have returned me the score. So same way I could have done SBC here. So if you do SBC, it's just modularizing our core. So once we have this method ready, I'm going to now use K-Fall on our digits dataset. So from SK-Lun.model selection, this time I'm going to use stratified K-Fall. So stratified K-Fall is similar to K-Fall, but it is little better in a way that when you are separating out your folds, it will divide each of the classification categories in a uniform way. And this could be very helpful. Of course, imagine you are creating three folds, for example, our IRIS flower dataset and two folds have two type of flowers and the one fold has just very different type of flowers than it might create problems. So this by using stratified K-Fall is better. So here I am going to say my N splits is equal to three. People usually use 10 N splits, but just to keep things simple, I'm using three here. And once you have your folds ready, so this method is exactly same as this. So K-Fall and stratified K-Fall is same thing. So we are repeating kind of the same thing that we did in these two lines here, but we are using now our real example of digits datasets. And I will prepare the scores array to prepare the scores of our different models. So L means the logistic regression scores and then random forest score. Okay, so I need these arrays and I will tell you why I need this array. So here, see same thing. We are doing the exact same thing here. Okay, but instead of this dummy data, now we are using our real digits data. Okay, and what will now happen is in our digits data, we have train index. And then again, in digits data, we have our test index. And these things, I'm going to store in X-Train, X-Taste, Y-Train, Y-Taste. Okay. Digest.data. So we have train and test index. Now I want Y-Train. So Y-Train is Digest.target. Okay. So Y-Train, train index actually. And then in the digits.target, now I want test index. Okay. If you do this, this, and if you look at the length of each of these sets, you will realize it is doing exactly same thing as it did in this cell number 26. Right now, it's a time to measure the performance of three of our models in each iteration. So since we have three folds, this four loop is going to repeat three times. Every time we'll take this different X-Taste and X-Train and Y-Train and Y-Taste, and we'll measure performance of our model. And then we'll append the scores in these arrays. Okay. So that's what we're doing. So let's first start with get score method. Get score method, as you know, it will. The first argument that it takes is the model. Then it takes X-Train, X-Taste, Y-Train, and Y-Taste. All right. Let me just print this. And I can do the same thing for three different models. So the second model is SVM and the third model is random for a classifier. When I print the scores, it is printing the different scores. Okay. So first iteration, these are the three model performance, second iteration, and third iteration. Now instead of just printing, why don't we append the same score here? So instead of sprinting, I'll say append that exact same score here. Scores SVM.append. And scores RF.append. All right. So now my scores should be ready. So I want to print them. SVM looks like this. You can see the SVM performance is not that great. And we have seen the same behavior before. All right. So logistic regression and random for us looks to be performing similar almost right on one instance. This guy performed little better. But what you can do is now you can take the average of these three scores and you can determine what is the best model for your given problem. Here it looks like based on this, it looks like our logistic regression model might perform the best. One optimization I can think of doing here is increase number of trees in my random for us classifiers. So if I increase the trees to 40, let's see how it performs. All right. I increase the trees to 40 and my scores are ready. So now I will once again evaluate this course. Nice. So you can see now random for us seems to be doing better because in 95 score matches here, but here is 8991 and here is 92 and 92. So now after I did little tweaking a parameter tuning in my random for us classifier, my scores improve. Now this code looks little messy because we have to deal with so many indexes. But luckily, as you learn library comes up with a ready made method called cross wells core, which you can use to do the exit same thing that I did here. So this thing I wrote this code just to explain you how K-fold works, but in real life, when you're solving machine learning problem, you don't need to write this much code. You can just call a cross wells core method, which I can demo it here right now. So to use that method, you can import the method from escalon model selection. It's called cross wells core. Okay. And once the method is imported, you can call this method with first argument is your classifier. The second argument is your x, which is digits dot data. The third argument is your y, which is digits dot target. So if you do shift tab to look at the documentation, it says estimated, which is a model, the second argument is your x and third argument is y. So that's what we did. And when you execute this line, this is showing you the similar score, basically, right. That score is being shown here. So internally, this method did the same thing as this whole look basically it created folds and it measure the performance and it prepared this course array. Okay. I will now call the same method on my SVM classifier and see how that goes again, SVM didn't perform really well. My accuracy is 39% 41% and so on versus here 89, 94% it's much better. And in the third case, I'll just copy paste this code here. So see, all you have to do is just call one line man, it's machine learning is not hard. If you know the internals of these libraries, then all you need to do is just just call one single method and say it's measuring the performance. Now, we compared different classifiers, you can compare same classifier with different parameters. This is called parameter tuning. For example, we have random forest classifier right here and random forest classifier we ran with 40 trees. We can actually around the same classifier with let's say, five trees and get the score. So you see, this is 40 classifier, this is the score with five classifier, we say the score went down a little bit. Let me try 15 for example and the score went little bit up. So looks like as I increase my trees, my score is increasing. So how would if I make my trees to be 50. So with 50, it increase even further 60, I think 50 was better than 60. So this way, you can take a model and then you can do parameter tuning. So you're using the same algorithm basically here random forest classifier, but then you are tuning the parameters and trying to gauge which one delivers the best performance. So you can see that cross validation technique is pretty useful. It not only allows you to compare different algorithms, but the same algorithm with different parameters, how it would behave for your given problem, it can tell you that. So machine learning is not like scientific equation where for a given problem, you use this model versus that model is something little bit like trial and error based where for your given problem and given data set, you need to try various models with various parameters and then figure out which one is the best for your use case. Alright, now comes the most interesting part which is the exercise. So what we want to do this tutorial is take the iris flower data set and use different classifier random forest, this is entry SVM and logistic regression and use cross wells core method to measure the performance of the base classifier. So for example, if I'm a teacher and I'm telling you to solve the classification problem of iris flower, which model will you use. So you'll use a cross wells core with these four different models and you will tell me which model performs the best. Alright, that's all I had for this tutorial. I have a link of Jupiter notebook used in this tutorial as well as exercise in the description down below in the video description section and stay tuned. I'll be back with the next machine learning tutorial pretty soon. And if you like my content, please subscribe to my channel. Please give it a thumbs up. It helps me with the YouTube search ranking. So please do that and thanks once again. Bye.",
    "Machine Learning & Data Science Project - 2 _ Data Cleaning (Real Estate Price Prediction Project).wav": " In this video we are going to download our data set into pandas and then perform data cleaning using some different techniques. Here I am on kegel.com. I have provided a link to this data set in the video description below. You can go to this link and click on download button to download this data set into a CSV. I have the CSV file here. My data set looks something like this. Where I have area type, availability, location size, the square foot area etc. All of these are called independent variables. My dependent variable is price. Price is something that I am trying to predict. The price here is in Indian rupees, lakh rupees. This is 39 lakh rupees, flat. This one is one crore and 20 lakh rupees. Remember that we are using supervised learning here. In supervised learning you need tagged or labeled data set. Tag or label data set is basically you have your input value and your output value. Based on that you are trying to build your model. If you are not installed it already please install anaconda distribution. This comes with data science, technology stack which is Jupyter Notebook, pandas, python, escalern etc. I have installed that already and I have created a new Jupyter Notebook here. An imported few basic libraries that will need for this tutorial. Once these libraries are imported you should read the CSV file that I just had open into a pandas data frame. Here you can see that I have my data frame loaded with all the data from the CSV file. When you run shape it shows you number of rows and number of columns in my data. I have 13,000 rows which is decent enough data set. Now let me first examine the area type feature. What I am going to do here is I will print a count of data sample in each of these area type categories. The way you do that is you group by your data frame by area type and then you aggregate the count. You can see that you have 87 samples where the area type is carpet area and these are different numbers. Now I am going to keep my model very simple and for that reason I am going to drop certain columns from my data frame. As such all of these columns are important but I am going to drop less availability. I am going to just assume that availability is not important in deciding the final price. Then society, then area type etc. This is how you drop those columns and let me do DF2.head. After I drop those columns my data frame looks something like this where it has only locations such as Tutorsquare, Footpath, Bath and Price. Now begins the data cleaning process. Data cleaning process starts with handling the NA values. For that I will use EZNL function. On data frame when you run EZNL function and when you do dot sum it will tell you the number of rows where particular column value is NA. For example I have Tutorsquare 73 rows where the value of bathroom is not available. I am going to drop all these NA values. Now you can actually do something else if you don't want to drop the column. For example for bathroom if I don't want to drop this 73 columns what I can do is I can take a median of bathroom values and fill the NA values with that median value. But since my data set is 13,000 rows and the NA rows are pretty small in number I can safely drop those rows. After dropping them and by the way for dropping you can use drop NA function. When you drop them and create this new data frame called DF3 you will find that this doesn't have any NA rows now. Once that is done I am going to explore the size feature. You can see here in the size that some values are 2bhk but then there are some rows where it says 4 bedroom. So I want to really see what's going on with that particular column. So I will do size and DF3 size will give you a panda series on which you can call unique function that will give you all unique values. Now you can see that 4 bedroom and 4bhk they are essentially same. So to take into account this kind of problems with the data set I will create a new column called bhk and the way you create a new column in a data frame is simply you can say something like this. And this column is created based on the size column and what you want to do on this size column is apply some function. Now what type of function you want to apply? You can take the string and you can tokenize it using the space and take the first token which is 2. So for example here is 2 here is 4 here is 3 and that will be your bhk value. And to do that we can use python lambda function. Lambda X will contain the column value for each of the rows one by one and on those value you want to apply some transformation. Now what kind of transformation you want to apply? So first let's say this X is 2bhk you want to split this string using space. How do you do that? X dot split so you do explode X dot split and space. This gives you two tokens out of which you want to take first token which is 0. And so let's say by doing this you are getting 2 or 4 etc. But that is still a string which you want to convert to integer. Okay so we did this and now when you check our data frame you see this beautiful bhk new column being created where it has number of bedrooms in your apartment. Now when you do unique on this bhk column which is newly created column you will find that you have these many different values of bedroom hall kitchen. Wow there are homes with 43 bedrooms in Bangalore and I want to see like what kind of home that is. So you can do df3 dot bhk let's say show me anything which has bhk greater than 20. And you find two homes like one home is 43 bedroom but the total square foot is 2400. See this looks like an error to me you cannot have 2400 square foot home with 43 bedroom. Okay so we are going to clean up these errors little later in order to tackle these errors actually I need to explore this total square feet feature as well. Okay and first thing I want to make sure is that this feature is available in a single number. Okay so first let's quickly check something so I will do df3 dot total sqft dot unique for example. Okay while doing that one thing I noticed is see sometimes I see this kind of value which is not just a single value it is a range. I want to convert this into a single number and one of the ways is I can take an average of these two numbers. Okay so first let me see what kind of variations I have in this total square foot feature. And in order to do that what I can do is I can detect if a given value in total square foot column is float or not. So first I will define a function called ease float. Okay and the way this function works is I will try to convert a value in total square foot column into float. And if it is not a valid value such as this range it will come into this accept block you know it will throw the exception when I try to convert into float. And here I can return false otherwise I can return true now I have this df3 data frame on which I will take total square foot column and apply this. This ease float function when I apply this ease float function it will return true but I want to look at the values where it is not a valid float number. So for that I can do this one this is a negate operation this will return a data frame back to me you can see that see and if you want to look at just few values you can do dot head. Here you can see I got all these ranges okay if I look at more values I also see cases like this where the value is in square meter it is in porch. So you can see this is a typical problem in any data science project where your incoming data is not uniform it is unstructured it contains outliers it has data error it has lot of problems. And majority of the time that data scientists spend is on data cleaning so we are doing that data cleaning right now. So to handle these non-uniformities what I'm going to do is anytime I have this range I will take an average of these two numbers. Any time I have values like this I will just ignore those rows okay now if you want to make your model later more sophisticated you can do a unit conversion between square meter into square feet you can also convert porch into square feet etc. But I'm going to just take out these range values okay and for that I'm going to write a python function which takes this range as an input and it returns the average value okay. So that function I'm just going to copy paste here that function looks like this where it takes input string we split it using this sign and then if the tokens are two we convert individual tokens into float number. And then we take the average okay otherwise we if let's say the number is your normal number then we convert it into a float okay. So let's taste to this function so first if you give this function a simple string let's say 2166 that it converts into float because it will go into this line here and it will convert. And let's say if you take this particular range you see what it did is it took the average of these two numbers and if you have cases like this let's say square meter let's see what it will do with that guy. So here see it did not return anything which is fine so now I will apply this function onto my total square foot column and create a new data frame okay. So remember that we are creating a new data frame at each stage in our data processing pipeline okay. So until now we had DF3 now I'm going to create a new data frame called DF4 which is a copy of DF3 when you do DF3.copy it will create a deep copy of your original data frame and on this data frame my total square foot column will be equal to DF4 total square foot on which I will apply. Now in apply you can write a lambda function or you can write a native Python function so I'm just going to apply this function okay. After you apply the function you can do DF4.head okay you can see that I had total square foot here now if you want to look at particular index. So here I had 212850 and the index was 30 so index 30 you can access by saying DF4.ploc location basically and you can see that my total square foot is 2475. Because I had 212850 and if you take the average of these two numbers I'll just take an average of these two numbers and that will be 2475 okay. So this looks pretty good so so far I have cleaned up my total square foot column I have also handled my NA and I have removed some unnecessary feature so my data frame looks much better compared to what I loaded it up with in the initial phase okay. So that's all I had for this video in the next video we are going to cover some of the feature engineering and dimensionality reduction techniques.",
    "Data Science & Machine Learning Project - Part 7 Build Website _ Image Classification.wav": " In last tutorial, we built a Python Flas server in this video. We are going to build a website or UI or a front end for our spot persons classifier project. We will be using a simple HTML, CSS, JavaScript to build the UI which will make HTTP call to our backend using jQuery. So that will be the architecture. In terms of my folder structure, I had model and server folder. I created this new folder called UI. In that, I have already added my app.css, HTML and JavaScript. Now if you are not familiar with this, then these three are called the Holy Trinity of Web programming. You always have an HTML file which will determine the anatomy of your application, different button controls and layout, etc. CSS will define the style guide which will contain information on the color, look and feel, font, button size, etc. JavaScript will have a dynamic code where if you want to make a call to backend, all of that code you will be writing in .js file. I also have this drop zone control file. So drop zone is nothing but, so let me just run it. So when I double click on this, I am double clicking on app.html and it is showing with this UI. And drop zone is this control where I can drag and drop an image and it will read that image. Now let's go over HTML to kind of see the anatomy of our application. You can right click here and say inspect. And it will show you the HTML file that I have created. So see HTML file has all these icons for the players. So there is my RSA power. Let me just expand this little bit. See there are all these icons. And then here I have this drop zone here. Now I had these two controls result holder and class table which are not visible right now but they will be visible once we have a classification result. So using Chrome inspect tool and the debugging tool is going to be very very important. I have opened my code in a Visual Studio code because for web programming I like to use Visual Studio code. It's an excellent code editor. You can even use that to write Python code. So here in app.html you can see the same HTML that we just previously saw in the browser where see we have this different controls. So this controls this particular row. It has the photo of messy, Sarapua, Fadrera, Serena Williams and Virat Kohli. So these are called div elements in HTML. Now I'm not going to go too much detail into web programming because that will be a topic of its own and we are mainly learning data science here. So there are many YouTube channels online resources if you want to get your understanding clear on the basics of HTML programming. There is a great Khan Academy course as well which you can follow. So these are all the icons that we have which is these icons and at the bottom we have this drop zone. So that drop zone is here. So this is that drop zone control. Then we have a classify button. See you can see this classify button. So that button is this one. So we have a result holder which will tell you what is my result and then there will be this table which will be showing you the probability score. The similarity score for each of the images and we will see how this UI control looks a little later. Now the most important file here is app.js. So in this the document.ready is this is a function that calls when the HTML document is kind of rendered by your web browser. And when it is rendered in the initial stage you want to hide the error and you want to also hide the result holder and the error. Now again do not worry too much about it. We will cover what these things are little later. But in the init function I am initializing the drop zone control with some default parameters. And then these functions will be called when you add a file, drag and drop the file all of these functions will be called and when you click on submit button this particular function will be called. Now when I run this function see when I do F5 I see the nice UI being rendered and also in the UI folder I have some test images. So let's see if I drag and drop this test image and click on classify nothing is going to happen really. I can click on F12 button and F12 will open the debugging tool where in the sources see in the sources you can see the same code. So I am looking into app.js here and I want to set a break point here. So when I when you see red dot it is setting a break point and when you refresh this page and after that when you drag and drop this image and click on classify you will see the control comes here which means this button click triggered this function call which is complete event. And on complete event in file dot data URL I get something now let's see what is that something. So here in the Chrome tool you have this file here you can see file and file dot data URL is nothing but a base 64 string. You can see the whatever image I dropped file dot data URL contains the base 64 string for that same thing. Now we saw in the previous tutorial that when we are running Python Flask server it can accept base 64 string as an input. So let me go there. So here I am just going to show you my Python Flask server that you wrote in a last video. You can right click and you can say run server. You can even right click and you can even say debug server. So when you do debug server what is going to happen is you can set a break point and the control will come here. So let's first try with her debug more. So this means my server is now up and running on this particular URL which is 123701 which is a local host and 5000 is a port so 5000 port is here. So my server is ready to accept requests from the UI. So now we are going to write our code in our javascript file. So in the javascript file what we will do is we will make a call call to that URL. Now what is that URL? Because the URL is this because I am running my server on this machine on 5000 port. So you can see that here. You see that this is the exact same URL I am using and the name of my method is Classify image. This is also called an HTTP endpoint. So HTTP endpoint is determined by this one. So if you call this ABC then you will put ABC here. Very simple. Now I will use dollar. Dollar is basically a jQuery syntax and I will make a post call. So post call means I want to post this URL. Now you can go to HTML basics. HTTP that are get post call delete calls. So these are like a restful calls. So then in the post. So this is my post call. In the post call I supply my URL first and I will receive it. So I supply my URL but I need to supply my base 64 image. So this will be my, this element will contain my base 64 image and the second argument will be this one. So what this is showing you is the callback. So this is your input request and this is the response basically. Whatever you get back from your Python Flas server. Okay. So what is my input request? If you remember from our last tutorial in our Flas code we are seeing request.form image data which means we need to pass our base 64 image and image data element. Hence I will just say image data. What is my image data? When you made the data is nothing but this. My base 64 string and that's it. All right. Now I want to once I receive a response I want to first dump the response. I want to see how the response looks and then I will add further code. So once I modify the code here or you need to refresh this page, web page. Okay. So let's see your web page. I'm refreshing it. So I refresh my page and now I'm going to drag and drop this image once again and say classify. Now where is a classified? Okay. I'm wondering why the control did not come here. Okay. Maybe there are multiple server running. So let me stop everything. Okay. Now I'm debug starting my server in debug mode. And then I'm refreshing my page. After I refresh my page I drag and drop my image here and I click classify. Okay. Now I'm not getting the control here. So I will just look at all the Python processes I have running. So I have multiple Python processes. So I'm going to just end all of that. Okay. Start the server in debug mode once again. Okay. And refresh this page and click on classify. So okay. I think this was expected because I have not made my STTP call yet. Okay. So at this point, now you can go to the next function by clicking on this icon or you can click on F11 shortcut or F10. Step is F9. So okay. So I'm going step is basically go to the next line. So this is going to the next line and you know it will make a call to my backend when I execute this line. But I want to set a break point here on this line. So this line will be when I get response back from my server. So now I can just click on this icon here to resume execution. And see what happens now is the control comes here. So now my call that I made from UI is with my backend. And when I execute this, see this image data that you're seeing here, like this particular base 64 is this particular image converted into base 64. So now I'm doing my classification. So I can do next here and I got my response actually. So my response is particularly this. Now response is kind of JSON, JSONified. Okay. And if you want to look at the JSON response, I don't know how to look at the JSON response. Maybe. Okay. We'll look at that response in our backend. So you can just let this go. Oh no, I'm kind of running the same. Hold on. I need to resume the execution. Okay. Looks like I kind of messed it up. So I'll just start over all over again. Okay. So I stopped everything. This time I will run in a normal mode because you kind of got a sense of, you know how you can debug problems when you run it in a debug mode. So I'm running it here. I'm just going to do everything once again. So refresh it. Drag and drop. Coholy is image, classify it. You come here. You say step. Okay. I'm just going to resume. So when I do that, now I got the response back. So this response is. So looks like it did not recognize any image because I think this is happening because Coholy has glasses on on his eyes. So maybe this is not able to detect his eyes. So what will happen is when you resume this, nothing is going to happen. So maybe we will try a different image. So let's try a different image now. So I will again stop this. I will start execution in a debug mode once again to show you why debugging is important. And on the UI. Let me try this simple image. Okay. All right. So classify once again. And here I am again here. And you can just say next next. And in the response, there will be data. Okay. So response has data. And this data has see this data has class Virat Coholy. So it actually identified that this is Virat Coholy. This is the response. This is the result that I will get back in the UI. So I think I can click here and it will go back to the UI. And in the UI, the data that I was debugging. So let's see how data looks. So data here looks something like this. So see, you see data here. So data has class of Virat Coholy. Class dictionary is this and class probabilities. This is the Virat Coholy's number is number 4. 99% it had a 99% match with Virat Coholy. So our wiring is working fine. You know, wiring of UI to backend is working fine. Now we just need to process this response and add more UI code. So let's go back. OK. Now what we'll do is first, if now we are writing JavaScript code, OK? So don't write Python code here. So if let's say some reason data I get is undefined or the data.length is equal to 0. In that case, what I want to do is I want to show an error. So what is this? So if you go to HTML element, OK, let's look for error in HTML. So the error is nothing but a text message which is saying, I cannot classify this image properly. You know, when you don't detect face properly or eyes properly, you want to show this error. And for that reason, I have this element which I'm showing and I'm hiding the class table in a result folder. And also don't worry about these two, we will look into it later later. But see, this error element, initially when our code started, I was hiding it. But now I want to show it because if there is an error, I want to show that first. So let's verify if this code is working properly. So I will refresh this page as usual. And I will try the image where it cannot detect face properly. So see, without cool is image, this image is good. But maybe because he has glasses on it, our classifier is not able to detect the eyes. So I will drag and drop and I will say classify and OK, I think maybe it did not load properly. And OK, I'm not so I should be seeing that code here. So due to some reason, you see in app.js, I'm not seeing the code. So let's see. Let me save the code. Maybe I did not save it, that's why. Yeah, that was the reason, OK. So see now I see that data.whatever code that I wrote. And I don't want to now execute this code, debug this code, you know, I will just keep it straight forward. So OK, now let's do drag and drop. Also, I will remove a breakpoint from here because now it's not needed. So when you do this classification, it is showing you this particular error. And if you right click and inspect, it will show you that this, this, but this is that element, you know. Like the ID is error, see ID is error. And that is the error I was here showing. So our error case is working fine. Now let's think about our success case. So in the success case, what I want to do is I want to, you know, if there are multiple faces in the data, I might get multiple data items. So my response, see, I will just add this comment just to kind of help with our coding. So our response will look something like this. You know, the data will be an array and each array element will have multiple elements. If there are two faces, for example, if we are calling Roger Federer and this are in the same image, then I will get two responses like this. And what I want to do is in my UI, I want to show the person who has the maximum score. For example, here in this case, I have 91% score here, 52%. So maybe I will just show we are covely here. I mean, if you want to do it in a proper way, you should probably display multiple players. But I'm just kind of keeping this project simple. Guys, I don't get much time to, you know, work on each and every fine details, but you kind of get an idea. So that's why I'm just picking one player. Ideally, I mean, you should do this as an exercise where you should show like two players. Okay, this photo contains two players. We are calling Roger Federer. In my code, I will just go with the maximum score, like maximum similarity score, whichever image has, whichever face has, I will just display that. So now you have to write a code. You go through these elements and pick the element which has maximum number in this array. So see, in this one, you have maximum number, which is 91% so therefore you will consider this element and you will discard this one. So I will write now the code for the same thing. So what I will say is this. Okay, let match is equal to now. So match will contain this particular element, you know, based on the similarity score. And let best score is equal to minus one. So this is like finding maximum number basically where you initialize on variable to be a negative number. And this is how you write a full loop in JavaScript. Okay, I'm just going to iterate in through my data array. And then what I'm doing here is I'm trying to find a maximum number out of class probability. See, data has class probability array, which is this array. And if you want to find the maximum number, you can use math dot max. So it will find you the maximum score for this class. So for example, when you are here, it will return 91% when you are at this element, it will return 52%. Okay. And what you will say is if my maximum score for this class is greater than my base code, then my base code becomes that. I'm just finding a maximum number basically. This is very simple logic. And my match will be data of I. Okay. All right. So this looks good so far. And if I, if at all, I have a match, then what? See, if at all I have a match, then I want to first of all hide an error. So if the error is visible from the previous dragon drop, I want to hide it. And I want to show these two elements. Now, now we'll see how these two elements look. Okay. And the result holder would be this. So result holder class would be you are comparing your data player with the match dot class. So see, this match would be this element. And the class would be lesser Roger underscore Federer. Okay. And that's what you are setting it to. All right. So things look good so far. I want to just click, click, click something. Okay. Now, you know what? Let's just run this particular code and see what happens. So I'm doing a five here. Just to refresh. And let's drag and drop here. So see, when I do classify this, this particular element is a result holder. So if you right click inspect. See, this is the result holder class. Okay. You can see it here result holder. And that result holders data player, I set it to be VDOT underscore Kohli. And so what is VDOT underscore Kohli? So if you look at this here, see this data player. So what I'm saying is this data player is VDOT Kohli. And that's what that data player is here. And it is matching the match class. So you have to make sure these elements VDOT underscore Kohli, Serena Williams are same as what is returned by your backend. See, these two strings must match. Otherwise, your code will not work. Okay. So far it's looking good. But I'm seeing a blank table. So in this table, I want to see face similarity score. So for example, for VDOT Kohli here, it will be 90 points, something more than 99% and so on. So how do I see this probability score here? So for that, first of all, my class dictionary will be this. Match dot class dictionary, which is again this particular element. And now what I want to do is for person name, let person name in class dictionary. So I'm iterating through this particular dictionary, which is this. So I'm iterating through each of our players. And I'm getting their index. Okay. So when I do class dictionary, person name for the original message will return 0, my SRPO, I will return 1 and so on. And then what I will do is the probability score will be this. You know, for that particular class, so if you have. Let me show you. You know, this class probability, these are the elements where accessing. And if you have this index, you can refer this index back to your class probability. Okay. Now if it is not visible because of the black background, let me add 2 not bad. So it looks something like this. So if you have class dictionary index, so this 0th index is this. 1 index is this. Okay. So that's what we're doing here. So in this code, I am getting the probability score. So this is this probability score. I'm getting from class underscore probability. All right. Now my element name would be this. So my element name would be score underscore person name. What is a person names? What is score underscore person? It's a score. See score is nothing but the particular cell in our grid. So in this grid, this particular cell will be called this particular cell on the right-hand side of Lionel Messi will be called score underscore Lionel Messi. So it means fact that particular cell. See score underscore Lionel Messi. And this is score underscore Maria Sarapua and so on. Okay. All right. Now what I'll say is this particular element HTML. HTML is this is nothing but a way to set the text. So I'm saying for this particular element, the HTML text should be the probability score, which is like 99.10 percent or whatever. Okay. So this looks good. Now let's run our code again. So I'm refreshing it here. And after refreshing, I'm dragging and dropping. Classify. Now it is showing me all this code and everything perfectly fine. If you're curious, if I went a little fast, just do, you know, like just debug the code. If you do F12, it will allow you to debug the code. And you can see it for yourself. What exactly is going on? Debugging is very, very important. So when you set a breakpoint here, let me just quickly show you. So I refresh it. I drag and drop. I say classify. And here my class dictionary is this. And you can see the state of all those variables. Here, see, you'll be seeing the state of all those variables here in the class dictionary. This one, the base code came out to be 99.22. And when you do next, next, see, I'm doing next here. So my person name, person name is Lionel Massey. And when you do next, the probability score that I got is 0.4. So you will see 0.4 in Lionel Massey. So I can say next, next. That way you can debug and see for yourself what's exactly going on inside your code. So now this looks pretty good. So let's try it on a few images. So in my test images, I have a couple of images here. So I'm going to drag and drop them to see kind of what's going on. So let's see. So I'll just put them side by side. Let's try V.co. some other image. So this is also working. See 80% similarity match. Then let's try Maria Sarapua. See here saying Maria Sarapua. So matching fine. You will find that Maria Sarapua and Serena Williams are girls. So see for girls, even the score see the next base possible match is Serena Williams because she's a girl. And some of her facial features might be similar. And if you look at comparing it with boys, the similarity score is very low. If you try this image, classify. See sometimes it cannot detect eyes face properly. And that's okay. You will see this kind of issues. Our classifier is not like super accurate. Okay, I want to try now this particular image. See here, I have Sarapua and Roger Federer both. So I want to see how this behaves. So here when I drag and drop and classify, it is saying Roger Federer which is fine. But how about Maria Sarapua? So let's debug this. So again, I will press F12 and I will just say, okay drag and drop, click on classify. And when I do that, you will realize that in my data element, this is the response I got back from the back end. See, I actually got both Roger Federer and Maria Sarapua. So this is working fine. It detected both the faces properly. So Roger Federer has 63%, 63% match. Maria Sarapua has 53% okay. And since 63% is more, it is why it selected Roger Federer. But if you want to change the UI, and this will be kind of an exercise for you where you take this UI and modify it so that it not only shows one person, but if there are multiple matches, it should show multiple matches here. So with that, we conclude our UI programming in this tutorial. In the next tutorial, we'll talk about deploying our application to production and some concluding remarks for UI. Since I'm teaching mainly data science and Python, I did not go too much detail into HTML, CSS JavaScript. For that, you can follow Khan Academy scores on web programming. So I hope this was helpful. I'm going to provide a link of my GitHub repository in a video description below. So you can just clone that repository and get access of all the code that we went through in today's tutorial. Thank you.",
    "Machine Learning Tutorial Python - 16_ Hyper parameter Tuning (GridSearchCV).wav": " In this video we are going to talk about how to choose the best model for your given machine learning problem and how to do hyper parameter tuning. Here is a list of topics that we are going to cover in this video. Let's say you are trying to classify SK learns iris flower dataset where based on the pattern and sample width and length you are trying to predict what type of flower it is. Now the first question that arises is which model should I use? There are so many to choose from and let's say use figured out that SVM is the model I want to use. The problem doesn't end there. Now you have hyper parameters. What kind of kernel and C and gamma values should I be using? There are just so many values to choose from. The process of choosing the optimal parameter is called hypertuning. In my Jupyter Notebook I have loaded iris flower dataset here and it is being shown in a table format. The traditional approach that we can take to solve this problem is we use train test split method to split our dataset into training and test dataset here I am using 7030 partition and then let's say we first try SVM model. So first I am going to show you how to do hyper parameter tuning and then we will look into how to choose the model. So just assume that you are going to use SVM model and using SVM model you can train the model and you can calculate the score. Here I randomly initialize these parameters. I don't know what is the best parameter so I am just going with some value. The issue here is that based on your train and test set the score might vary. Right now my score is 95% but if I execute this again, X train X test samples are going to change so it will change from 95 to now it change to 1. I cannot rely on this method because the score is changing based on my samples. For that reason we use k-fold cork cross validation. I have a video on k-fold cross validation so if you want to pause here and take a detailed look at you can go there but I will just give you an overview. As shown in the diagram what we do in a k-fold cross validation is we divide our data samples into n number of false. Here I am showing 5 false and we take 5 iteration in each iteration. One fold is test set remaining our training set. We find the score for that iteration and we take these individual scores from each iteration and we make an average. This approach works very well because you are going across all the samples. We have a method called cross-well score which can tell you the score of each iteration. Here what I have done is tried cross-well score for 5 fold so CV is equal to 5 means 5 fold and tried this method on different values of kernels and C. Here kernel is linear, here it is RbF, C10 and C20. For each of these combinations I found the scores. You can see there are 5 values here and these are the scores from 5 iteration. You can take the average of these and find out what is your average score and based on that you can determine the optimal value for these parameters but you can see that this method is very manual and repetitive because there are so many values you can supply as a combination of kernel and C. C could be 1, 2, 3, 100. For how many times you are going to write this line? The other approach you can take is you can just run a for loop. I am doing the exact same thing but using a for loop. I have my possible values of kernel and then C and then I ran a for loop on both of these and I am supplying those values here. You can see here K well and C well and then find the average scores. When I execute this I get these scores. With RbF and 1 the score is this, RbF and 10 the score is this and so on. Just by looking at the values I can say that RbF1 and the value of C being either 1 or 10 or linear kernel and C being 1 will give me the base score. You can see that these scores are low. This way I can find out the optimal score using the hyper parameter tuning. You can see that this approach also has some issues which is if I have 4 parameters for example then I have to run like 4 loops and it will be too many iterations and it is just not convenient. Luckily, Escalon provides an API called Grid Search CV which will do the exact same thing. Grid Search CV is going to do exact same thing as shown in this code here in line number 14. I am going to do the same thing but you will not that we will be able to do that in a single line of code. The first thing you do is you import Grid Search CV from Escalon model selection and then we will define our classifier. The classifier is going to be Grid Search CV where the first thing is your model. My model is SVM.SVC. I am supplying gamma value to be auto if you want gamma to be in your parameters you can do that but for this example I am just keeping it static. Now the second parameter is very important. Second parameter is your parameter Grid. In parameter Grid you will see I want the value of C to be 110 and 20. These are like different values that you want to try. The second parameter is kernel and you want to try the kernel and you want the value of your kernel to be RBF and linear. These are the two values. There are other parameters in Grid Search CV for example how many cross validations you want to run. Grid Search CV is still using cross validation. It is just that we are making this particular code blog convenient and we are writing the same thing in one line of code. CV is this. There is another value call return train score. This is some parameter that this method returns which we don't need. That's why we are saying it is false. Once this is done you will do model training by saying iris.data and iris.target and once that is done we will print the cross validation results. When you execute this you get these results. Now if you look at these results you will notice that you got this mean test score. CV results are not easy to view but luckily SQL learn provides a way to download these results into a data frame. Here I have SQL and documentation and it says that this can be imported into pandas data frame. So that's the next thing I'm going to do and all you guys are I think exports into pandas by now. So you just create pandas data frame and supply CV results as an input and when I run this I get this nice tabular view. Here you can see that these are the C parameter values then kernel values and these are the scores from each individual split. We then five full cross validation. That's why you get split 0 to split 4 and then you have mean test score as well. Some of these columns in this grid might not be useful so I'm going to trim it down and just look at parameter values and means go. So you can see that these are the possible values of parameter c and then kernel and these are the scores I got based on this I can say that I can supply of first three values into my parameters to get the best performance. So we already did hyper tuning of these parameters. You see that this how this works right now you can have many many parameters all you have to do is supply them in parameter grid and this grid search CV will do permutation and combination of each of these parameters using k fold cross validation and it will show you all the results in this nice pandas data frame. I can do dIR on my classifier and see what other properties this object has and I see some of the properties such as based estimator, best perms and based score. So let me try a based score for so CLF dot based score and the base score it is saying 0.98 which is well it's a 0.98 is the base score. I can also do CLF dot best perms and it will tell me the base parameters. In our case there are multiple parameters which gives you optimal performance but you can see the point you just run grid search CV and then call based perms to find out the base parameters and these are the parameters you are going to use for your model. One issue that can happen with grid search CV is the computation cost. Our data set right now is very limited but just imagine you have millions of data points into your data set. And then for parameters you have so many values. Right now C values I randomly took them to be 1 to 10 but what if I just want to try a range let's say number 1 to 50. Then my computation cost will go very high because this will literally try permutation and combination for every value in each of these parameters. To tackle this computation problem SQL library comes up with another class called randomized search CV. Randomized search CV will not try every single permutation and combination of parameters but it will try random combination of these parameter values and you can choose what those iteration could be. So let me just show you how that works. Here I imported randomized search CV class from the skill and model selection and the API kind of looks same as grid search CV. I supplied my parameter grid, my cross validation value which is again five full cross validation and the most interesting parameter here is an iteration. I want to try only two combinations. Okay here we tried total six. You see 0 to 5. So here it will try only two combinations and then we'll call fit method and then we'll download the results in two data frame. When I run this you can see that it randomly tried C value to be 1 and 10 and then kernel value to be linear and RBF. When I run this again it changed the value of C to be 20 and 10. This way it just randomly tries the values of C and kernel and it gives you the base core. This works well in practical life because if you don't have too much computation power then you just want to try random values of parameters and just go with whatever comes out to be the best. Alright we looked into hyper parameter tuning. Now I want to show you how do you choose a based model for your given problem. For our iris data set I'm going to try these two classifiers. Okay SVM, random forest and the logistic regression and I want to figure out which one gives me the best performance. You have to define your parameter grid and I'm just defining them as a simple JSON object or simple Python dictionary where I'm saying I want to try SVM model with these parameters random forest with these other parameters. I want the three value of random forest to be 1, 5 and 10 and this n estimator is an argument in random forest classifier. Okay similarly the value C is an argument or a parameter in logistic regression classifier. Once I have initialized this dictionary I can write a simple for loop. So I'm just going to show you that for loop here and this for loop is doing nothing but it's just going through this dictionary values and for each of the values it will use grid search CV. So you can see that grid search CV the first argument is the classifier which is your model. So here you can see the model is classifier. So it is trying each of these classifiers one by one with the corresponding parameter grid that I have specified in this dictionary. You can see that parameter is the second object, second argument and then cross validation is five. I then run my training and then append the scores into this scores list. When I run this my scores list has all those values and all I'm going to do now is convert those results into pandas data frame. When I do that I see a nice table view which is telling me that for SVM model I'm going to get 98% score random forest is giving me 96 and logistic regression is giving little more than 96. So here I have my conclusion that the best model for my iris dataset problem is SVM it will give me 98 person score with these parameters. So not only we did hyper parameter tuning but we also selected the best model. Here I have used only three models for the demonstration you can use 100 models for example here. So this is more like trial and error approach but in practical lives this works really well and this is what people use to figure out the best model and the best parameters. Now comes the most interesting part of my tutorial which is the exercise you have to do this exercise guys just by watching video you are not going to learn anything. So please move you but work on this exercise here we are going to take SK learns handwritten digits dataset and then classify those digits using the listed classifiers and also you're going to find out the best parameters for it. Post your answer as a video comment below and if you want you can tell your answer with the solution I have provided now my solution is not the best one because I just tried only few parameters so you should try more parameters and I hope you can find better score than me. So don't click on the solution link until you have tried it yourself. Thank you very much for watching this video if you like the content please give it a thumbs up subscribe to my channel and share it with your friends. Thank you very much I will see you next tutorial.",
    "Machine Learning Tutorial Python - 17_ L1 and L2 Regularization _ Lasso, Ridge Regression.wav": " Overfitting is a very common issue in the field of machine learning and L1 and L2 regularizations are Some of the techniques that can be used to add test the overfitting issue In this video we'll go over some theory on what exactly L1 and L2 regularization is and then we'll write a Python code And we'll see how a model which is an overfit Can be addressed and the accuracy can be improved when you use L1 and L2 regularization We'll be using a housing price data set from the city of Melbourne and will first build a model using a simple linear regression And we'll see it is overfitting the model and then we'll use L1 and L2 regularization and we'll see How we address that overfitting issue and how it improves the score on our test set. So let's get started Let's say you're trying to predict number of matches one Based on the age now usually when The player gets aged and his boss person or atlet gets aged the matches one kind of reduces So you can have this kind of distribution where to build a model you can create a simple linear regression model and the equation might look like this So matches one will be theta 0 plus theta 1 and into age. So theta 0 and theta 1 are just the constant. This is a regular Our linear like simple linear equation But you see that This line is not really accurately describing all the data points It is trying to find a best fit in terms of a straight line But you see all these data points are kind of going away And then if you have test data points which are lying somewhere here Then this is not a very very accurate representation of our data distribution Then you can build a distribution which might look like this So here we are trying to draw a line which kind of exactly passes through all our data points And in that case your equation might look like this So it's a higher order higher order polynomial equation Where you are trying to find out the matches one based on the age of a person But here the issue is This equation is really complicated the line is zigzag type of line which is just passing through all the data points and now If you have some general data points at the top here Again, this is not generalizing the The distribution really well What might be better is if you have a line like this So this is This is a balance between these two cases that we saw previously So you can have You know only theta 2 into a square The line will look like a curve And it can generalize your data really well So that tomorrow if new data point comes in This equation will be able to make a better prediction for you So the first case is called underfitting the second case is called overfitting and the third case is balance fit So you kind of get an idea on overfitting here where If you try to run training too much and try to fit too much to your training data set Then you will have issues with Testing data set when you try to predict your new data points It might not do a better prediction So you always have to keep a balance between these two extreme cases Now If you don't know about all these equations and everything Please refer to my linear regression tutorials I had in the same playlist I had post few videos on linear regression So you should watch that that's kind of a prerequisite Now how do you reduce overfitting So here is my overfit line along with the equation And if in this equation I somehow make sure that my theta 0 and theta 4 is almost close to 0 Then I will get an equation like this. So just think about it if theta 3 and theta 4 is almost close to 0 Then you're kind of ruling out this whole factor And then you can create this type of equation So the idea here is to shitting your parameters Your parameters which is theta 0, theta 3, theta 4, you want theta 2, theta 1 If you can reduce this parameter if you can keep this parameters smaller Then you can get a better equation for your prediction function Now how do we do that? We earlier saw in our linear regression video that We calculate mean squared error So when we run training we pass first sample and then we calculate why predicted on some randomly initialized weights Then we compare it with the truth value and then this is how we call Calculate mean squared error or ms e Here why predicted is actually h theta x i where h theta x i could be High order polynomial equation like this, okay? And x1 x2 is nothing but it is your Feature so in our case it will be age of a person if you are to talk thinking about housing price prediction It will be the size of the house now just Think that in your Mean squared error function. So by the way, this mean square error function We use during training and we want to minimize the value of this this error You know on each iteration. So in this Equation what if I add this particular parameter? So what is this? So there is this lambda which is a free parameter you can control it is like a tuning knob And You are making a square of each of these theta parameters So now if your theta gets bigger This value will get bigger the error will be big you know and Your model will not convert so Essentially what you're doing is you're penalizing higher values of theta here So whenever model tries to make the theta value higher you are adding a penalty here So by adding this penalty you're making sure your Theta value doesn't go too high. So they will remain very small You can fine tune this thing using this this parameter lambda here And if you make this bigger the theta value will even get smaller and smaller You know and if you get this smaller then theta value can be bigger So this is called l2 regularization. It is called l2 because we are using a square and in l1 regularization You're using the absolute value. So that is the only difference between l1 and l2 That in l1 you're using absolute value of theta parameter Here again if theta is bigger the error overall error is big and It kind of acts as a penalty so that During your training overall the value of theta will remain smaller and again going back to this equation here When these values remain smaller you come up with a simpler equation You know you don't make it really complicated and simpler equations are the best are to represent the generic case of your prediction All right, so let's get into coding now for the coding. I'm using housing price Data set for Melbourne city. I got this data set from kegel And we are going to build a linear regression model. So you can see that there are different features such as room Uh the distance postal code bathroom car and so on And in my notebook. I am going to as usual first import this Data set and just call it data set You know So I imported this into my data frame now and data frame is looking good I'm going to do some exploration of my data frame now and kind of print of value unique values Um in data set. So you see like there are 351 suburbs These many addresses and so on Also if you look at the shape of the data set There are 348 by this by seven Uh records and 21 columns in total Now I'm going to discard certain columns. I just did visual observation and discarded certain columns which I don't think are very useful. So For example date Right, so date is it's not useful. So I will just say okay Here are the columns that are useful And I'm just being copy based to save the time on recording and when you Uh pass these columns to this data set you get the filter data set okay and then Again, you can run data set.head. So now I have less columns. Okay, so if you do data Set.shape You know, I have 15 columns instead of 21 Now I want to check for the any values and you can do it by calling is any function on your Pandas data frame and you can do dot sum and we'll tell you so bedroom 2 has total 8217 any values, you know any rows. So now we need to handle these rows. So I'm going to fill Some of these columns with Value 0, okay, and those columns are these so columns to fill with 0 all this For example, if car is any which means you know there is no car parking Available for that particular property And when you run this function so in your data set you are saying All these columns fill any with 0 it will take those any values and it will spill them with 0. So now After doing this, I will do this again And you can see that for example Property count it was three so property count now is 0 all the any values are filled similarly car car parking There were 8,700 rows With any value now I made 0 Now What we're going to do is we'll look at certain other columns such as lens size and building area And we will like this true and we will Carefully the mean and we'll fill those with a mean value So the way you do that is using this function So you're doing lens size dot fill any with the mean of the same column Okay, so this is a safe safe assumption And after that when you do any You find that See there are no values with Or 0 I mean there are some prizes, but if you look at your independent features The that's that's basically now curated and there are There are only two columns region name and this Council leaders. I'm not going to care about it too much now. I will drop So like these three so if you have couple of random columns with any value you can drop it you know our data set is huge and if you drop like these three and three six rows It's not a big deal So I will just Drop them and run The same function again And you see like none of the columns have well any values now So now Some categorical features which I want to Convert into dummies, you know, I want to do one hot encoding so you guys might know about one hot encoding If you don't I have one hot encoding video So things like Suburb for this example right any text column that you have You need to do a dummy encoding and pandas provide a very convenient API call get dummies So from data set you get This and you drop first because you want to avoid the dummy variable trap again You if you don't know about dummy variable trap you can watch my one hot encoding video Uh, and this will kind of drop that first column When you do uh encoding So now when I look at my data set you see everything there's no text column And if you look at this council later as a council area underscore so it created a Separate column for each of the council area. So this is what Uh dummy uh or hot encoding means Now I want to create x and y So my x so I can y is basically price and x is you know this one And by the way, I had some prizes as NA and when I did drop NA the prizes Those any value got dropped as well. So my data set looks pretty good and I will do now train test split So train test split is this All of these are like standard usual methods. We have covered in previous video. That's why I'm kind of going over it a little fast You are doing a 30% split so 30% test 70% training And you get all these different data sets Now we will use a regular linear regression. Okay, so this is how you do regular linear regression And then uh when you run this it's gonna do training and the model is fit now Now I will do the score on x-test and while test You realize that my score comes out to be really low 14 person which is Very very low score, but if you do a score on training data set say it is 68 person So this is clearly overfitting your data set Your data set was so much overfit that for training samples it gave a good accuracy But for test samples the data samples that it has not seen before it gave a horrible score So how to address this SK learn provides a model called lasso so lasso regression is basically l1 regularization If you do as they learn lasso regression See this is the lasso regression and it is l1 regularization So I'm going to use that model So I Created a model imported that and then I created a lasso regression object with alpha value I'm initializing this alpha value to be random you can play with these values and see which one gives you a better accuracy And I initialize few other parameters as well and when I now run My regression it is fitting it with the regularization parameter on So if you look at our Equation or layer, so let me open our Equation In that equation L1 regularization will add an absolute theta value In your error and this is the formula that we'll use during the training Simple linear regression without any regularization will not have this red parameter. So that's only difference now let's do a score on your test So I will do lasso regression score And you find that the accuracy improved and I just to make sure I will also do a score On x-test and on training as well So you see training and tests both are giving a very good Not very good, but 67% accuracy which is Compared to 13% it improved to 66% you can see How much of a big difference our regularization can make There is l2 regularization as well and it's called rigid regression. So if you do sk-larn rigid regression This one is an l2 regularization and Here I can import that from ascalon library as usual I create a rigid regression class object which will look like this and call a fit method on this And then when I do a score on your test data set it looks six again. It's 67% So it's pretty good and let's check on training data set Training data set is also You know, it's pretty pretty good So you saw that by using rigid regression and lasso regression. So rigid l2 lasso is one your accuracy for your Not seen data samples which is your test sample improved a whole lot If you are trying to learn machine learning you can just In youtube you can just search code basics machine learning tutorials I have the complete list of Machine learning videos here you can go in sequence if you're watching regularization video if you want to Get a fundamental clear initially on linear regression as etc I would suggest you watch tutorial two and three I hope you like this video if you do please give it a thumbs up share it with your friends and thank you very much for watching",
    "Machine Learning Tutorial Python - 14_ Naive Bayes Classifier Algorithm Part 1 (1).wav": " In this two part tutorial we are going to learn about naive base. In the first part we will cover some theory and then predict Titanic survival rate using naive base. In the second one we will build email spam detector. And here is the list of topics along with the timelines. If you want to skip to a specific topic you can do that. Let's start with some basic probability theory. You all know that when you flip a coin the probability of getting head or tail is 1 by 2 because there are two possible outcomes and the chance of getting head or tail is 50%. Similarly when you pick a random card the probability of that being a queen is how much? Just think about it. It's very simple. There are total 4 queens, 52 cards. Hence the probability of getting queen is 4 by 52 which is 1 by 13. If you know that the card that you have picked is a diamond then what's the probability of getting a queen? Well this is called a conditional probability where you know that even A is occurred and then now you are trying to predict the probability of B. So total diamonds are 13, queen is 1. We all know by just simple intuition that the probability here is 1 by 13 but this is how the conditional probability is represented where you say P of queen slash diamond where you know that even diamond has already occurred and the probability of getting a queen is something you are calculating. So that's called a conditional probability and the way that is represented is P of A slash B where even B has occurred already which is that you know that the card is a diamond and you are trying to find the probability of event A which is whether the card is queen or not. Thomas Bayes gave this famous equation of finding a conditional probability where you can find probability of A given the event B has occurred by knowing some parameters which is the individual probabilities of A and B and knowing the probability of B given that A has occurred. So let's look at it in the context of our queen and diamond problem. So here this is the same equation I have represented for our specific use case and you know this individual probabilities and if you put them into our equation you can easily find this conditional probability. Now this is a very powerful theorem where you know probability of certain events but you don't know the probability of some other events and using those certain events you can find other probabilities. You all know about Titanic crash there was a movie that was made and it was a super hit but that event actually occurred and it was said that so many people died. We have a data set of this Titanic crash where there are name of the people along with certain features which is your fair ticket cabin etc. based on that we are trying to find out the survival rate and here we can use base theorem where we are trying to find the probability of survival based on the features such as if the person was male the class, age, cabin, fair and so on. The reason is called naive base is because we make a naive assumption that the features which is male, class, age, cabin and so on are independent of each other. In reality some of these features might be dependent such as the fair is fair and cabin are kind of related but we assume for simplicity purpose that these are not related hence it is called naive based and it is a simple assumption which reduces our calculation and makes the algorithm simple yet effective. If you want to go in math a little bit you can watch this video. This video is by Luis Serrano and he has really explained it well. I have a link of this video in the video description below so go watch it it's going to be very useful if you want to know the details. naive base is used in email spam detection and rigid character recognition, the weather prediction, face detection and news article categorization. Now let's go straight into the coding. From the KGLE website I have downloaded the Titanic dataset and it's available locally here in form of CSV. You can see all the passenger names, their features and whether they survive or not. I have downloaded this file locally. I have loaded the same CSV file into a pandas data frame in my Jupyter notebook. Now we are going to do some data exploration first. We can see that some of the features are probably not relevant so I am just going to assume that they don't have an impact on my target variable. For example name, right? Like name doesn't matter like what was the name, it doesn't have an impact on the survival rate. I am going to drop those variables and make my data frame a little simpler. I dropped all these variables and now I have this data frame. One thing I noticed was there is this target variable which I want to separate out into a different series. I will say target is equal to DF dot survived and inputs which is my dependent variables. For that variable I will drop survived column and just put a rest of it. So this way I have my independent variables and dependent variables into two separate entities. We can see that the sex column is text and we want to convert it into a dummy columns because we all know that the machine learning models can't handle text so we have to convert them into numbers and I have one hot encoding or the dummy's tutorial and there I have explained why it's needed but here that's what I am going to do. So the dummy's will basically convert the sex column into two different columns and it will just put zero and one values. We are going to append these dummy's columns to our inputs data frame and the way you do that is by using the pandas concat function where you concat these two data frames on columns and when you check the data frame which came as a result of concat operation you will find that now I have these two columns. Now I need to also drop the original sex column because now I have dummy columns so I don't need that particular text column. So now my data frame looks like this it is much simpler with all the numbers there is no text. I also want to find out whether there are any numbers in any of the columns and the way you do that is by using this. When you run it it tells me the age column has some any values and I am just curious to find out what those values are. So I am just going to print like the first 10 values and I find that some of these values are n and n. Now I have to turn on how to handle n and n values in pandas so you can refer to that but the popular approach here is you can take a mean of the entire column and then feel those n values with the mean value. And the way you do that is by doing this you can say inputs dot age dot fill n a with the mean and when you do that let's print first five the fifth one had I guess the n value and you will see that it filled it with the mean value. I think fifth or the sixth okay the sixth value had it. So you see that this was the mean value. I know point 69 probably doesn't make much sense so you can just average it out you can just make it like a whole number but yeah. Now we are going to use sklons train test split method to split our data set into training and test sample this is what we usually do before training our model. So that the model is not biased when we are trying to calculate the score and this is how you train test split. This is something we have done in all the tutorials so this is pretty simple there is no rocket science to it. I am going to divide my test and training sample into 20 80% ratio all right so when I do control enter it's going to divide it into training test samples. I can just quickly check the length of x train and the x train x test to see the distribution so you can see that and the total is length of inputs so the total was 891 and 80% of that is 712 20% is 179 you can also print these individually if you are interested for example if you want to check what is your x train you can print like that. All right now it's the time to create the naive base model. Now there are couple of naive base classes we are going to use Gaussian naive base the Gaussian naive base is something that you will use when the your data distribution is normal or it is like a Gaussian distribution this is a concept in statistics you can learn about it it is also called a bell curl okay so we can use that model here okay so I have created that model and now it is time to train the model you all know that you have to call fit method whenever you want to train the model it and you train it on x train and y train and x train and y train is something that we got here so when you execute this it will train it now the training here is very fast because our samples are very less when you have so many samples it might take time and you have to use parallelization GPUs and so on. The first thing I do after my model is train is I always measure the score to find the accuracy and I found that it was 77% I know it's not very high but it actually depends on the samples as well so if I run this again the score is going to vary so you can see it is now 78% now let's look at our x-taste samples so x-taste I will check the first 10 samples and I will see this is what it is I will also check my y-taste sample to see what are their values so y-taste is this oh this is probably not a good taste set okay let me just run it once more so when I do control enter it again creates some random samples and the score is a little better this time you can see there are a couple of ones here and you can now use model dot reddit for our first 10 samples and you can now compare that to y-taste so you can see that it was 0 1 here is 0 1 0 0 0 so 0 0 this sample it didn't get right you see it says it's 0 so our score is like 81% so it is expected it will make mistakes sometime and that's fine you can also use predict probability function to figure out what are the probabilities of each class like in our case each class is whether a person survived or not survived when you run this this sample says that 97% probability is the person didn't survive and this much probability that actually this much probability that person survived and this much probability that he didn't survive right that's why it makes a decision that it is 0 in the second one the person survived it is 1 so that's why this probability is like 93% this all I had for part 1 in part 2 we will cover email spam detection using naive base and we'll also have exercise at the end of that part 2 tutorial thank you",
    "Data Science & Machine Learning Project - Part 2 Data Collection _ Image Classification.wav": " In this video we are going to talk about data collection aspect of our project. Now as you know that when you're doing supervised learning you need lot of throat data. throat data is basically you have an image which you can call it as X or an independent variable and then you have a label, a class label which will tell you whether this image is of Virat Kohli, Maria Sarapua. So that label is called Y or a target variable. You need lot of these images to train our classifier. Now how can you collect all these images? There are four different ways of collecting this. Number one is very obvious you go to Google images, start downloading these images one by one. Now this approach is very manual and labor intensive. For our project it works we need probably 50 or more images so it's not that bad but if you are training a huge algorithm with so much data then manually downloading these images might be very tough. So the second approach is you can use Python and automated test software along with Chrome driver. I found this nice article on towards data science which talks about how you can scrap images from Google and how you can download them in automated way. Now I'm going to provide a link of the code. I don't want to discuss this code in detail for one specific reason. The reason is that the code is working today. I'm not sure if it's gonna work after one week and this is because Google is improving their algorithms and they are trying their best so that people do not scrap their website. Webscaping in general is a very gray area in terms of legal implication. Google images are a public information so on one hand you would think that why can't I scrap it but if you are Google then you don't want people to be writing these automated boards which can scrap the content. Google in fact had this issue when Bing came out, Bing started scapping a lot of Google articles to improve their own search performance and Google of course did not like it. Any website would not like the fact that you are a web scrapping their content and they will try that best to stop you from writing this web crawler. So you have to be very very mindful when you are using any automated way of scrapping internet just keep in mind that your code might just stop working one fine day and you have to continuously improve or continuously change that. Now in this video clip you already notice that when we are running this program or to scrap Google it is opening a Chrome of Indo on its own and it is trying to download the images one by one and at the top you notice that Chrome is controlled by automated testing software. Now if you know about Selenium Selenium is an automated testing software which will simulate manual action. So it is like a computer going and clicking different things and right clicking and downloading it. So it is that paradigm. There is RPO also which is called Robotic Process Automation which can be used for automating this kind of manual task. Third way which I kind of suggest is probably a better way is to use Chrome extension called Fatcone. So the Fatcone is something you can add it to your Chrome easily and after that you can open a tab of Google images and you can just say download and it will download all the images. You can also filter based on Wyrton Height. I want to say thank you to my dear friend Kenji who is a head of data scientist. He also runs a YouTube channel for data science. So I will provide a link of his YouTube channel. You can go check it out. He's doing some awesome work. So thank you Ken for suggesting me this Fatcone tool. Also I want to thank you my dear friend Dave Jyothi Paul who is a data scientist at Amazon. He has helped me throughout the project. The fourth way of collecting images is to buy these images. If you are working for a big company they will have a budget and I was using that money they can even buy the images from some news website. You know let's say you are a CNN or NDTV or Times of India. These are news portals. The companies will have a lot of images. And there could be another third party vendors who might be selling those images. So the fourth option which is by paying a price you can buy these images. Now if you are less than Times of India or CNN yourself then your company will have a team engineering team who will have access to these images. So you can contact the team and you can get those images from the team and and you store it in your data warehouse. For your convenience I have given a link below where you can just download all these images. So if you don't want to bother about all this you can I have this ready-made data set that you can use. But if you want to play with this Google image capping code then I have a link GitHub link in the video description below. Also try this Fatcon tool. In the next video we are going to talk about data cleaning and future engineering.",
    "What is feature engineering _ Feature Engineering Tutorial Python # 1.wav": " Feature engineering is a very important process in the field of data analysis and machine learning. And I am very excited to announce this new series of tutorials where I will be going over different feature engineering techniques and will be writing Python code along with the exercises. With that, let's first understand what exactly is feature engineering. There was a study published by Forbes Magazine on what data scientists spend most of their time on. And from this chart, it was very clear that they spend majority of their time in data cleaning. Because in the real world, the data looks pretty messy as shown in this funny picture here. And you need to clean that data so that you can feed that clean data to your machine learning model. Machine learning model will not work if the data is messy. Even if you are not using machine learning and doing simple data analysis, still you need to clean data. So data cleaning is very, very important and that's where data analyst and data scientist spend most of their time on. And feature engineering is something that helps you with data cleaning. Let me give you an example. Let's say you are studying the home prices and these are the home prices in the city of Bangalore in India. To do the analysis on this, one of the things you can do is you can create a new column which will be price per score. Just to kind of get a feel of your incoming data set. Now this data set can be coming from internet from the variety of sources and it can have errors. Okay, it can have problems. So by creating this new column, you're trying to analyze if you have any data errors. All right. And once you have this new column, now you can use either your domain knowledge. So if you have a business manager that you are working closely with, he would say that in Bangalore, you cannot get a home at a rate of price per square foot equal to 3500 square foot. So when you see this particular data point highlighted, which has price per square foot as 500, it clearly looks like an error. Okay. And this error you detected by using a domain knowledge, you can also use visualization. And by plotting this bar chart, this data point clearly comes out to be different than a rest of the data point. You can also use mathematics and statistics where you can use techniques like two standard deviation to detect these kind of spatial observation. Now this data point here is called outlier and these three techniques listed on the right hand side are basically the outlier detection techniques. If you have followed my pandas tutorial, we also looked at how to handle missing values. Sometimes when you receive your incoming data set, it might have missing values. Here for these two data points, the bedrooms are missing. So what you can do is just take a median, which will be three and fill those missing values. Okay. So handling missing values is also very important during the process of data cleaning. We also had a tutorial on one hot encoding in my machine learning series where we replace the text data with numeric data. Here the text data is town. So these are the property prices in different towns in the state of New Jersey. And when you want to build a machine learning model, the model doesn't understand text. So you have to convert that into numeric values and we converted this text column into these three numerical columns and this process is called one hot encoding. So these three techniques that we covered, which is outlier detection, one hot encoding and handling missing values, these are the few examples of feature engineering. So feature engineering is basically a process of extracting useful features from raw data using mathematics, mathematics, statistics and domain knowledge. We have seen advancements, especially in the area of neural networks where you can auto detect the features, the meaningful features. So the examples that I showed you, we used domain knowledge, mathematics and statistics to extract meaningful features out of our data set. These processes were manual but there have been some automated ways of detecting these features and we'll be covering all of that in this feature engineering tutorial series. So I hope this gets you excited enough and I will see you in the next tutorial. Thank you.",
    "Machine Learning & Data Science Project - 6 _ Python Flask Server (Real Estate Price Prediction).wav": " Now that our model is built and the artifacts are exported, the next step would be to write a Python Flas server which can show HTTP requests made from the UI and it can predict the home prices for you. So in this video we are going to write that Python Flas server which will be used as a backend for our UI application. Here I'm going to click on PyCharm icon. PyCharm community addition is free to download from JetBrains website. So you can download it. It's a Python editor basically. Here I'm going to click on Open to open a project. So you go to see code directory and in here I have created a folder called BHP which is Bangalore home prices. If you look at this folder, the folder has three subfolders client. So I have created all these folders and these are empty right now. So client is empty. This is where our UI application will live. Another folder is server. This will contain the Python Flas server that we are going to write in this video. And the model has the notebook that we covered in all our previous tutorials. So this is that notebook and these two are the artifacts. So this one is the saved model and this one is columns.json which we exported here in our notebook. See these are the two files that we exported. So let's open that folder here. When you click on OK, PyCharm will open that folder and this is how it looks. We're going to do work in this server folder in this video. Right click here and create a file called server.py. In which you should import flask module. Flask is a module that allows you to write a Python service which can show HTTP request. Here I need to configure the interpreter. So file, settings, interpreter and I'm going to use Anaconda as my interpreter because Anaconda comes with flask. If you're not using Anaconda that you have to do pip install flask yourself. Okay server.py is my main server file where I imported flask module and created an app using this line. And in my main function I can just run app.run and it will run the application on a specific port. I'm going to write a simple hello routine. Okay. So we can say hello which just returns high. And the way you expose HTTP endpoint is by writing app.route. Here you can say this is hello which just returns high. Okay. And I can now run this server by simply saying python space server.py. By doing this it's going to run the servers on this particular URL which I can copy in my browser. And I can call this hello routine. And when I run this you can see that I got high back. So this is a very simple basic Python Flask server which we have up and running. Okay now I'm going to close this server and write my actual routine. So we need two routines. So first routine would be to return the locations in Bangalore city. Now if you remember we have our locations in this columns.json file. Okay. So I'm going to create a sub directory within server directory call it artifacts artifacts and copy those exported artifacts which is my model and column.json. So control C. Control V. So now I have those artifacts here. You can see that my artifacts have columns.json and the actual model. And then I'm going to read this column.json. If you look at column.json it contains all the data columns and from here onwards are all my locations. So in my UI application I want to have a drop down where I want to show all the locations. And for that I'm going to write my first routine which will just give me locations. So I will call that routine get location names. Just call it get location names which will return all the locations. Okay. And the way you return all the locations is using this jsonify method. All right. So let me just copy paste some code here. And what I'm doing here is I'm returning a response which contains all the locations. Now what are my locations? So for locations I'm going to create a new file called util. And util will contain all the core routines. Whereas the server will just do the routing of request and response. So right click on the server create a new file call it util. So this is util.py which I'm going to import here and just say import util. And util will have this function call get location names. So this get location names routine just returns the location name. So let's first write this routine and test it out. Okay. So get location name should read the column dot json and it should return me the list of all the locations starting from this column because the first three columns are they are they're not locations you know they are other features. All right. So let's read the column dot json file and then return those locations. So I'm going to write a function called load saved artifacts. This method will load the saved artifacts which is my column dot json and bangler home prices. And I'm going to store both of these into a global variable. So I'll create two global variables. In fact three global variables. So these are the three global variables I have and locations. Under some this call location will contain all the location. So in this method I will just write that I'm loading the saved artifacts. And then when I say global it will treat these two variables as a global variable here. Otherwise it will create just a local variable which won't be good. So first let's let's open uh from the artifacts directory columns dot json file. Okay and we are reading the file hence r as f and you can do json dot load f because this file is a json file and you can call json dot load method on that. We need to import of course a json module here. And whatever object is loaded will be converted into a dictionary on which we can call this data columns key. That will return me all my data columns. And out of all those data columns starting with column number three are my locations. So 0, 1, 2, 3. So we can use Python index slicing to get elements from the list starting with number three. And you all know that we can use a syntax like this where I'm saying start index is three column nothing nothing means take everything else. And that would be my locations. We will also load a saved pickled model into underscore model. Okay. So let's say with open.artifracts the name of the model which is this. And since it's a binary model I am going to use rb as f underscore underscore model is equal to pickle. So we are going to import pickle module as well. Pickle dot load f. And I will say that loading the artifacts is done. Once this routine is done get location names can just return locations. Okay. And you can run this code by right clicking and saying run. And when I do that I am getting none for obvious reasons because I did not call this method here. So I need to call this method and when I run it again it says okay I have a spelling mistake. So artifacts artifacts. So this is loading the artifacts into memory. And it loaded it and after that it printed all the locations. So you can see that my location names are correct. See I got all these locations. Once that routine is done the second routine that you want to write is a function which can return you an estimated price given the location the square foot area BHK and bathroom. So let's write that routine. We'll call it get estimated price which takes few arguments. These are the arguments that it takes and it should return me the price for this given features. In order to return the price what we need to do is we have a model on which we are going to call a predict method. You all know that when you have ask a loan saved model you can call a predict method which will take an input which is your axe and then for that given input it should return the output which is the estimated price. Now the input that it will take here will be a two-dimensional array and we are going to create a numpy array and we'll call it x and then for that x I will wrap it up into the array so that it becomes two-dimensional array. So what is my axe? x is my numpy array. So let's import numpy module first. Here I'm going to create a numpy array with all zeros because we saw in our previous Jupyter Notebook that we need to have a numpy array with zeros. So let me open the notebook that I had. So you saw in this function. So this is pretty much the function that we used. So I can actually copy this function. Now I don't have axe.columns here but what I have is underscore underscore data columns which is a simple Python list. Here it was a numpy array and that's why we used np.ware. So we are not going to use np.ware because it's a Python list and you know in a Python list to find an index you can call dot index method. And whatever location you are getting you need to convert into lowercase because remember that our columns.gson has all lowercase locations. And the behavior of Python list dot index method is that if the element is not found it throws an error. That's why we are going to wrap it up into try catch block like this. Try accept and if it cannot find the index we will just initialize it to minus one. And here the number of zeros will be equal to the length of data columns. So we have square foot bath bhk and the specific location index will locate and then make that particular element one and the remaining elements would be zero. Now we are using a dummy one hot encoding and that is the reason we have like so many columns like this. So we are finding the appropriate column for a given location and setting that value to be one and remaining values are zero. And once our model does a prediction we get a two-dimensional array back. And since our array has only one element we can access zeroth location in that element and that will give us the estimated price in the lakh rupees. Now this would be a float number. So we are going to round the float number to two decimal places like this. Okay so my get estimated price function is pretty much ready and I'm going to call that particular function here. So I had taken a regular location here and these two are actually other locations. Okay so let us right click on this run it and when I run it it's loading the artifacts in this line. Okay it says none type object has no attribute predict. Okay why does it say that? All right I think it is saying that because underscore underscore model here should be a global. Okay so I forgot to define that as a global. So you can see that it predicted the price of two properties in first phase jpnagar. One was with three bedroom and three bathroom another one two bedroom two bathroom. You can say two bedroom two bathroom prices little less. So first one was 86 lakh. The second one is 83 lakhs for these two other locations property prices are same. Now that my utl methods are ready in my server.py I have created another endpoint and let's call it predict home price. This takes a post method okay so htp post again if you don't know what htp get and htp post is you should pause this video and watch some other tutorials to clear your basics on how get and post work. Also if you know want to know more about python flash server just search for some tutorial on flash server. I have this dummy routine and which I'm going to implement now and the way it works is I have imported this request class and whenever we will be making a htp call from our html application we will get all the inputs in request.form okay and those inputs will look something like this request.form will have an element called total square foot which we will convert it to float and store it in our local variable called total underscore sqft and we will get other variables in a similar fashion. Once we have all these variables we need to simply call utl.get estimated price function okay so here we are calling utl.get estimated price function where passing the input parameters and that will return us the estimated price back. All of our code is return and it is time to do some testing now. For this we can use a postman application you can download it for free and the postman application allows you to test your htp calls okay so first we are going to test our get location name scores. In the postman application you can select get here and I have the url and after that I am mentioning my htp endpoint which is get location name. This is the same thing which is mentioned here. Let's run our server now so you can run the server by executing python space server.py command and when you do that here it is saying that my server is ready and it is running. Once the server is running you can pull in the postman application and specify like a get call you can specify post call as well and click on send button when you click on send see you are getting this response back which has all the locations okay so this call looks good. Let's move on to another call which is predict home prices. So I have another tab here where I am saying predict home price and in the body since we are using JSON form you need to click form data where you need to specify all your key and values. So key and values are total square fruit location bhk and bathroom. So square fruit is 1000 location we are using first phase jpnagar bhk and bathroom is 2n2 okay and when you make a send call here it is done 83 lakh as an estimated price you can change bathrooms let's say you have a bigger home with 2000 square fruit area 4 bedroom and 4 bathroom. When you send it will cost you 1 crore 68 lakh rupees all right so now my HTTP endpoints are ready and tested and my python flash server is kind of complete so it is now the time to build the UI. So in the next video we are going to build the UI which will make these STP calls with that video we will complete our application.",
    "Outlier detection and removal_ z score, standard deviation _ Feature engineering tutorial python # 3.wav": " In this video we are going to look at how we can use Zscore and Standard Deviation to remove outliers from your data set. We'll be using a real data set from Kegel.com and remove outliers using Zscore and three standard deviation. In the end we'll have an interesting exercise for you to work upon. We will be using Weight and High Data Set from Kegel. Thanks most of our Ali for providing this data set. This data set has height and weight to columns. It's basically the weight and height of different people. Just to make things simple, I have a removed weight from that data set and my CSP file looks something like this. You can see that it has 10,000 records in it and I'm going to load that into my Jupyter Notebook. Here I imported couple of important modules and then I imported the data set into Pandas DataFrame. It looks something like this. The first thing I'm going to do now is plot a histogram just to understand the data distribution. The histogram will look something like this. Here I have this height column that I'm using to plot the histogram and beans is like how many bars that you want to see on the chart. If you increase the bin size, you'll see more bin size here. This histogram shows normal distribution. If you don't know about histograms, this is very simple actually. What this is saying is for 65 inch height, I have more than 1,000 samples. For 60 inch height, I have around 370, 380 samples. The y axis shows the number of samples that you have in data set for that X value, which is your height. If you don't know about histogram, again, Google it, get yourself familiar. Once you have the histogram, you can figure out if your distribution is a normal distribution or not. Now what is a normal distribution? If you go to this website, mathisfund.com, this is a great website by the way. This website explains how a normal distribution looks like. It often has this shape of a bell curve because it looks like a bell. And what it means is majority of the values are centered around mean or an average. And then as you go away from mean, the number of values goes down. In nature, we see many data sets which follow normal distribution. For example, here, they are saying heights of the people, size of the things produced by machines, blood pressure, marks on taste. These are all examples of a normal distribution where if you are taking example of heights of the people, which is this data set, you will notice that majority of the people have heights between, you know, in this area, 65 to 68 inch. And then people having around 80 inch height, which is more than, which is around seven feet is very less. So you can see number of samples are less. Similarly, number of people having very less height, which is like four feet, four feet, five inch is also less. So this let us, it is clearly following a normal distribution. Now what we're going to do is we will thought a bell curve that you saw here. So this is this yellow chart is a histogram and this curve is a bell curve and we'll just plot that bell curve for our visualization purpose. And you will need to use a scipy module. So if you have installed scipy module already, it I think comes with an account installation. So you should be good. These three lines are same as this. Okay. So you are plotting a histogram here. And then in these two lines, you are plotting the actual bell curve. Now what are these two lines? So this line is producing the range of X values. So df.hide.min and max will give you the minimum and maximum value for your height. So let me just print them here for your information. So the minimum height that we see in our data set is 54 inch, which is four feet and six inch because four feet is 48 inch. So four feet and 60 and six inch. The max height in the data set is 78 inch, which is around six feet six inch. Okay. You can also do describe and that will tell you the quick statistics on the height column. You know, here it says minimum height is 54, max is 78. Then this is a standard deviation. The count is 10,000. We saw Excel file has 10,000 data points. Okay. So once this is all done, you can now execute this cell and it will plot the chart for you. Now, NP is not defined here. So I will say import numpy as NP. And that will plot this nice looking bell curve for you. And this bell curve clearly shows normal distribution. We already saw on this particular chart. You know, if you remember this chart says shows the normal distribution here. Now, what we're going to do next is find out the mean and standard deviation. Now mean and standard deviation, we already saw in this describe parameter. But if you want to just kind of print it out, then you can say df.high.mean that will print the mean of this column and then df.high.std. This is printing standard deviation. Now, what is standard deviation? If you don't know, standard deviation is basically showing you how far away the data point is from mean value. Okay. So in this example, most of the data points are within one standard deviation. If your data set is normally distributed, 68% of the samples will be within one standard deviation value. Then 95% samples will be in two standard deviation and 99.7% samples will be in three standard deviation. Okay. So standard deviation is just, it's nothing but it just shows you how far away you are from mean. For example, our mean value is 66 inch. And if we have a data point which is let's say this one, 78. So 78 is quite far away from your max. So this will naturally have higher standard deviation. But if you have something like let's say 67, 68, then those will be most likely within one standard deviation range. So see, what this shows is if you have anything in like a 3.84 range from 66. So 66 plus 3.84 is one standard deviation away minus 3.84 is two standard deviation away. So now we will use a three standard deviation first to remove the outliers. So three standard deviation is kind of a common practice in the industry to remove the outliers. Sometimes people use four standard deviation, five standard deviation as well. If the data set is small, I have seen people using even two standard deviation, but you have to really use the sense of your judgment to come up with that threshold on how many standard deviation you want to use. So here I will figure out my upper limit by saying okay, I want mean and three standard deviation to be my upper limit. And anything which is more than 77.91, I'm going to mark that as an outlier. You can do similar thing on lower end and you can say mean height minus three standard deviation is my lower limit, which means any height less than 54.82, I will mark that as an outlier. Now let's quickly see what are the outliers in our data frame. So in our data frame, when you do something like this, where you're saying if my height is greater than upper limit or lower lower limit, then show me those data points. And I find these seven data points where the height is really high. This is six feet six inch actually. The height is an inch, you can convert it into feet an inch. 54 is four feet six inch. Now these heights are actually they might be valid heights, you know, I'm not saying these are data errors. But many times even if the data point is legitimate, we can actually remove those points as an outlier because that can help with your model or it can generally help with your data analysis process. Sometimes the because of the error, you might see crazy values such as you know, you might see let's say 2000 or 1000, those kind of crazy values. In that case, removing those outliers becomes really easy. You don't have to think much. But here, you know, I can discuss this result with my business manager for whom I'm building my model. And if he agrees, I can remove these as an outlier. Okay. And how do you remove these as an outlier? So what you can do is you can say DF, okay. And here you can specify a condition. So you can say DF dot hide. I want the height to be in this limit. It should be less than upper limit. And the height should be greater than lower limit. And say it will give you this data frame, which will have your normal like good looking data points. Okay. And this you can store in a new data frame. And this new data frame, you can just print a shape of it. And you will find that out of 10,900, 9,093 are the valid samples. Okay. So you can just say, this is the shape of your filtered out data frame. And if you do shape of zero will return your number of rows. Okay. And if you do this, you will find that you removed seven outliers using this method. Now we are going to use z score to do the same thing. Now z score and standard deviation are kind of very similar things. z score just tells you the it gives you a number which will tell you how many standard deviation you are away from the mean. So let's say if your data point is a three standard deviation away, then the z score for that data point will be three. If it is 2.5 standard deviation away, then it will be 2.5. Okay. So I'm just going to pull this other notebook and just kind of walk you through the definition of z score and the equation. So see here it's saying it indicates how many standard deviation away your data point is. For example, our mean is 66.37 and 3.84 is the standard deviation. And if your data point is 77.91, then the z score will be three because 77.91 is three standard deviation away. The equation is extremely simple. The z score is equal to you x which is your data point value which in this case was 77.91 minus mean divided by standard deviation. Okay. So this is an extremely simple equation. z score is extremely simple thing guys like don't this there is no rocket science in it. It is very similar to standard deviation. It just tells you a number on how many standard deviation you are away from the mean. And to calculate the z score, I'm going to create a new column in my data frame call it z score. And you might know that this is how you can create new columns in the data frame. Okay. And this will be dependent on my height minus height mean. Okay. So it's like x which is your this data point minus mean divided by the standard deviation. And this is how you can get standard deviation. df.height gives you a column in the data frame which is a numpy array. And then dot s t d can give you the standard deviation. And once that is done, you can print in a couple of samples. And you will find that now I have a new column called z score. And I see all these z scores here. Now you can filter out those rows which has a z score of three three or above or minus three or below. Okay. So this score three means three standard deviation. So you are basically doing exactly same thing that we did here. Okay. So if you don't want to use z score and use this technique fine. You can stop watching this tutorial. But z score is just an alternate way of doing the same thing here. Okay. So here let's say I got z score 1.94 for this data point. How did I get it? Well, all you do is you take this data point minus the mean. Okay. Now what is the mean? Well, the mean is 66.37. So you did this and you divided that by 3.84 which is your standard deviation. And that's how you got this 194. Pretty simple. Okay. Nothing to worry about. Now I will figure out all the data points which has a z score more than three. Okay. This is how you can get all the data points with z score greater than three. And you see, you got those same 78 inch data points. Similarly, if you do z score less than minus three, you will get these two data points. Okay. So 5 and 2, 7. These are the same same outliers that you saw here. So those outliers can be filtered in one shot or or let's first view all those outliers by having this condition. So I'm just doing or on these two conditions so that it will show both of them. It's a union of both of these sets. And you see, this 7 outliers. And to filter these outliers again, you can use the same approach that we did previously, which is you can say, okay, I want to keep those rows which has z score greater than minus three or less than three. And you will find this will be your new data frame. And in this data frame, when you do something like this, where you are saying df.shape 0 will give you the number of rows in the df. And this is the new data frame that you have and the rows in that. And you find that you removed say one outliers in total. All right, that's all I had now is the time for exercise and for the exercise, I have this particular data set for you, the Bangalore property prizes. So let me open that. So this is the data set I have for you. And what you need to do is first remove the outliers using the percentile technique. Okay. And then you get a new data frame. Now on that data frame, you have to use four standard deviation to remove the outliers, then plot histogram. And then you use z score of four to remove the same outliers. Now I have a solution link here, but please try it out on your own first and then only look at the solution. The link of this particular notebook, I am going to provide in a video description. So you can look at the notebook for the code that we covered in this tutorial today. And then go at the bottom, you will find the exercise description. The CSV file, you will find it here. So watch this. So if I'm on this particular notebook, you can just remove this guy here and you find a folder. And then there is an exercise folder here. If you go there, the CSV file is located here. This is a Bangalore home prize data set which I got from Kegel. So thanks Kegel for providing all these amazing data sets. And this is a solution for that exercise. But do not look at that solution first try it out on your own. And then only look into the solution. I'm going to cover many more interesting tutorials on feature engineering in the future. So keep watching, keep learning. Feature engineering is very, very important in data analysis. Whether you are building machine learning model or not, it will help you tremendously if you are targeting a data analyst or a data scientist carrier. So please pay close attention to all the feature engineering tutorials that I'm going to provide work on the exercises. And if you like this tutorial, please give it a thumbs up. Share it with your friends. It really helps with my YouTube ranking. So please share it with as many people as possible. Thank you.",
    "Machine Learning Tutorial Python - 6_ Dummy Variables & One Hot Encoding.wav": " We are going to continue our linear regression tutorial series with the discussion of categorical and dummy variables as well as one hot encoding. As usual I have home prices data here but the thing that is different is this extra township column. Till now we were trying to predict the home prices in my hometown which is monotone ship but we want to expand this problem further to the nearby towns. That's why I have home prices data for waste-finser and robinsville towns. Given this data we want to build a predictor function that can take square foot of the home and name of the township as an input and it can tell me the price. Now when you look at this data the first question that arises in anyone's mind is how do you handle the tax data because machine learning models are good with handling the numeric data. So one way to do this would be to use integer encoding or label encoding where you convert name of the town to a specific integer number as shown in the grid on the right hand side but the problem with this approach could be that just think you are assigning numbers to these townships and when you give this to your model which is working on the numbers it will assume the orders and it will make assumptions such as monotone ship is less than waste-finser which is less than a robinsville or monotone ship plus waste-finser is equal to robinsville. This doesn't quite make sense isn't it? These variables are called categorical variables and the name of the township that we are dealing with they are called nominal variable. Alright so in categorical variables there are two types. First is nominal where the categories don't have any numeric ordering in between that. They don't have any order relationship between each other. Other type of nominal variables are male, female or the name of the colors. The second type of categorical variable is called ordinal where the categories have some short of numerical ordering in between them. For example you are talking about your college degree if it is graduate or master or PhD then you can say that graduate is less than master which is less than PhD. And there are other examples of ordinal categories such as customer satisfaction survey for example where you give a rating such as satisfy neutral or dissatisfied and these you can order numerically. Now we know that we are dealing with nominal categorical variables and simple integer encoding is not going to work. Hence we are going to use a technique called one hot encoding. Pretty exotic name isn't it? So the way one hot encoding works is you create a new column for each of your categories and assign binary value of one or zero. So as shown in the diagram here for monotone shape I have put one here and remaining values are zero same thing for a waste fincer and Robin's will. Alright so we are going to use this one hot encoding technique to solve our problem and these extra variables which are created they are also called dummy variables. Alright so enough of the talking let's write some code. In my Jupyter Notebook I have loaded home prizes into this data frame which looks like this and the first step that I'm going to take is create those dummy variable columns. Pandas has this method called get dummies which will return the dummy variable columns. So my data frames category column is down and when I call get dummies method it will return me those three dummy columns for township. So if you have 10 township it will create 10 columns. I will store this data frame into this dummy variable and the next step is to concatenate this dummy's data frame with my original data frame. I will call that merged and on pandas as you know probably there is a concaten method which you can call to join two data frames. Alright so it takes an array as an input where you will supply the two data frames that you want to join. So the first data frame is df the second one is dummies. You also need to tell how do you want to join. So the axis that you want to use is of course columns and when you execute this you will get this kind of merged data frame. Now you need to drop the original town column here because you already have these dummy variable columns you don't need this text column anyway because that's not going to work on our linear regression model. So we will drop this town column. So we are done dropping this. The second thing that we want to do is drop one of these dummy variable column. Now why do we want to do it? You can google about dummy variable trap and what that concept means is whenever one variable can be derived from a rest of the variables these variables are said to be multi-coal linear and whenever you have multi-coal linearity in your data set it creates the problem of dummy variable trap that can mess up your machine learning model. Hence the rule of the thumb is you have to drop one of the dummy variable column. So if you have five dummy variables let's say you drop one and you are left with four. We have three dummy variables here so we are just going to drop one of them. You can pick any column out of these three. I'm going to drop a wish-winter column. So we are dropping two things from our merge data frame. I will just call it final and how do you drop it? So on the data frame you can call drop method and in the array you can supply the things that you want to drop. So the first is town and the second column that you want to drop is waste-winter. You need to supply axis column, axis argument sorry and the value will be columns. Let's execute this. All right, now my data frame looks pretty good. It has all numerical columns and one of the dummy variable column is drop. Now remember that when you are using S-K-Learn linear regression model it will work even if you don't drop it because linear regression model is aware about dummy variable trap and it will drop it automatically but in general it's a good practice that you drop it on your own. Now let's create a linear regression model. You all know from our previous tutorials that you need to import linear regression from our linear model and then I will create an object of this class. I need to remove this bracket from here. So let's see. Okay I have a typo here. All right, so I have created my linear regression model object and now I need to give X and Y for training. So what is my X? My X is all these columns except the price because price is your dependent variable. It is your Y. So your X is area, monotone shape and Robin's well. And to easily get this X what I'm going to do is again drop price column from my final data frame and then I have X now. You see price column is not here so it is removed since it's a dependent variable. And Y is nothing but a price column in your final data frame. All right, I have my X and Y ready. So you all know the next step is to do a training. So when you do model.fit you are actually training your machine learning model. All right, we have very less data. So it's going to take a fraction of second to perform the training. But in real life situation you will have lots and lots of data. So your training step might take more time. And whenever you have huge volume of training data and if training is taking too much time and if you don't want to do training again and again, then you can save this model to a file and then you can load it later on whenever you want to perform the predictions. And I have another tutorial on how you can save your model to a file and load it back from the file. Okay, now the most interesting part which is doing a prediction. First thing that I want to predict is price of a 2800 square feet home in Robbinsville. Let's see how my X looks and as per that I need to supply the parameter. So my X has first is area. The second one is Monroe and third one is Robbinsville. Alright, so now I want to predict the price in Robbinsville. So my second parameter needs to be one and the first parameter needs to be zero. Cool, so that's the price that I came up with. Next one is 3400 square feet home in Wiss Winsor and for that I'll just copy this cell. I'll change the square foot to 3400 square feet and let's see how my Wiss Winsor looks like. Wiss Winsor means you need to supply zero in both of these dummy variable column. So I'll make this zero, execute it. It is telling me a $680,000. Now 3300 square foot home in Wiss Winsor was 650. This is a little bit bigger. So 680 one kind of makes sense. If you want to know how accurate your model is, you can use model dot score method, supply your X and Y. And what it will do is it will calculate the predicted values for all of the rows in X. And then it will compare predictor value with the actual values, which is this Y. Remember that it will use some formula to calculate the score. Here the score is 95, which means my model is 95% accurate. If the score is 1, it means it is perfect. So you use core method after you build your model to get an idea on how accurate it is. Now we are going to use SK learns one hot encoder to essentially do the same thing. What I have here is my data frame. And in order to use one hot encoder, first you need to do label encoding on the town column. So for that I will use label encoder from a scalar preprocessing and create the label encoder class object. The class object is ready and then use that on our original data frame. So we call it df label encoder. That will be the result on data frame and I will say Ali dot. So first all right. So first let's create a new data frame. All right. I will call it df Ali and that is equal to df. The second step is use label encoder to fit and transform. All right. So fit and transform means it takes the label column as an input. So my label column is down. Okay. And it will convert it will return the label. So labels are these basically it converted. Monro into zero. Weiss Vinser is two and Robinsville is one. All right. But you don't want this array. You want to just assign it back to your data frame column. So my df which is label encoded looks something like this. All right. So I have all my town categories converted into integer numbers. Once I have that what I need to do next is create my X variable. All right. So what I need to do is create a model takes X and Y. X is a training data set. Y is your dependent variable. X is here. All right. What is my X? X is town and area. Town area because price is why right. And this time I will use values. Because I want X to be a two dimensional array and not a data frame. OK. So if you call dot values on a data frame, you get two dimensional array. And my Y is all right. Let's see. OK. I think it's a reference variable. That's why it doesn't matter. Or a df, alley dot values that will be my Y. No, it's not a values. It's price. OK. So here is my X. Here is my Y. Now if you remember, we need to create the me variable columns here. So for that, let's import one hot and border. That is available in a skill and pre-processing module. It's called one hat hot and quarter. And I will create an object of this class first. Call it OHE. And OHE, again, fit and transform. Fit and transform what? OK. Let's see. Fit and transform actually this X. OK. Now, before you do X, if I just execute this thing about it, what will happen? It will assume all my X columns to be categorical variables. Hence, what you need to do when you create one hot and and quarter is you need to specify categorical features. What I'm saying is later on, whatever X I'm supplying, the zeroth column in that X is my categorical feature. OK. And you need to do one more thing, which is convert if to and all right. So that is my X now. So my X looks something like this. This is actually 2600 square foot because of this epsilon. It looks like a weird number, but don't worry about it. It is your 2600. This one is 3000 square foot. And what you have got here is 1, 0, 0. So it created three dummy variable columns here to avoid dummy variable trap. I'm going to drop one of the columns. I'll just drop the first column. And the way you do that into dimensional array is you will say, take all the rows to the first part. This one means take all the rows and for the columns takes all the columns from index one onwards. This will essentially drop your zeroth column. And when you execute this, your X looks like this. So you see you have three columns now. Previously you had three plus one four. And in the end, I dropped this first column. Okay. All right, my X and Y is ready. So I'm going to just do a training on my model model.fit X and Y. My model is trained and ready. And now let's do some predictions. Okay. So in my X, the third column is the square foot. So the thing that we want to predict is 2800 square foot home in Robbinsville. So that will be my third column. My first column is actually, let's see, it is Wispins. It is Robbinsville. I think. Yeah. I have Monroe Wispins and Robbinsville. And if you look at this X here, the last one is Robbinsville. Okay. So one means Robbinsville. The second column is your Wispins. Okay. This is expecting two dimension array actually. So I'm just going to put extra bracket. All right. So that's the price of my home in Robbinsville. The third column is the total number of three hundred square foot home in Wispins. And you notice that it gives me the same prediction as before. 681, 590. If you compare that with the result here, 681 and 590. So these are matching. All right. We just saw two methods of creating dummy variables. One was pandas get dummy method. And the second one was escalant pre-processing one hot and colder library. Okay. That's all I had for this tutorial. Now I have an interesting exercise for you guys. So let me open my exercise folder. And what I have here is the prices of different cars based on their mileage. So these are the used cars which were sold recently. This column contains how many miles is there on their automata, their cell price and the age of the car. Okay. Now given this data, what you need to do is you need to predict the price of a Mercedes Benz that is four year old with a mileage of 45,000 miles. Second thing is you need to predict the price of BMW X5 that is seven year old with a mileage of 86,000 miles. And then in the end, you have to tell me the score of your model. So first plot this data on a scatter plot chart and see if you can really use linear regression model because choosing the correct model is very important. So if you want to make sure that linear regression model is going to work and then you'll probably use dummy variable for this car model and then build a regression model to get the answers of these questions. The notebook used in this tutorial is available in the video description below and the exercise folder is available at the same level as this notebook on my GitHub page. So go ahead, find it out and do this exercise. And in the comment section, let me know the answers that you find out.",
    "Machine Learning Tutorial Python - 8  Logistic Regression (Multiclass Classification).wav": " This is part two of Logistic Regression Tutorial. If you haven't watched the first part, then you should watch that first. In the previous tutorial, we discussed about binary classification, where the output classes are binary in nature. They are either yes or no. And this one, we are going to discuss multiclass classification. For example, when you are trying to predict which part your person is going to vote for, the possible outcomes are one of these three. The concrete problem that we are going to solve today is to recognize the handwritten digit. For example, here, this one maps to one of the output categories, which is nothing but digits 0 to 9, similarly here for maps to this particular output category. So we will use a training set with lot of hand digit characters, and then we'll build a model using Logistic Regression. And at the end of the tutorial, you will have an interesting exercise to work on. So let's jump straight into writing the code. As usual, I am going to use my Jupyter Notebook as an IDE. And here I have imported Matplotlib, and also Psychit Learns dataset. So SKLearn.dataSets has some predefined readymade datasets that you can use to learn machine learning. From this, I am using load digits dataset. So if you read the documentation, all it is is 1797 handwritten digits, upsize 8 by 8. So it looks something like this. And what we are going to do is given these digits, we are going to identify that. What digit that is. So let me just run it. So this has run fine. I am going to now call load digits method to load my training set basically. And I want to explore what this training set contains. So it contains a couple of things. It has data which is your real data. So let's print few elements. So as it's written in the documentation, there are 1797 samples. So I am just going to print the first one. And it's an array. As such, it is an 8 by 8 image, but the image is represented as a one-dimensional array. So if you count these elements, it will be 64, which is 8 by 8. And if you want to see this particular element, then you can use matplotlib. So I am going to do plotplt.gray. And plt has a method called matshow. And what you can do is you can print the corresponding image. So data has the numeric data and images will have the actual images. So you can see that data 0 and image 0, they kind of relate to each other. And the only difference between the two is that you have numeric data here versus you have an actual image. So if you want to print, let's say first five samples, then you can just print it like this. And you will see that, see 0, 1, 2, 3, 4. Okay. And corresponding numbers will be in this data array. So that looks pretty straightforward. Now what we are going to do is use this to train our model. Now before we do that, let's take a look at target and target names. Okay. So our target. So if I print the gist.target 0, let me print 0 to 5. So you see like 0 to 5 is literally in the sequence, the first element is 0, 1, 2, 3. And that's what this is printing here. It is saying that this image is 0. The last image, this is 4. So this is our complete training set, which has our image as well as the target variable, you know, like it says what it is. So we can use data, data and target to train our model. Now before training our model, the usual thing that we do is we import from model selection, we import train, test split. And we try to divide our data set into a training and test samples. So the way you do it is you say, X train, X test. I don't exactly remember the order of the argument. So I'm going to what I'm going to do it. Okay, let me do this. So to intes split digits.data because that's your data set. Then you have digits.target because that's your target variable. Okay. And if you hit shift tab, it will show you all the nice documentation of that API. So here it says this is the order in which it returns the output. All right. So now what I just did by executing this command is I had input at output variable from my training set and I divided them into test and train sets. Now the reason that we do this typically is we don't want to overfit our model. We don't want to make our model such that we just bias it against the training data. That's why the data that the model is trained against should be different than the data that the model is tested against. Okay. So that's why we split these two. So if you look at, okay, I have to supply the size. So I'm going to probably supply test size, test size. So I want 20% of my samples to be test size and 80% to be the training. Okay. So if I look at length of X train, it is this. And if I look at length of X, it is this. So this is roughly 80% of all available samples. All right. So I have a training and test data set split. Now I can create my logistic regression model. So from this, I want to import logistic regression and create a model object so that you can train it later and you all know the way you train it is by calling a fit method. And fit method, you will call it against X-taste X train, sorry, and Y-train. When you run that, the model is getting trained using this X train and Y train data set. So again, to repeat X train has the hand written characters and Y train will have the corresponding output. It will say, okay, for this image, it is four, et cetera. Now since my model is ready, the first thing I always do is I calculate the score. So the score tells you how accurate is your model. And the way you do that is you have to supply X-taste and Y-taste. So using the X-taste, it will calculate the Y predicted value and it will compare those Y predicted value against the real value, which is Y-taste. Turns out that my model is doing pretty good. The accuracy is 96.67 percent almost, which is really good. So now I'm going to make my actual prediction and you know that you have to call predict method for that. Now let's see. So before I call this method, what I want to do is I want to pick up a random sample. So I will say plt.mat show digits.images. Let's say I'm just picking up a random sample, okay. This is pretty hard. Even I don't know what this number is actually. Let's see. So this number is actually digits.target.67. So you have to access the same index in your target. Okay, so this is six. Okay. So let's see what our model will predict for this guy. So I will say model dot predict. Okay, model dot predict what? Okay, what do I want to predict? I want to predict. Now see, I'm not going to supply images here because images are all binary data. My model likes numeric data more. So I will use the same index 67, but I'm using data instead of images. Okay, this is the error. You get when you're not supplying multi-dimensional arrays, I'm just going to supply multi-dimensional, I just for the sake of it. And you can see that it is predicting the target variable. All right. Okay, let's just, okay, let me just create a new cell here and let me predict. Okay, what do I want to predict? Okay, I want to predict 0 to 5. Now you all know 0 to 5 is literally 0 to 5. So 0 is 0, 1 is 1 and so on. When executed, see, my model is doing pretty good. So my score is 0.96. How do I know where it didn't do well? Okay, because all the samples I tried seems to be doing pretty well. So I want to know where exactly it failed and you know, I want to get overall feeling of my model's accuracy. And one of the ways of doing that is confusion matrix. So I will show you what confusion matrix is really. For that, I have to import from this matrix, I need to import confusion matrix. Okay, and then before I do that, I need to get the predicted values. So I will say, predict x-taste. When I run that, I get all the predicted values for this x-taste. Okay, and then I create a confusion matrix. And in the confusion matrix, what you supply is y-taste, which is the truth and then y-predicted, which is what your model predicted and then you get confusion matrix back. When you run that, you get this 2 by 2 dimensional array and you are wondering what the heck this is. So this is better visualized Matplot Labor Seabond. So I will use that library for the visualization here. I'm just going to copy paste the code for confusion matrix visualization. Here I'm using Seabond library, which is similar to Matplot Labids used for visualization. And I'm calling a heat map here with the confusion matrix CM variable that we created here. And when you run that, this is the confusion matrix that you got. Now, the way this works is, see here you see 37 number. What it means is, 37 time the truth was 0 and my model predicted it to be 0. Here, this 2 means, 2 times my truth was 8, meaning I fed my model the image of 8, but my model said, no, it is 1. So these are the instances where it's not doing good. So you can see that in anywhere in this area and this area, when you don't see 0, it means your model is not working right. So here, for example, again, 2 times my images were off digit 4, but my model predicted it to be 1. So that's what this is. So confusion matrix is just a nice way of visualizing how well your model is doing. All right. Now it's the time for exercise. Today's exercise is going to be using a scale on datasets Iris flower dataset, which has following four features. So if you don't know about Iris, Iris is a type of flower and the flower has a two type of leaves, you know, one leaf is called, one leaf is called sepul, the other one is called pattle and they have like a height and width. And based on these height and widths, you can predict what kind of Iris flower it is. Okay. So our dataset will have three kind of flowers. These are the names of three different Iris flowers. And the features that we have are these four, which is basically pattle, width and height and sepul, width and height. And you will use this dataset, the iris dataset, and you will load all those 150 samples, then divide them into test and training samples, and then build a logistic regression model and tell me the accuracy that you can come up with. And then you can just do a few predictions using that model. All right. That's all I had for this tutorial. I have the link of this Jupyter notebook down below and you can find the exercise also. So make sure to refer to those useful links and please, please do some practice yourself just by watching this video. You are not going to become expert. All right. Thanks for watching.",
    "Outlier detection and removal using IQR _ Feature engineering tutorial python # 4.wav": " In this tutorial, we will be removing outliers using IQR technique. As usual, we'll go over some theory first, then we'll write code in Python Pandas to remove outliers. And in the end, we'll have an interesting exercise for you to solve. Here I have a Heights database. Now visual examination, you can easily tell that there are some outliers here. These Heights are in feet. Of course, you cannot have a person with 40 feet height. Hence, all of these are outliers, you know, anything more than 70, 7.1. Similarly, this is an adult data set, hence 1.2 height is clearly. These two Heights are outliers. Now how can you detect the same thing using an IQR technique? In order to understand IQR, you first need to understand percentile. Now what is percentile? Well, percentile is basically, for example, here I have this 5.35, which is 25th percentile. It is also called Q1. 25th percentile value means in your data set, 25 percent of samples are below this value. Total samples here are 20. 25 percent of 20 is 5. And 5 samples here are having value less than 5.35. You see 5.25.1. All these values are less than 5.35. Hence, this is 25th percentile. Similarly, 75th percentile will be a value, which is 6.27. So 75 percent of samples are having value less than 6.27. I hope you get it. So that's why 6.27 is called 75th percentile. Your maximum value will be called 100th percentile because of course all the values are below that value. Similarly minimum value will be called 0th percentile because that doesn't have any value less than that particular value. So this is a quick summary of percentile. Now once you know percentile, you can easily calculate IQR by dipping Q3 and Q1. Here Q3 which is 75th percentile is 6.27 minus 5.35 is Q1. 0.925 is your IQR value. Now to remove outliers, you need to come up with lower and upper limit. The lower limit will be Q1 which is 25th percentile minus 1.5 IQR. Now IQR if you think about it, it is this distance. It is this much distance. So you take that distance multiplied by 1.5. So you add half into it and then you extend that from here. So from 5.35 minus 1.5 and you get this value 3.96. Similarly upper limit will be Q3 plus 1.5 IQR. You know they just came up with this number. You can just remember this that you have to do 1.5 IQR plus Q3 to get your upper limit. Once you know the upper and lower limit, you can exclude all data points which has value less than lower limit. So 3.96 C. These two are excluded. They are termed as an outlier and any value above upper limit which is 7.66 can also be excluded. So you can clearly see that these three values are now excluded. So just by looking at this data set visually, you can kind of say that IQR is kind of working okay in this case because most of the people will have heights in this range. So having height 2.3 feet is very, very, it's probably not possible and adult having a height of 14 feet is also not possible. The person doesn't need stairs if he has this much height. He can just go to your first or second floor. Alright let's get into coding now. I have that height CSV here and I'm going to load that into my notebook and it will look something like this. So here is my data frame and now from this data frame, I will quickly do DF does subscribe describe to see the statistics so you can see that when this is 25th percentile value, 75th percentile value, maximum is 40, minimum is 1.5. I always use describe function because it can tell you those quick stats. Now let's calculate Q1 and Q2. Now by the way you can get Q1 and Q2 from here as well but the other way of getting Q1 and Q2 is you take your height column and you say quantile. Quantile is a function so quantile and if you want 25th percentile, this is what you need to do and if you need 75th percentile, this is how you do it and I will quickly print Q1 and Q3. So these are my percentage value. IQR will be very simple formula guys. IQR is really a very very simple technique. IQR value is this and now I can easily find my lower and upper limit. Using this formula. So I am doing Q1 minus 1.5, QR, Q3 plus 1.5, QR, these are my upper and lower limit and once you have upper and lower limit in a data frame, you can easily detect these outliers by doing this. You can say height less than lower limit or height greater than upper limit. Those are my outliers and you can clearly see see all these are coming as outliers. These are the same ones that we saw in our presentation. The sales mark in the yellow color. Alright now if you want to remove these outliers from your original data frame, what you can do is something like this. So I am using end condition. I want to make sure the remaining samples are within this limit of lower limit and upper limit. Okay. And the resultant data frame that you get, you can maybe store that into a new data frame, call it DFNOW outlier and this is how it looks. Alright so that's all I had. Now the most interesting part of this tutorial which is an exercise. Believe me I am not giving you any hard assignment. These are very simple assignments but it will consolidate your understanding once you practice it on your own. What you are going to do is use the heights and weights data set. It is very similar to what I used but it has height and weight 2 parameters. I got this file from this Kegel link. You can download it or if you don't want to download it you can just go to my GitHub page and you will find it in the exercise folder. And then you need to plot histograms first for weight and then for height. And then use IQR to detect outliers based on weight and then do the same thing based on height. Now I have a solution link but I know you all are very good students since the students. So you are not going to click on this link until you have practiced this on your own. Alright thank you very much for watching. I will see you in next feature engineering tutorial.",
    "Machine Learning Tutorial Python - 20_ Bias vs Variance In Machine Learning.wav": " I'm going to explain bias and variance using a very simple language and you are already wondering what's the meaning behind showing all these three pictures. Well, look at the pictures carefully and try to find out what I want to show you here. The first picture of course shows a t-shirt that I'm wearing which is too much fit basically it's over fit. If my body increases or decreases you know if I start eating junk it will not fit. The second one is under fit which is too loose basically you look at my sad face here clearly it indicates what's the issue here. The third t-shirt however is a perfect fit where even I grow in size a little bit or or if I you know thin up a little bit it's going to be okay. Some concepts apply in machine learning world where your model can be an over fit under fit or balance fit and we're going to look at these scenarios using a simple housing price prediction example where we are using one feature which is the square foot area of a home and based on this feature we are trying to build a machine learning model that can predict the home price. Here I have a scatter plot of all my training data set basically my data set super for the supervised learning and in this scatter plot when we train our model what we do is we split this data set into training and test samples. Let's say all these blue dots are my training samples the orange dots are my test samples and we can train a model that fits to these blue dots. Let's say our model is such that our training methodology is such that we end up in an over fit model and over fit model tries to fit exactly to the training samples where your training error becomes close to zero but the problem here is the test error can become high because once you have this model train which is the screen line now let's say you want to figure out an error for this particular orange test data point the predicted value will be somewhere here and the error will be this gray dotted line and you can measure the error for all your test data set and average it out let's say you get this error as 100 I'm just making this up let's say this error is 100. Now remember one thing when you have a data set and when you pick your training samples you pick it up at random you have trained test plate let's say 80 to 20 person you pick your training samples at random there's a different data scientist they might be choosing different set of training sample let's say this and their model might look different here they're using the same model same methodology so your training data set error is still zero because you both are trying to over over fit the model but the problem that happened here is your test data set error is let's say 27 you can compare these two images and you can easily see that the test data set error here is much higher than the second scenario you see all this you just sum up all this gray dotted lines here and some of the gray dotted lines here this error is much less than the previous error so if you compare these two scenarios side by side in one case your test error is 100 second case test error is 27 which means the test error varies greatly based on your selection of training data point and this is called high variance because there is a high variability in the test error based on what kind of training samples you are selecting now you're selecting training samples at random so your test error varies randomly which is not good and this is the common issue with over fit model they tend to have high variance so high variance again just to summarize is high variance happens when your test error varies by great deal based on your selection of training data points so just to remember when you're talking about variance high variance you're always talking about test error let's look at another scenario where I've same data set I have split into train and test samples and this time instead of having a complex model that overfits my training samples I'm trying to come up with a very simple model there's a linear equation which is under fitting your training sample because linear equation is a straight equation it cannot truly capture the pattern in your training samples okay the straight line cannot pass through all the blue dots so it's a simple model where your training error is actually high in previous case the complex model was complex your training error was zero here it is lesser 43 and your test data set error is around 47 again 4347 these numbers I've just made up to explain you this concept when you selected different set of training data points see in both the cases the line will be different but your training and test data set error is still kind of similar you know in the first case it was 47 second case it was 37 so this is called low variance because based on what data point you select as your training samples your test error doesn't vary that much 4737 that's still similar it's not like 121 you know so your test error doesn't vary too much hence low variance but if you look at your train error in the previous case your train error was zero here you have some big train error and that is called high bias high bias is a measurement of how accurately a model can capture a pattern in a training data set so when you're thinking about bias you're always thinking about train error when you're thinking about variance you're always thinking about test error bias is basically your model's ability to capture the pattern in your training data set in an accurate way in the previous case where we had overfit the model the bias was low because a train error was close to zero so if the higher the train error higher the bias okay now let's look at the idle scenario where you come up with a model that kind of accurately describes the pattern that is available in your training data set here your training error and test error both are low even if you select different set of training data points see the training data points here are different but still your model selection is such that your train and test error both are kind of low and in this case it's called low variance low bias model because your test error doesn't vary too much based on what training samples you have selected also your train error in general is low so that's why it's called low bias so we already cover this concepts like whenever you have an overfit model it's likely that you will get high variance when you have underfit model it is likely that you will get high bias and when your balance fit you will get low variance low bias and as a data scientist you want to come up with a with a model that has a balance fit you know just like our t-shirt you want to go to a store and buy a t-shirt you will always buy a balance fit t-shirt now there is a bull's eye diagram where the central circle represents the truth this is your truth samples okay and when your predicted values so these wide diamonds are let's say your predicted values when they're near to the inner circle which is your truth it is called low bias bias is all about how close you are to the truth so if you are closer to the middle circle you have low bias if you are far away from the middle circle you have high bias but in both the cases these diamonds they are clustered together you know they are like a close family they're together so whenever that together it is called low variance in high variance they're scattered apart you see they're scattered apart but they're still near to the truth which is the inner circle hence there is a low bias. Voskey scenario is high bias high variance where you are far away from the truth which is this middle circle that means you have high bias and also these individual predicted values they are separated out hence you have a high variance if you remember this image I think remembering bias versus variance trade-off is going to be easier for you how do you get a balance fit model so there are several techniques and the first one is cross validation I have had a video on this before so if you watch it you'll get an idea another one is regularization L1 and L2 dimensional to reduction ensemble techniques bagging boosting I'm going to be making videos on this in next 10 days maybe so those videos should be available so these are different techniques to get a balanced model I hope you like this video if you did give it a thumbs up and share it with your friends thank you if you have any question post in our comment box below",
    "Data Science & Machine Learning Project - Part 3 Data Cleaning _ Image Classification.wav": " In last video, we looked at data collection for our image processing project. In this video, we are going to talk about data cleaning. In any data science project, majority of the time is gone in data cleaning process. And you will see that in this project also, we will be spending significant amount of time in cleaning our images. Because when we download our images from internet, the images might have a lot of issues. Now when you want to detect a person from that image, just think about it. Like when I show a photo of a person, how would you go about detecting that this person is X or Y? Majority of the time, you will be using the face of a person. Now using the hide hands and legs, you can tell about a person to some extent, but your final decision of who that person is is mostly based on the face. And we are going to use the same concept. All the images that we downloaded from Google, we will first try to detect the face of the person. Sometimes face might be obstructed. So we want to detect if the face is clearly visible or not. Now how do you detect that? So you will try to detect two eyes as well. So if in a photo, you can find a face with two eyes clearly, then you will keep that image, otherwise you will discard it. For face detection and detecting the eyes, we will be using open CV, which is a famous image processing library in Python. And for the specific detection, we will be using a technique called hard cascade. That's a famous technique on how you can detect the face and the two eyes. And we'll see how exactly you can do it. It's pretty straightforward. At the end of this tutorial, you will have a clean data set that you can do for the feature engineering. All here are some sample images from our data set. You see a lot of variety here. For example, here, Serena Williams face is not clearly visible. In these two pictures, along with Virat Kohli, there are two other people here is Anuska, his wife and Amestoni. In this picture, Massie's face is visible, but it is only a side face. So his two eyes are not visible. Now how do you handle these different images? Let's look at a very simple example. So this is the image of Maria Sarapova. Here the face and two eyes are clearly visible. So we'll detect this using open CV. And once you detect face and two eyes clearly, you keep that image. In this photo, there are two faces. So first we'll detect those two faces. And using open CV, you cannot say that this is Virat Kohli versus this is Dhoni. Once we can only tell you that there are two people. And here are the regions where they have their faces and two eyes visible. So once you get these two cropped faces, we'll have to run a process of manual verification. 80 to 90% of our data cleaning is happening through our Python code in automated way. But there is 10% where you have to spend manual effort in cleaning your data. So we'll delete image of MS Dhoni in the manual verification step. In this case, Serena Williams face is not visible properly. Hence we will not use this image in our classification. Because if we do, then our classifier might make mistakes. So we want to make sure our classifiers accuracy stays high. In this image, although we know messy from the side face for computer, it might be hard if two eyes are not visible. And hence we will also discard this image. So just to go over the overall data cleaning process, you had raw images. You created cropped faces. You detected basically the faces out of all these images. You also discarded some images where faces were obstructed, such as masses and Serena Williams pictures are not visible here. Then you run a manual data cleaning process where you delete unwanted images. In our case, we are doing a sports celebrity classification only for five players, which is Kohli, Roger Federer, Serena Williams, Lionel Messi and Maria Saraboa. And here there is an image of Anus Kasharma and Dhoni who are not our classification classes. Hence we are just deleting those images. In the next video, we will be looking at wavelet transform and how you can do feature engineering to extract the features from the cropped image in an effective way. We'll then use this wavelet transform images as well as the raw images. We will do vertical stacking of raw and wavelet transform image and train our model and then hypertune it. Once that is done, we will save our model to a file and we have already looked into this architecture where we'll write Python's Flassover around it, which will be serving to our website. Let's jump into coding now. In C code dry, I have created sportsperson classifier folder where I'm going to host all my code for this project. I have already created three different folders. In the model folder, we'll do a model building and the training. The server will host Python Flassover code and UI will host UI of course. It's obvious from the name. If you go to model, I have placed my data set, the Google images that are downloaded. Into this data set folder. You can see that there are five people here. If you look at Maria Sarapova, all her images will be in this folder and same applies to other players. You have already done that. The other folder I have is OpenCv. I will talk about that folder a little later and I'm going to provide this folder on my GitHub so you check it in a video description below and you will be able to download this folder. I also have Requirement.txt which contains all the modules that you need to install. The way you install it is you go to your command prompt and see here right now I am in this folder so I can just go to my model folder and just say, PAPE install minus R, Requirement.txt and this is how you install all your modules. Now I have already installed this module so it's going to say these modules are already installed. If you get this kind of error where it says access is denied, what you can do is you can start Git Bash as an administrator. So if you run it as an administrator, you will be able to install things okay. I assume that you already have Anaconda installed so you install Anaconda and on top of that you have to install these three modules. I'm going to provide Requirement.txt file which is this in my GitHub so again you have to just download it and just run PAPE install minus R on Requirement.txt. I also have this test images folder guys where I have a couple of test images to try a few things and with that now we can start our Python coding. So I went, I opened Jupyter Notebook, I created a new notebook by going here, I call it Sports Person Classifier Model and imported a couple of important modules here. This is a CV2 module which is open CV basically and it will be helping us a lot throughout this project. So the very first basic thing you can do in CV2 is read an image. So here from the test images folder that I have, I'm reading an image of Maria Sarapova. So this is a beautiful looking image. I'm just downloading it and when you read this image in CV2 you will realize that the shape has three dimensions. So this is X and Y, X and Y coordinates and the third dimension is the RGB channels. You know that any color can be represented using RGB values and therefore you have this third dimension for your RGB values. Now if you want to quickly show that image you can use PLT which is Metplotlib basically and PLT has this method call IM.Show and that will show you this image. Now when you look at this image it's a colorful image basically which has RGB values and if you want to change it to a gray image you can do something like this where you can see that it is removing that third dimension that you have. Alright and the gray values, I mean ultimately they are all numbers. It's an dimensional array with numbers from 0 to 255 and when you plot a gray image using again a Metplotlib. Metplotlib has this IM show function you can use CMAP gray, the gray image looks something like this. Now where you are going to detect the face from this image and also the eyes. Now if you look at open CV documentation they have this nice article on how you can detect face and eyes using hard casket. We are not going to go too much into detail on what is hard casket, how it works because it requires a long discussion. But just to give you brief idea you have this line and edge features and it will use a moving window of this edge features to detect where is your nose and where is your eyes. For example in this image when you have eyes the area of eye tend to be more darker than the area below. Similarly when you have nose the area of eyes tends to be darker and the tip of the nose will be little brighter. So you can use all this mask to detect these areas. Open CV documentation contains this ready made API which you can use to detect the face and image. So if you don't want to bother too much about the hard casket and inner workings of it just assume that there is this cool technique called hard casket which helps you detect face and images and your result will look something like this. So I am going to try the same code here on our image of Maria Sarapova. Now going back to our folder structure let's see here we have this open CV folder which I downloaded from open CV GitHub and it has all these hard caskets. So what are these hard caskets? They allow you to detect different features on the face. They allow you to detect face, eye, left eye, right eye. So these are the different XMLs or pre-trained classifiers that you can use for detecting various features. And I am going to first try face. So and I am just copying pasting the code from my other notebook so that it saves me time on typing because there is like lot of code that will be writing. So I loaded that XML file which is called front face default and also I loaded eye casket. I am not using eye casket for now. So when I load face casket and when I say detect multi-scale from this gray image what is this gray? There is nothing guys but this gray image of Maria Sarapova. On that you are saying now detect me faces and what it return is an array of faces. So if you had two faces it would return two faces right now it return only one face and this is an array of four values. So what are these four values? So it is your x, y, width and height. See this image has this scale so 352. So 352 see you see 300 here so 352 will be somewhere in between this and 38 will be somewhere here. So see at this point the face starts and the width and height is 233 so you go 233 here 233 here and this will be your face. So let's draw that face so that we know how it looks. So since faces is a two dimensional array we are going to detect the first face we are going to store that first face in x, y, w and h values. Once you have this you can now crop that face not exactly crop but you can draw rectangle around that face using open CV. So in open CV you can say CV2.rectangle in my image so IMG what is IMG? Well IMG is my original image and in that IMG image I am saying draw rectangle with a red color. See this is RGB. R is 255. That's why this is going to draw red rectangle and the rectangle dimension will be it will start with x and y and then x plus w and y plus h. And I will store that into my face image and when I draw it you get something like this. So now my face is very clearly detected. Now I am going to draw the two eyes. So this is the code nothing fancy about it. The open CV documentation has this code so I have just done copy paste from there. So what we are doing is we are iterating through all the faces. In our case even if you don't have full loop it will work because we have just one face. And for each of the face we are first drawing face image. So see face image is nothing but this. So we are doing that and then we are applying eye cascade. So eye cascade will give you eyes and you might have multiple eyes. So you are going to run full loop on those eyes and again do the same rectangle but you see I am doing rectangle in now green color. So this is RGB. This is RGB. This was read before. Now I am doing that in green. This code is extremely simple believe me. Now you get these two eyes detected. In this code RY color was nothing but the rectangle region that read rectangle that you are seeing for the face. So if I just plot RY color you will notice that I get a cropped face and this is something we are interested in. I am calling it RY because it is region of interest. We are interested in the facial region of every image in our dataset. So we will be cropping the face region from all the images and we will store this cropped images into a different folder and use that for our model training. So now what I am going to do is write a function where I can input this image. For example I had this image right the origin of image. Let's say I have a function where I input this image and function returns me the cropped face if the face and two eyes are detected clearly. And that function I can run on all my images. So let's write that function. It's the same code. I am just creating a simple function out of it. So my image will be supplied using an image path as an input to this function and it will read the image. It will then convert it to gray and then detect the faces first then you go through all the faces and if the number of eyes that you get in your face is greater than equal to two then it returns you the region of interest basically. So let's try this function on Maria's image. Whatever we did previously we are now doing the same thing using the function. So see this is the original image I have and let's see what kind of image is returned by this function. So I am calling this function on that image. I am passing the path in that and that function is returning me the cropped image. When I plot that image it looks something like this. So this is pretty cool because this way I get a cropped image. Now if the face is not clear and if the two either not clearly visible we want this function to return nothing because we want to ignore that image. Now if you look at our test image folder so let's see. In our test images I have the second image of Maria where her face is actually obstructed because the two eyes are not clearly visible. So I don't want to use this image for my classification purpose. So let's see how our function behaves for this particular image. So first I loaded this image and I plotted it here so this is how it looks. Now I am going to call my function. See I am calling this function on this image, Sarapur, which is this image. And when I run this function I get nothing. So cropped image noise is now none which means the face is obstructed and I don't want to use this image in my model training. Alright now in my dataset folder I want to create a new folder called cropped which I will do programmatically. In that I want to store all the cropped images. So if you look at my original images they are original and they have a lot of things. And I want to generate a cropped folder. So how do I do that? The first thing I am going to do is initialize couple of variables. So dot slash means the current directory. So my current directory for this notebook is the C. Here is my notebook. This is my current directory and my dataset is in dataset folder which you can see here. And my cropped dataset. So CR means cropped dataset. I am going to store in the cropped folder. And first let me store the path of all the induces subfolders in a Python list. So I am using a Python OS model. And when you do OS dot scan directory what it will do is it will go through all the sub directories within my dataset folder. So my dataset folder has how many directories these five directories. Those names of those directories are going to be stored in this image directory variable. So if you print that variable it will look something like this. Now I have complete paths of induces of folders for each of these players. So now what I am going to do is if cropped folder doesn't exist then I am going to create it. So right now there is no cropped folder inside my dataset. Okay. So let's see. So this is my dataset folder. This doesn't have any cropped folder. You can see that. But this code will generate that folder if it doesn't exist. See now I got cropped folder. So what this code is doing very very simple code. What I am saying that is if the folder exists OS dot part dot exists means does this folder exist? No. Oh sorry. Here what it is doing is if the folder exists then I am removing it. So that if you are doing multiple run then if you have some old image you want to clean it. So first thing is if the folder exists remove it. Then this line will create that folder. So make directory will create that folder. So now I have this folder. Pretty cool is looking good guys. So far life is pretty good. We have no issues. Now what we are going to do is we are going to iterate through each of these image directories. So for image directory in image directories. Okay. So I am going to iterate through all these images first. And I am going to build let me just copy paste some variables here. So I will need these two variables. I will explain you the reason. But crop image directories nothing but the it's similar to image directories but it contains the cropped folder path for each of our five players. So when I go through this image directories first thing I want to do is what is my celebrity name. So I am just again going to copy paste. I am doing copy paste just to save time. So when you do this what happens is see what is my image director first of all image directories this. Okay. When I split this string by slash it will give me these two tokens data set and Lionel Messi. Okay. And these two tokens will be stored in a list. And you know in Python list when you do minus one it will give you the last element from the list. So here what's happening is I am splitting all these strings one by one and taking the last element which will be the name of the celebrity. So if you want to just verify really quickly and if you say celebrity name you see you are now getting celebrity name in this variable called celebrity name. All right. What is my second thing now? So my second thing is I want to now I trade to each of these folders and I trade through all those images. So let's see what is my folder. So my folder here is less a Lionel Messi. So now I am going to go through all these images one by one and use that get cropped if two eyes function to create a crop image. Okay. OS dot scan direct is a nice function. You supply the image directory it will tell you. It will give you the iterator which can help you go through each of the images or each of the files from that folder. Okay. So my entry. All right. So the entry dot path. Okay. So the third path will have the path of that image on this path I want to call my function. So when I call my function in RY color I will get the cropped face if the face and eyes are clearly visible if they're not what will be the value of RY color. We already saw that the function will return none. See if you look at this function it returns only if the eyes are clearly visible. Otherwise it returns nothing which means it's none. So now what we have to do is first we have to check if RY color is not none. If this is not none which means my face and two eyes are clearly visible. In that case you can store that image into a crop folder. So first you need to get individual folder for the celebrity. So what is path to see our data? So path to see our data is data says less cropped. Okay. So let's see. So it will be this. In this crop folder you want to create a sub folder for your player first. So who is a player? Well celebrity name. Celebrity name is the player. So you are going to create a cropped folder which will be path to CR plus celebrity name which will be data sets, slash crops, slash\u5229onal missy. If you print this name of the folder it will print that. And if that folder doesn't exist. So here see if this folder does not exist then your first thing is you create the folder. Python's OS module is pretty handy. You just call OS.make.dfs and it will create that folder for you. All right. So now I am going to just print this folder so that we know you know it's generating this folder. If you want to run this code just for fun you can run that. See right now it is doing cropping. We are not saving the images yet but I want to just show that you can see it's going through all those\u5229onal missy images creating cropped images and generating a folder is still working on it. Now see it went to my RSARAPOVA and it is saying generated a folder for my RSARAPOVA and so on. Okay. So I am going to stop this cell. So if you click on this button it will stop this execution because I wanted to just demo the code so far. If you look at your cropped folder see\u5229onal missy and my RSARAPOVA. You got two folders but they were empty. So I just delete it. I will finish rest of the coding and then we will run the same code block again. Okay. So I have now cropped folder. What I am going to do is now I have this cropped image directories. That is nothing but a list of your cropped image directories for each of the sports person. In that I am going to append the cropped folder. Okay. So this is just a helper variable. It will help us later on. So that is what it is. The code so far is extremely simple. Okay. I hope you are understanding it till this point. If you don't take a pause try to think about it. No rocket science guys. This is extremely simple project. All right. Now I am going to do one more thing which is once you create a folder outside that if log I want to generate the name of the file. So name of the file I will just call it like you know like Lionel Messi 1.png Lionel Messi 2.png and so on. I want to keep it that simple. So then I need a count for one two you know so that's where I have count here celebrity name count dot png. Now count I have not initialized here. So I realize I need to initialize that in here. Okay. And this will be the name of the file and this will be the full path of that file. Now what we are going to do is this ROI color we will save that as an image in this cropped file path. Very simple. Now how do you do that in open CV. CV 2 dot I am right. Very simple. What is the first argument your file path second argument your region of interest. Whatever you got back from your cropped get crop image if twice that's an awesome function we wrote guys. We made a big achievement by writing that function. Although that function is a big achievement there is one short coming I will tell you so that you don't complain later on. If you have two images is going to return only first image. Okay. So if you want to make this more robust I'm kind of feeling lazy but if you want to make it more robust you can return to ROI colors as an array and then save those two images. But you know sometimes I feel lazy and I don't care about little details. Because guys understand my schedule is very busy and I'm doing this YouTube thing on side and I don't have time to go too much into details. You know although I try my best but you understand my situation. Okay. Enough of a side talk. Now once you execute this line your cropped image is stored in cropped folder amazing. But we need to do one more thing. We need to store the name of all those image file paths into a dictionary. That dictionary will be useful later on. So this is that dictionary. Celebratory file names dictionary. So what is this? So the key in this dictionary will be the name of the celebrity and the value will be the list of file paths. So it will look something like this guys. See. You'll say Leo. Leo. Now let's see. And it will have the file path. You know like data says crop messy whatever messy one dot png. Messy to dot png. You know you want this kind of dictionary so that it helps you later on. We'll see how it helps. But you know like you get an idea this is very easy. You are creating a dictionary where you have the path of all your crop images stored here in this beautiful looking dictionary. See this is the dictionary I'm trying to create that is celebrity file names to dig. Now here I can append but I need to initialize the key of this dictionary. Somewhere here. So how do I do that? Well you can just do this. Okay so what is this? So when a dictionary is empty the first key is Leo and messy and you are creating a blank array. So blank array will be this value. And once you have blank array you can insert all these image paths one by one one by one friends that's what I'm doing here. Alright now looks like my code is ready for big execution. So I just want to see that I'm not doing any bluff. I have only crop folder it is blank and now I'm going to control execute. So now it's executing the code. It is going through all the images generating the crop images. I paused this video while this was going on because we were doing some heavy lifting based on your computer speed it might take you few minutes. But after this execution is complete for this cell let me show you how my crop folder looks like. So crop folder you see now it has five beautiful subfolders in each of these subfolders I see the crop images see. I see crop images guys guys and girls crop images my life is pretty cool. See okay. Now one issue you notice is we have cropped images but then we have an image of Venus Williams she's a sister of Serena Williams we have this image which looks pretty blurry then we have this image I think this person is Serena's husband I think if I'm not wrong see. So what is this person doing in Serena's folder so what happened was this image so this image see it has this face so it just return that face into a crop folder. So using Python you can crop this images but you can do the data cleaning to only certain extent. Most of the companies they will try to do data cleaning in as automated way as possible but you have to rely on humans or to do some manual remediation. So many companies what they do is they will hire this workforce in countries where the labor is cheap some companies also use crowdsourcing platform. For example in our case we can easily use a crowdsourcing platform and we can throw these images as a crowdsourcing job and we can ask people that does this image look like Serena Williams and people will easily detect this and they will say no this doesn't look like Serena Williams in that case you can delete the image okay. So companies have this workflows where they use crowdsourcing platform or many different tools and assign these micro task to people especially in the countries where the labor is cheap to do manual remediation or manual cleaning of these images. Now our data is small so we are just going to eyeball and clean these images okay. So what I am going to do is go through each of these folders see just quickly it's okay we are not for a case of this looks like Serena Williams or Venus Williams or sister this image is blur so I'm deleting it so I'm just manually deleting it so now I'm done with my automated way of data cleaning now I'm doing manual data cleaning see this was a background image which it caught. See this is an image of someone else okay so Serena Williams folder is clean. Similarly see I look at Lionel Messi and I see this boy's image maybe this is Lionel Messi's son this another boy so I'm going to delete that. Virat Kohli my favorite player Indian Cricketer and I see this blur image I see this another Virat Kohli see I want to delete that guy and then see Anuska Serena like her wife his wife I see three images see one two three oh I see Dippy Kasper okay delete all those images and once you delete all these images you have a clean data set that you can use it further okay so so that's all I have for this tutorial in the next tutorial we are going to look at Wavelet transform and how we can use that to generate the features and we'll use Wavelet transform as well as these raw images for our model training later on if you're liking this series so far please leave a comment below because your comments kind of helps me in designing the future content in a better way so if you like projects like this if you want me to build these kind of projects please give it a thumbs up or comment below so that I know that there is a demand for doing these kind of projects and I can maybe focus more on these projects in the future thank you",
    "Machine Learning Tutorial Python - 13_  K Means Clustering Algorithm.wav": " Machine learning algorithms are categorized into three main categories, supervised, unsupervised and reinforcement learning. Up till now we have looked into supervised learning where in the given data set you have your class label or a target variable present. In unsupervised learning all you have is set of features you don't know about your target variable or a class label. In this data set we try to identify the underlying structure in that data or we sometimes try to find the clusters in that data and we can make useful predictions out of it. K means is a very popular clustering algorithm and that's what we are going to look into today. As usual the tutorial will be in three parts the first part is theory, then coding and then exercise. Let's say you have a data set like this where x and y axis represent the two different features and you want to identify clusters in this data set. Now when the data set is given to you you don't have any information on target variables so you don't know what you're looking for. All you're trying to do is identify some structure into it and one way of looking into this is these two clusters. By visual examination we can say that this data set has these two clusters and K means helps you identify these clusters. Now K in K means is a free parameter wherein before you start the algorithm you have to tell the algorithm what is the value of K that you're looking for. Here K is equal to two. So let's say you have this data set you start with K is equal to two and the first step is to identify two random points which you consider as the center of those two clusters. We call them centroids as well. So you just put two random points here. If your K was let's say three then you will put three random points. And these could be placed anywhere in this 2D place doesn't matter. Next step is to identify the distance of each of these data points from these centroids. So for example this data point is more near to this centerid hence we'll say it belongs to red cluster whereas this data point is more near to green so we'll say this belongs to green cluster. The simple mathematical way to identify the distance is to draw this kind of line connecting the line between those two centroids and then draw a perpendicular line. Anything on the left hand side is red cluster on right hand side is green cluster. So there you go you already have your two imperfect clunky clusters and now we try to improve these clusters. So you started you already got your two clusters now we'll make them better and better at every stage. And the way you do that is you will try to adjust the centroids for these two clusters. For example for this red cluster which is these four data points you will try to find the center of gravity almost and you'll put the red center there and you do the same thing for green one. So you get this when you make the adjustment and now you repeat the same process again again you recompute the distance of each of these points from these centroids and then if the point is more near to red you put it in a red cluster otherwise you put it in a green cluster. Okay so you repeat the same method and see now these points got changed from green to red so they're more near to red that's why they're in red cluster. When you keep on repeating this process you just recalculate your centroids then recalculate the distance of individual data points from these centroids and adjust the clusters until the point that none of the data points change the cluster. So here right now see there is only one green which is changing it's a cluster so now it's in red but after this we are done even if you try to recompute everything none of these data points will change their position hence we can say that this is final so these are now my final clusters. Now the most important point here is you need to supply K to your algorithm but what is a good number on K because here we have two dimensional space in reality you will have so many features and it is hard to visualize that data on a scatter plot so which K should you start with. Well there is a technique called Albo method okay and we'll look into it but just to look at our data set we started with two clusters but someone might say no these are actually four clusters. Third person might say oh they are actually six clusters so you can see like different people might interpret these things in a different way and your job is to find out the best possible K number okay and that technique is called Albo method and the way that method works is you start with some K okay so let's say we start with K is equal to two and we try to compute some op square error. What it means is for each of the clusters you try to compute the distance of individual data points from the centroid you square it and then you sum it up. So for this cluster we got sum of square error one. Similarly for the second cluster you will get the error number two and you do that for all your cluster and in the end you get the total sum of squared errors. Now we do square just to handle the NIGATE value there is nothing more than that okay. So now we computed SSE for K equal to two you repeat the same process for K equal to three four and so on okay and once you have that number you draw plot like this. Here I have K going from one to eleven and then on the y axis I have sum of square error. You will realize that as you increase number of clusters it will decrease the error. Now it's kind of intuitive to think about it. At some point you can consider all your data points as one cluster individual where your sum of square error becomes almost zero okay. So let's assume we have only eleven data points at eleven value of K the error will become zero okay. So error will keep on reducing and the general guideline is to find out an elbow. So the elbow is on this chart this point is short of like an elbow okay. So here is the good cluster number okay. So for example for whatever the dataset this chart is representing a good K number would be four all right. So that was an elbow technique let's get into Python coding now all right. So the problem we are going to solve today is cluster this particular dataset where you have age and income of different people. Now by clustering these data points into various groups what you're trying to find out is some characteristics of these groups. Maybe the group belongs to a particular region in US where the salaries are higher or the salaries are lower. Or maybe that group belongs to a certain profession where the salaries are higher versus less okay. So you try to identify some characteristics of these groups. So right now we have just name age and income and first thing I'm going to do is import that dataset into pandas data frame. So here you can see that I imported essential libraries and then I have my data frame ready with that and since the dataset is simple enough I will first try to plot it on a scatter plot okay. So when you plot it on a scatter plot of course I don't want to include name I just want to plot the age against the income. So DF dot age DF income in dollar. I'll just use the same convention you can use dot also but since there is a bracket here I will use the same convention. Okay when you plot this on scatter chart you can kind of see three clusters one two and three. So for this particular case choosing K is pretty straightforward. So I will use K means so K means is something we imported here okay and of course you need to specify your K which is an underscore clusters and by the way in Jupyter notebook when you type something and when you hit tab it will auto complete okay. So it creates this K means object for you and it has all this default parameters you can tweak all these parameters later but I'm just trusting on the default parameters. The second step is fit and predict. So in previous supervised learning algorithms we used to do fit and then calculate the score here I'm just directly doing fit and predict. So fit and predict what okay. I'm going to fit and predict the data frame excluding the name column because name column is string and it's not going to be useful in our numeric computation so I want to ignore it. All right so you do fit and predict and what you get back is why predicted. So now what this statement did is it ran K means algorithm on Asian income which is this scatter plot and it computed the cluster as per our criteria where we told our algorithm to identify three clusters somehow okay and it did it it just assigned them different labels so you can see three clusters 0 1 and 2 now visualizing this array is not very very much fun so what we want to do is we want to plot it again on on a scatter plot so that we can see what kind of clustering result did it produced okay so I am in my data frame I am going to append this particular column so that my data frame looks like this. So now this is a little better where I can see these two guys belong to same group these two belong to same group and so on but it is still not as good as scatter plot okay. So let's do this plot dot scatter plot all right now what we need to do is we need to separate these three clusters into three different data frames so let me do that DF1 is equal to DF DF dot cluster cluster is equal to 0 okay so what this is doing is it's returning all the rows from data frame where cluster is 0 and the second one will be this and the third one will be this so now we have three different data frames each belonging to one cluster and I want to plot these three data frames onto one scatter plot okay now just to save some time let me just copy paste the code here okay I will come at this little later but see three different data frames and we are plotting these data frames into different color okay so cluster 0 is green then red and black let's see how that looks okay so DF oh I made a mistake here I had a typo good all right so I see scatter plot here but there is little problem so this red cluster looks okay but there is a problem with these two clusters you know they are not grouped correctly so this problem happened because our scaling is not right our y axis is scale from less of 40,000 to 160,000 and the range of x axis is pretty narrow so it's like hardly 20 versus here is 120,000 so when you don't scale your features properly properly you might get into this problem that's why we need to do some pre-processing and use minmax scalar to scale these two features and then only we can run our algorithm all right so we are going to use minmax scalar so the way you do it is you will say scalar is minmax scalar and this is something if you already noticed we imported here okay all right so scalar is this and scalar dot fit DF so now I want to fit first the income all right so my scalar minmax scalar will try to make the scale 0 to 1 so after I'm done with my scaling I will have a scale of 0 to 1 on y as well as x axis all right so DF let me just copy paste this guy here is equal to scalar dot transform okay so now scalar will scale the income feature all right so DF this okay let's see how that did it so you can see that the income is scale right it's like c0.21 0.38 and so on so it is in a range of 1 to 0 you will not see any value outside 0 to 1 range we want to do the same thing for our age also okay so let's do that scalar dot fit DF dot h DF dot age is equal to scalar dot transform DF dot h and then we print our DF and you can see the age is also scale okay I have this extra column because I made a mistake previously but you can ignore that you can ignore cluster also so we have age and income features properly scale now okay and even if you plot these on to scatter plot they will look structure wise at least they will look like this okay all right so the next step is to use k-means algorithm once again to train our scale dataset this is going to be fun now let's see what scaling can give us and as usual y predicted is equal to km dot fit and predict so again I started with three clusters and I'm using I'm just fitting my scale data age income all right and let's see y predicted so it predicted some values which yet don't know how good they are so I will just do cluster is about to y predicted I will also just drop the column that we type would and then let's look at DF okay in places in place is equal to true okay so now this is my new clustering result let's plot this onto our scatter plot I'm just going to remove this for now now you can see that I have a pretty good cluster see black green and red they look very nicely formed one of the things we started in our theory section was centroids so if you look at km which is your train or k-means model that has a variable called cluster centers and these centers are basically your centroids okay so this is x this is y so this the first centroid of your first cluster second centroid and third centroid and if we can plot this into a scatter plot it can give a nice visualization to us right so pld dot scatter so first let's plot x axis okay so x axis for this will be it will be what okay so using this syntax you can say I want to go through all the rows which is three rows here and then the zero minth first column which is this okay and your y is your first column and just to differentiate them with regular data points I will use some spatial marker and color so you can see that these are the centers of my clusters all right let's look into now alboplot method see this data sort was simple but when you're trying to solve a real life problem you will come across dataset which will have like 20 features it will be hard to plot it on a scatter plot and it will just get messy and you'll be like what do I do now well you use your alboplot method so in alboplot as we saw in theory we go through number of k's okay so let's say we'll go from k equal to 1 to 10 in our case okay and then we try to calculate sse which is sum of square error and then plot them and try to find this alboplot so let's define our k range let's say I want to do 1 to 10 this will be 1 to 10 but whatever okay and then sum of squared error is an array so for k is equal to 1 you'll find sse k equal to 2 you'll find sse you'll store all of that into this array and then use matplotlib to plot the result okay so for k in k range so I'm just going through 1 to 9 and then each iteration I create a new model with clusters equal to k and then I call fit okay and what do I try to fit okay I try to fit my data frame but I use this syntax because my data frame has name column I don't want to use name column all right you'll be like what the heck this guy is doing all the time using this crazy syntax but that's to avoid name if you want you can just create a new data frame just drop name column that is fine too and all right so now what is my sum of square error how do I get that when you call k m dot fit after that on your k means there is a parameter called inertia that will give you the sum of squared error and that error we want to just append it to over all right that we have all right that was pretty fast because our data set is very small okay let's see what is sse so sse you can see that sum of squared error was very high initially then it kept on reducing and now let's plot this guy into nice chart okay when you do that you get our elbow plot remember elbow plot elbow all right where is my elbow where is my elbow okay here is my elbow you can see that k is equal to 3 for my elbow and that's what happened see I have three clusters for accessors we are going to use our iris flower data set from escala and library and what you have to do is use pattern length and width features just drop sample length and width because it's it makes your clustering a little bit difficult so just drop these two features for simplicity use the pattern length and width features and try to form clusters in that data set now that data set has a class label in the target variable but you should just ignore it okay you can use that just to confirm your results and in the end you will draw an elbow plot to find out the optimal value of k all right so just do the exercise post your results into the video comments below also I have provided a link of jupiter notebook used in this tutorial in the video description so look at it when you go towards the end you will find the exercise sections also don't forget to give it a thumbs up if you like the content of this tutorial you can also share it with your friends",
    "Machine Learning Tutorial Python - 8_  Logistic Regression (Binary Classification).wav": " The goal of this tutorial is to solve a simple classification problem using logistic regression. If you have followed my previous tutorial, we have learned a lot about linear regression, especially the home prices example. Linear regression can be used to predict other things such as weather and stock prices. And in all these examples, the prediction value is continuous. There are other type of problems such as predicting whether email is spam or not, whether the customer will buy the life insurance product or person is going to be bought for which party. All these problems, if you think about it, the prediction value is categorical because the thing that you are trying to predict is one of the available categories. In the first two example, it is simple yes or no answer. In the third example, it is one of the available categories. Whereas in case of linear regression, the home prices example, we saw that the predicted value could be any number. It is not one of the defined categories. Hence, this second type of problems is called classification problem. And logistic regression is a technique that is used to solve these classification problems. Now, in the classification examples that we saw, there are two types. So the first example was predicting whether customer will buy insurance or not. Here, the outcome is simple yes or no. This is called binary classification. On the other hand, when you have more than two categories, that example is called multi-class classification. Let's say you are working as a data scientist in a life insurance company. And your boss gives you a task of predicting how likely a potential customer is to buy your insurance product. And what you are seeing here is the available data. And based on the age, the information you have is whether customer bought the insurance or not. Now, here, you can see some patterns such as young people don't buy the insurance too much. You can see like there are persons with 22, 25, these kind of ages where zero means they didn't buy the insurance. Whereas as the persons age increases, he's more likely to buy the insurance. So you know the relationship and you want to build a machine learning model that can do a prediction based on the age of a potential customer. So as a data scientist, now this is the job you're having given. Now, the first thing you would do when you have this data is you will plot a scatter plot which looks like this. When you have worked on linear regression already, the first temptation you have in your mind is you start using linear regression. So when you draw a linear equation line using the linear regression, it will look something like this. Now, how did we come up with this line? For that, you can follow my previous linear regression tutorials. If you think about it, what I can do here is I can predict the value using linear equation line and say that if my predictor value is more than 0.5. So here, this is 0.5. If it is more than 0.5, then I will say, okay, customer is likely to buy the insurance. If it is less than that, then he's not going to buy the insurance. So anything on the right hand side is, yes, anything on the left hand side is no. Now, of course, we have these outliers, but we don't care about them too much because for 90% of the cases, our linear regression will work okay. Now, imagine you have a data point which is far on the right hand side here. So say a customer whose age is more than 80 years. Let's say he bought your insurance, okay. Then your scatter plot will look like this and your linear equation might look like this. In this case, what will happen is when I draw a separation between the two sections using 0.5 value, here the problem arises with these data points. Actually, the answer was yes here, but my equation predicted them to be no. So you can see that this is pretty bad when you use linear regression for data class like this. Now, here is the most interesting part. Imagine you can draw a line like this. This is much better fit compared to the previous linear equation that we had, okay. And here, when you draw a separation between using 0.5 value, you can clearly say that this model works much better than the previous one. The question arises, what is this line exactly and how do you come up with this, right? If you have learned statistics, you might have heard about sigmoid or legit function. And that's what this is, okay. Now, the moment you hear this term sigmoid, you might pause this video and start googling about sigmoid. And it is fine. You can read all the articles about sigmoid or legit function to get your understanding correct on mathematics behind it. But if you don't want to do it, I will give you a basic idea. The sigmoid functions equation is 1 divided by 1 plus e raise to minus z, where e is some mathematical constant. It's called Euler's numbers. The value is this. Now, think about this equation for a moment. What we are doing here is we are dividing by 1 by a number which is slightly greater than 1. And when you have this situation, the outcome will be less than 1, correct? So, all you are doing with this sigmoid function is coming up with a range which is between 0 to 1. So, if you feed set of numbers to this sigmoid functions, all it will do is convert them to 0 to 1 range. And the equation that you will get looks like s shape, right? So, if you plot a 2D chart, it will look like s shape function that we saw in the previous slide. Essentially, what we are doing with logistic regression is we have line like this, which is linear equation. And you know the equation for our linear line which is mx plus b. All you are doing is you are feeding this line into a sigmoid function. And when you do that, you convert this line into this s shape. So, here you can see that my zi replaced with mx plus b. So, I applied sigmoid function on top of my linear equation. And that's how I got my s shape line here, all right? Now, all of this math is just for your understanding. As a next step, we are going to write logistic regression using sklearn library. And these details are abstracted for you. So, don't worry about it. You don't have to implement all of this mathematics. You will just make one simple call and it will work magically for you, all right? So, let's get straight into writing the code. Here is the csv file containing the insurance data. You can see there are two columns age and whether that person bought the insurance or not. And we are going to import this into our pandas data frames. So, I have loaded my jp.nodbook by running jp.nodbook command on my command line. I imported couple of important libraries and then I imported the same csv file into my data frame, which looks like this. And now I'm going to plot a scatter plot just to see the data distribution. And you can see that I get a plot like this. Here these are the customers who didn't buy the insurance. These are the ones who bought the insurance. And you can see that if the person is younger, he is less likely to buy the insurance. And as the person gets older, he is more likely to buy the insurance. The first thing now we are going to do is use trained test split method to split our data set. So, if you look at our data, we have 27 rows. So, we are going to split these rows into training set and test set. Again, I have a separate tutorial for how to do train and test split. So, you can watch that. It is basically from a scale on model selection. You import train test split method. Here my x is dfh. Now, I am doing two brackets because the first parameter is x, which has to be multi-dimensional array. So, I am just trying to derive a data frame here. And bot insurance is y. And I will say what is my test size. If you want to see the argument, you can do shift tab. And it will show you a help for this function. So, I use this a lot. It is pretty useful. So, let's see. So, there is this test underscore size parameter. So, let's use test underscore size. We are going to do, let's let's say train size. So, training size is 0.9. So, 90% of the example we are using for training. And 10% we will use for actually testing our model. Now, what do you get back as a result? So, these are the things you get back as a result. I am just going to copy from here. And that's it. Hit control enter to run it. Okay. So, here is some warning. Maybe they are asking us to use test size. It doesn't matter. Okay. Let's look at our test. So, our test is 1823 and 40. So, these are the three values. We are going to perform our test on when you look at our x train. These are these are the data samples. We will use to train our model. So, let's now import logistic regression. So, from same linear model, you can import logistic regression logistic regression. SK learn. All right. So, we will have logistic regression class imported. And we are going to create an object of this class. We will call it a model. And that model now will do a training. Remember, in sk learn, whenever you are using this method fit, you are actually doing your training for your model. So, x train and y train. This is what you use for your training. When you execute this, this means your model is trained now. And it is ready to make predictions. So, for these three values, we are making a prediction. So, I will do model dot predict and x test. So, here what it is saying is 001, which means first two samples. It is saying these two customers are not going to buy insurance. And you can see that it's kind of working because they have age of 18 and 23 year old. And we saw that as the age is the younger age people do not buy the insurance. Whereas, I think anything more than 27, 28 they buy. So, here the age is 40. So, the answer was 1. If you want to look at the score, score is nothing but it is showing the accuracy of your model. So, what you are doing is you are giving x test and y test and here the score is 1, which means our model is perfect. Now, this is happening because of our data size is smaller. We have only 27 samples. But if you have more wider samples, then it will make mistakes in at least few samples. So, your score will be less than 1. Because of the small size of our data set, the score is pretty high here. Another method to try is you can see that when you hit by the way tab, it will show you all possible functions that start with Prattie. So, here you can also predict a probability. So, when you predict a probability of x test, it will show you a probability of your data sample being in one class versus the other. The first class here is if customer will not buy the insurance. So, for the age 18 and 23, you can see there is 0.6% probability that they will not buy the insurance. Whereas for the person with age 40, it is reverse. There is 0.6% probability that he will buy the insurance and 0.39% probability that he will not 0.39% really, it is really 39% that he will not buy the insurance. If you want to do one off, then you can just do model.Pradict. T6, he will buy the insurance. That's why you had one. And if you had something like 25, he will not buy the insurance. That's why you get 0. So, this model that we built is working like pretty well with logistic regression. That's all I had and now is the time for exercise. So, if you know about KGLE website, this is the website that hosts different coding competitions. And it has one of the more important features which is the datasets. So, if you go to this dataset section, you can download various datasets based on the type, based on the file type, or you can even search for datasets. So, if you want to do some Titanic, Titanic data analysis, you can search for that. Basically, you can just explore these datasets for exercises. From this, I have downloaded this HR analytics dataset, where there is an analysis on the employee retention rate or employee attrition rate. If I open that CSV file here, it looks like this, where based on the satisfaction level, based on number of projects, or average monthly hours that person has worked on, you are trying to establish correlation between those factors and whether person would leave the form or whether he would continue with the form. These kind of analytics are very important for HR department because they want to retain their employees. And if you can build a machine learning model for HR department, then they can focus on specific areas so that employees don't leave at the firm. So, that's what you are going to do. You are a data scientist. You are going to work for your HR department and give them a couple of things. So, I have mentioned all of those things in the Jupyter notebook, which I have available in the video description below. So, if you open that notebook, you will see all the code that we just went through in this tutorial. And at the end, you will find this exercise section. So, there is a link here. You download the dataset. If you don't want to download it, the same level as this notebook, there is an exercise folder. So, download the CSV from that. And you are going to give answer on these five questions. First one is out of all these parameters that we have, you want to find out which factors affect the employee retention by doing some exploratory data analysis. You will also plot bar charts showing the impact of employees' salaries on retention. Also plot the bar charts showing the impact of department and employee retention. And then using the factors that you figured in step one, you will build a logistic regression model. And using the model, you are going to do some prediction. In the end, you will measure the accuracy of the model. Let's do their exercise in the comments below. Let me know your answers. And if you want to verify the answers, then I have a separate notebook at the same level in exercise folder, which has all the answers. But don't look at the answers directly. A good student is someone who tries to find the solution on his own and then he looks at the answer. All right. That's all we had. Thank you very much for watching. I'll see you next time.",
    "Machine Learning Tutorial Python - 20_ Bias vs Variance In Machine Learning (1).wav": " I'm going to explain bias and variance using a very simple language and you are already wondering what's the meaning behind showing all these three pictures. Well, look at the pictures carefully and try to find out what I want to show you here. The first picture of course shows a t-shirt that I'm wearing which is too much fit basically it's over fit. If my body increases or decreases you know if I start eating junk it will not fit. The second one is under fit which is too loose basically you look at my sad face here clearly it indicates what's the issue here. The third t-shirt however is a perfect fit where even I grow in size a little bit or or if I you know thin up a little bit it's going to be okay. Some concepts apply in machine learning world where your model can be an over fit under fit or balance fit and we're going to look at these scenarios using a simple housing price prediction example where we are using one feature which is the square foot area of a home and based on this feature we are trying to build a machine learning model that can predict the home price. Here I have a scatter plot of all my training data set basically my data set super for the supervised learning and in this scatter plot when we train our model what we do is we split this data set into training and test samples. Let's say all these blue dots are my training samples the orange dots are my test samples and we can train a model that fits to these blue dots. Let's say our model is such that our training methodology is such that we end up in an over fit model and over fit model tries to fit exactly to the training samples where your training error becomes close to zero but the problem here is the test error can become high because once you have this model train which is the screen line now let's say you want to figure out an error for this particular orange test data point the predicted value will be somewhere here and the error will be this gray dotted line and you can measure the error for all your test data set and average it out let's say you get this error as 100 I'm just making this up let's say this error is 100. Now remember one thing when you have a data set and when you pick your training samples you pick it up at random you have trained test plate let's say 80 to 20 person you pick your training samples at random there's a different data scientist they might be choosing different set of training sample let's say this and their model might look different here they're using the same model same methodology so your training data set error is still zero because you both are trying to over over fit the model but the problem that happened here is your test data set error is let's say 27 you can compare these two images and you can easily see that the test data set error here is much higher than the second scenario you see all this you just sum up all this gray dotted lines here and some of the gray dotted lines here this error is much less than the previous error so if you compare these two scenarios side by side in one case your test error is 100 second case test error is 27 which means the test error varies greatly based on your selection of training data point and this is called high variance because there is a high variability in the test error based on what kind of training samples you are selecting now you're selecting training samples at random so your test error varies randomly which is not good and this is the common issue with over fit model they tend to have high variance so high variance again just to summarize is high variance happens when your test error varies by great deal based on your selection of training data points so just to remember when you're talking about variance high variance you're always talking about test error let's look at another scenario where I've same data set I have split into train and test samples and this time instead of having a complex model that overfits my training samples I'm trying to come up with a very simple model there's a linear equation which is under fitting your training sample because linear equation is a straight equation it cannot truly capture the pattern in your training samples okay the straight line cannot pass through all the blue dots so it's a simple model where your training error is actually high in previous case the complex model was complex your training error was zero here it is lesser 43 and your test data set error is around 47 again 4347 these numbers I've just made up to explain you this concept when you selected different set of training data points see in both the cases the line will be different but your training and test data set error is still kind of similar you know in the first case it was 47 second case it was 37 so this is called low variance because based on what data point you select as your training samples your test error doesn't vary that much 4737 that's still similar it's not like 121 you know so your test error doesn't vary too much hence low variance but if you look at your train error in the previous case your train error was zero here you have some big train error and that is called high bias high bias is a measurement of how accurately a model can capture a pattern in a training data set so when you're thinking about bias you're always thinking about train error when you're thinking about variance you're always thinking about test error bias is basically your model's ability to capture the pattern in your training data set in an accurate way in the previous case where we had overfit the model the bias was low because a train error was close to zero so if the higher the train error higher the bias okay now let's look at the idle scenario where you come up with a model that kind of accurately describes the pattern that is available in your training data set here your training error and test error both are low even if you select different set of training data points see the training data points here are different but still your model selection is such that your train and test error both are kind of low and in this case it's called low variance low bias model because your test error doesn't vary too much based on what training samples you have selected also your train error in general is low so that's why it's called low bias so we already cover this concepts like whenever you have an overfit model it's likely that you will get high variance when you have underfit model it is likely that you will get high bias and when your balance fit you will get low variance low bias and as a data scientist you want to come up with a with a model that has a balance fit you know just like our t-shirt you want to go to a store and buy a t-shirt you will always buy a balance fit t-shirt now there is a bull's eye diagram where the central circle represents the truth this is your truth samples okay and when your predicted values so these wide diamonds are let's say your predicted values when they're near to the inner circle which is your truth it is called low bias bias is all about how close you are to the truth so if you are closer to the middle circle you have low bias if you are far away from the middle circle you have high bias but in both the cases these diamonds they are clustered together you know they are like a close family they're together so whenever that together it is called low variance in high variance they're scattered apart you see they're scattered apart but they're still near to the truth which is the inner circle hence there is a low bias. Voskey scenario is high bias high variance where you are far away from the truth which is this middle circle that means you have high bias and also these individual predicted values they are separated out hence you have a high variance if you remember this image I think remembering bias versus variance trade-off is going to be easier for you how do you get a balance fit model so there are several techniques and the first one is cross validation I have had a video on this before so if you watch it you'll get an idea another one is regularization L1 and L2 dimensional to reduction ensemble techniques bagging boosting I'm going to be making videos on this in next 10 days maybe so those videos should be available so these are different techniques to get a balanced model I hope you like this video if you did give it a thumbs up and share it with your friends thank you if you have any question post in our comment box below",
    "Machine Learning Tutorial Python - 2_ Linear Regression Single Variable.wav": " Today we are going to write Python code to predict home prices using a machine learning technique called simple linear regression. In this table, I have the prices of home based on the area of that home in my neighborhood in monotone ship, New Jersey. Using this data, I will build a machine learning model that can tell me the prices of the homes whose area is 3300 square feet and 5000 square feet. You can plot available prices and areas in form of a scatter plot like this, where this red marker shows the available data points. Now we can draw this blue line which best fits these data points. Once I have this line, I can tell the price of any home. Basically, I can say, okay, 3300 square feet home is going to be this price. Once I have the linear equation. Now you might ask how did you come up with this blue line because this line is not going through all these data points and there are number of ways you can draw different lines like for example this red and orange. Okay, so why did I choose this blue line? What we do is we calculate this delta which is an error between the actual data point and the data point which is predicted by your linear equation. We square individual errors and we sum them up and we try to minimize those. So we do this procedure for all these lines. So for orange, red and blue line, I repeated that procedure and what I found was this blue line was giving me the minimum error. Hence I chose that line. Now if you remember from your algebra class during your school days, then you have probably learned the linear equations which looks like Y is equal to MX plus B where M is a slope or gradient and B is an intercept. In our case, the slope is M here but the Y is price and the X variable is area. Area is called an independent variable whereas price is called a dependent variable because we are calculating price based on this area. Now we are going to write Python code for doing home price predictions. Here I have launched my Jupyter notebook and I have imported some useful libraries. The most important library here is from Escalon import linear model. So Escalon is the library. It is also called scikit-learn. So if you google it, you will find that this is the library we are using and it comes with anaconda installation. So once you have installed anaconda, you should have this library available for import. I have the prices available in form of this CSV file. So the first thing I'm going to do is load this prices in Pandas DataFrame. PD.readCSV. Name of the file is homeprices.csv. And I have the DataFrame. Now if you don't know about Pandas and DataFrame, I recommend you watching my tutorials on Pandas because Pandas is going to be extremely useful in your machine learning journey. Once I have a DataFrame, next thing that I'm going to do is plot a scatterplord just to get an idea on the distribution of my data point. And you all know if you have used JupyterNoughtbook before, is that you have to use a matte plot lib in line magic in order to draw the plots. And first I'm going to plot dot scatter. So I'm going to plot area versus price. Now I'll make some modification and I'll set color to be red, marker to be plus. I'm just making my chart a little fancy. And then also setting x and y label because you can see that x and y labels are not available right now. So the x label would be area in square fit. And y label is the price in US dollar. So this is square fit area and US dollar price. All right. So once I look at this plot, I get an idea that the distribution is suitable for linear regression model. And hence I will now go ahead and use the linear regression. Okay. So first you need to create a linear regression object. So you can see that from SK learn Python module, I have already imported linear model. And here linear regression. So I will create an object for linear regression. And then I will fit my data. So fitting the data means you are training the linear regression model using the available data points. Okay. Now the first argument has to be like a 2D array. So you can supply your data frame here. So I'm going to supply a data frame which only contains area. Okay. And the second argument would be your y axis on your plot, which is your price. So when I execute this, it worked okay without any error, which means now this linear regression model is ready to predict the prices. So let's do our prediction. So what we wanted to predict was the price of a home whose area is 3300 square fit. And you can see that it predicted this price. Now you might be wondering why it came up with this price. So let's look at some of the internal details. So when I train my linear regression object using this available data, what happened is it calculated the coefficient and intercept. So if you go back to our mathematical equation, you know that for any linear equation, there is a slope and intercept. This is also called coefficient. So my model calculated the value for m and b. So let's see what are the values. Okay. So when you do reg.cof, it will show the value of coefficient, which is this m. And when you do intercept, this is showing you the value for this b. Okay. So now the equation for price is m into area plus b. Okay. So now we have m area and b. So let's see what value it gives. So y is equal to m into x plus b. Okay. And here m is this number. So let me just copy it. into x. x is the area that you want to predict. So area you want to predict is 3300. And your intercept is this. Okay. So when you execute this, you get this value. So now you know how it was able to predict this value right here. Okay. Similarly, if you change this to the second value that we wanted to predict was the 5000 square feet. So for 5000, you can just do this and you get this value right here. So this is pretty amazing. Now you have a model which you can use to predict your home prices. You might have a CSV file like this where you have list of areas available and you want to predict the prices for these homes. Right. Until now what we did is we were individually giving the the area and we are predicting the prices. So what I want to do now is generate another CSV file where I have these list of areas and their corresponding prediction on prices. So for this, I will create a data frame first using a read CSV. So here first I created a data frame. I have list of areas available. Now I will use regression model to actually predict the prices. So I will just supply D here and that will return me the prices. So you can store the prices in variable P and then what you can do is in your original data frame, you can create a new column. So when you do like this, it will create a new column in your data frame and you can assign P here. And now when you print your data frame, you can see the prices are available and then you can just use two CSV method to export the value to prediction.csc. So if I open my prediction.csc, you will now find that I have area and prices. It imported, it exported actually the index as well. And if you don't want to do that, then index is equal to false. If you do that and execute this again, and now if you can prediction.csc, you won't find the index. It will be just area and prices as you can see here. Okay. So once you have this model built in, you can apply this model on a huge CSV file and come up with a list of predictions. Now going back to our original example, so let me go back again here and read my original prices and do a fitting on that. What I want to show you is how does my linear equation line look. Okay. And for that, I'm again plotting a scatter plot and along with the scatter plot, so this much line of code will just plot the scatter plot. And I have added one more line where I'm using a data frames area. And I am predicting the prices and plotting them on the Y chart. So let's see what happened here. So df.air is not defined. So that has to be df.air here. So let's go step by step. So here I have my scatter plot and what I'm doing is PLT.plot on my x axis. I want df.air and on my y axis, I want to predict area like this. Okay. So it shows the visual representation of my linear equation here. All right. So that's all I had for this tutorial. I have an exercise for you guys. What you want to do is given Canada's adjusted net national income per capita, you want to predict the net income in year 2020. I have provided a CSV file in the exercise folder. So if you go to GitHub and download my notebook. And I have by the way, the link of Jupyter notebook available in the video description below. So download the notebook, study it first, and then download the exercise folder. In the exercise folder, you will find this CSV file, which has Canada's per capita income for the year 1970 to 2016. And your job is to find out the predicted income in year 2020. I highly recommend that you do the exercise because just by watching the video, you are not probably going to learn that much. I mean, you'll learn something but it's not very effective. So it's better that you do some practice as well as we go through these tutorials. And I'll make sure I provide simple exercises at the end of every tutorial. Okay. So again, just to summarize, this tutorial was all about building a simple linear irrigation model using one variable. And in the future, we are going to cover a little more complex linear regression models. Thank you. Bye.",
    "Data Science & Machine Learning Project - Part 8 Deployment & Exercise _ Image Classification.wav": " Once you have built the model and build the website, the next step would be to deploy it to production. Now you can deploy it either to Amazon EC2, Google Cloud or HeroCoo. There are, even there is Walter. So there are a couple of options that you have in terms of deploying your model to production or to a cloud server. I'm not going to make a separate video on how to deploy this model or the whole project to cloud because this will be very, very similar to the deployment video that I did for Bangalore or home price predictions. So I'm going to link that video here. You can follow exactly same instruction and deploy this whole project to production. It will be, it will be exactly same. There is like no, not much difference. So follow that video. I have also talked about NGIX web server and many things like that. You can build a full-fledged production website by following those guidelines. Now if you like my work, then I want something in return. This will be my fees because you are learning a lot of things on YouTube through my channel for free. And if you find that the work that I'm doing is helping you, then I want some fees back. And my fees is this. It is an exercise for you. So the first exercise is you should deploy this project to cloud Amazon EC2 by following my other video and see how it goes, make it work in production. The second exercise is that I want you to modify this project such that instead of doing sports, celebrity, classification, you do classification for your five role models. Now these role models could be your parents, your teacher, your spiritual leader or an entrepreneur that has influenced you. So pick those five personalities that you like the most and make the same project for those five people. And upload that project to GitHub and send me a link of your GitHub project. You can post in a video comment below. Share your work with other people. So that will be the fees that you'll be giving back to me. I also want you to share these projects with your friends and with other people because I'm doing a lot of hard work. Trust me, making content on YouTube is so much hard work. I have nine to six job family, a lot of priorities. And I spent so much time on weekends actually, my personal time. So if you feel that it has given any benefit to you, I want you to spread word about my channel, my projects to as many people as possible. Now how can you spread the word? You can post it in a LinkedIn. You can say, okay, this is an image classification or a regression project which I found useful. You might also find useful. You can post it in Facebook groups as a Facebook post. So you can do a lot of things. In India, there was this tradition of giving Guru Daksina. Guru Daksina means when you learn something from a teacher, because in ancient time, teacher will not charge any fees actually. They will give the education for free. And in the end, they might ask for something in return. And it will be like a foreign novel course. So I'm of course not asking you for any money. You might join some online course. They might charge you so much fees. And I'm not, you don't have to pay any fees to me. All I want is, I want the word to be spread out for the work that I'm doing. Because all the hard work that I'm doing, if I see that more people are benefiting from it, it will give me motivation. It will give me satisfaction to continue this work. So if you are in agreement with these points, please share it with as many people as possible through Facebook, through LinkedIn and so on. And also give it, give this video a thumbs up if you liked it. Also, if you have any question, try to post in a video comment below. And if other people have question, try to help them. So it shouldn't be always about taking, sometimes you need to give back as well. So with the spirit of give back, follow these guidelines. I wish you all the best. If you like these kind of projects, again post so that I know that I have to build more of such projects. I'm planning to do deep learning series, NLP series and many more projects that can benefit you. So I have all those plans. My schedule is tight. Sometimes I don't have that much free time. And for that reason, I'm not able to upload as much content as I would like to. But hopefully it will change in future. So thank you very much and happy learning.",
    "Data Science & Machine Learning Project - Part 1 Introduction _ Image Classification.wav": " With this video we are beginning an end-to-end data science or machine learning project on image classification. Our end goal is to build a website where you can drag and drop an image of a sports person and it will tell you which sports person that is. Now we are restricting our image classification to only five classes. I have selected my most favorite five sports celebrities. So we will restrict to only those five celebrities. And by doing this project not only you will learn a lot of important concepts in data science and machine learning. But you will get a feel of how projects are executed in big companies in a typical corporate environment. We will use images which are scrapped from Google. I will show you a couple of different ways of scapping images from Google. And then we will do some feature engineering. We will use OpenCV which is a famous image processing library. So it is going to be cool. And we will cover a lot of important concepts such as wavelet transform. How you can use these images to train SVM classifier or a logistic regression classifier. Now typically nowadays people use neural nets because they are the best when it comes to image classification. But I want to demonstrate that how by using simple algorithm such as SVM you can still get a pretty good accuracy. So during this tutorial series I am not going to cover any deep learning. But in future I am thinking of adding a couple of more neural net based tutorials in the same project series. So you will get a feel of how the model performance looks when you use SVM or logistic regression. And how it varies when you try to use deep learning here. In terms of project architecture we will build a model first, hyper-tunit, export it to a file. Then we will run a Python Flas server around it and our website will make a call to that Python Flas server. Now this project is a classification project. We previously did a regression project on Bangalore property price prediction. The structure of this project is going to be similar to that. It will be 8 to 10 part to the series. But it will be a lot of fun for you because in the last project you did regression. Now you are doing classification. And if you want to apply for a job or if you are in general trying to learn data science. It is important that you do at least one regression project and one classification project. Now let's see what happens if you are working for a big company and this kind of project is given to you. Usually business analysts will come up with the requirements. Let's say they come up with this requirement that I want to build this website where you drag into up an image and it tells you who that person is. Once you get these requirements there are a couple of different roles in a big corporate company who plays an important role here. There is a data engineer. Data engineer is a person who looks at all the database architecture and he knows about different data sources that exist within a company. There could be a data analyst who will help data scientist with the data analysis process as well as data analyst can help with data collection. Usually data analyst and business analyst and the engineering team they work in a close coordination to collect the data. Sometimes in some companies data scientists also play an important role in data collection but that's a little minor work for them. Mostly they are focused on building the model. Once you have the collection of data the data scientist will come into picture and he will try to build a model. Once the model is built let's say it's fine tune using grid source, CV or whatever optimization technique that you are using. They will hand over this model to an engineering team. The engineering team will then write a Python Flask server, they will write a website and all of that. They will create this complete software application or software infrastructure around the train model and that will give you an end product and working product that can be used by your clients. That's how you really begin with this project and now in the second part we will talk about the whole data collection aspect of this project.",
    "Machine Learning Tutorial Python - 18_ K nearest neighbors classification with python code.wav": " In this machine learning tutorial, we'll look into what is K nearest neighbor classification will go over some theory, then we'll write Python code, we'll build a model and then you will have an exercise to work on. This you are doing a classification for iris flowers. Here I have a picture of a vorsical flower, which is one of the three types and based on a sepal width and height, you can actually figure out which of the three flowers category it is in. So we are classifying an iris flower into one of the three classes, Satoshi Varsigolor, Vajinika based on sepal width and height. You can plot sepal width and height in this kind of 2D scatter plot and you can plot all your data points, all the data points for which you know what is the class. Now let's say you have build a model and you have a new data point. So yellow data point you don't know what class this is and using k and you want to figure out. By looking at the graph itself, you immediately get an intuition that this has to be blue color which is vajinika because it is more near to this cluster. And k and n works just like that. In k and n, you first figure out what is the value of my k and you can figure out the value of k by trial and error. There is just no like specific rule to it. Usually people use 5 but you know you can change it. So let's say here I use k is equal to 3. And this means I need to figure out the most nearby 3 data points. So the most nearby 3 data point using Euclidean distance which is just a simple distance between these 2 points are these 3. And since these 3 are vajinika, my yellow data point is also vajinika. K nearest algorithm, neighbor near k neighbor nearest neighbor algorithm is super simple. You figure out the most nearby k data point and whichever data point category that data point belong, my due data point belongs to that category. Let's say my k is 10. Again, you figure out the most nearby 10 data points. So here I have 7 vajinika to vajicular. You take the highest number which is 7 which is vajinika. How about if k is 20? Well, then you have a problem because your total number of data point in vajinika class are very less. So now it is misclassifying this yellow data point saying that this is vajicular because the rule is you figure out the most nearby k data point and you take the maximum count. So here k is 20. So maximum data points which are near are vajicular which is 11 data points 11, 7, 2, 11 maximum number. But this will be wrong because clearly by looking at the graph, you know that this yellow data point is actually vajinika. So you have to carefully choose a value of k such that it is not very high or not very low. Again, style and error depends on your situation depends on your training data set and so on. That's right Python code now. One thing I would like to clarify is that here we had only two features, apple, width and apple height but k and works equally if there are more than two features. You know there could be a number of features and k and will just work fine. I mean as a human we can't visualize it but mathematically it should just work okay. We are going to use same iris data, flower data set for our coding. If you go to youtube and search for support vector machine Python, I have this SVM video in which I used iris flower data set. So I am going to use some of this same code here. So if you go here and look at the code and if you have followed this video, you would already be familiar with this code. So I will use the same code till the point that I create a classifier. So same code is here. Basically you are loading iris data set from scalon data sets. Then obviously the feature names are apple, width and length, petal, width and let. Seppel and petal are two different flower leaves like the components of the flower and based on these four features. We determine whether it is Satosa or Varsikolor or Vajinika. So over target classes all three. I have loaded iris dot data in a data frame and then I am adding a new column called target in the same data frame. So you see now my data frame has five columns, four are my features. Number 5 is my target. Basically zero means Satosa one means Varsikolor, two means Vajinika. This code my friends is very, very easy. If you have followed my SVM video, you would have no time understanding this. You need to have some knowledge of Pandas as well. Now if I do df target equal to one, it will show me all the Varsikolor flowers and you see this start from the index 50. If I do two, which is all Varsikolor flowers, they start from 100. So basically I have 150 training samples out of which first 50 are Satosa, 50 to 100 is Varsikolor, 100 to 150 is Varsikolor. And I am going to split this main data frame into three data frame. So when I do nothing column 50, df zero will have all Satosa flower, df one will have all Varsikolor flower and df two will have all Vajinika flowers. And I am using matplot to do some plotting. I have a scatter plot of separate width and length between df zero, which is Satosa, df one which is Varsikolor. So these are the two kind of clusters that we have. And looking at all these clusters, we get immediate intuition that we can apply KNN classifier here. And I am doing train test split. Usually thing if you have followed my videos, you would know what this is. So we are dividing our samples into training and test data set and training set now has 120 samples. And test has 30 data sample. All right, now comes the most interesting part, which is creating a K nearest neighbor classifier. Google is your friend. You probably all know that in that specify KLAN KNN classifier, you will get scikit-learn documentation. This is how you create KNN classifier in scikit-learn. And usually they give a nice little example at the bottom. So I am just going to use my most important skill, which is control C control V and create a KNN classifier. So I will say KNN classifier. Okay. What are the parameters in the classifier? This one is neighbors. This is basically the value of K. So I am just going with, you know, three. And there are other values. You can read the documentation. I am going to keep everything else default. This is a beginner tutorial. But metric, mincoversky. What is that? Let's read. So metric. So mincoversky means it's basically your if p's, two and metric is mincoversky, it is using Euclidean distance. Okay. So where is here? Okay. So see read this. It is going to use your standard Euclidean distance for computing the nearby neighbors. You can use some other distance as well. And this parameter can control that. I am just going to use whatever is default. And simply call KNN.fit method. So I created KNN classifier, call KNN fit on X train and Y train. And my classifier is trained already. And first thing I do usually after training my classifier is compute the score. So when you compute the score, you are giving X test. So using the more train model, it will find out the prediction. And then it will compare that with Y test. And it will tell you how accurate is a model. My model is so much accuracy. It got all prediction right. You can maybe change this value. I mean, three is ideal, but I'm just changing the value just for fun. And see with different value of K, you get different scores. So whenever you are using KNN, try to figure out best value of K for your use case. You can use grid search, CV or K fold cross validation to figure out the optimal value of end neighbors. For us, the optimal value is three, but I am going to keep it 10 so that, you know, we can see some variation in our confusion matrix. So in the previous videos, we have already plotted confusion metric. So confusion matrix tells you for which classes it got the prediction right for which classes, it did not get the prediction right. So to plot it, you will import confusion matrix from S-K-LON matrix library. And then what you do is you use KNN for making prediction. So you give x-tas, you find y predicted and then in confusion matrix. There are two parameters. First is truth and second one is prediction. Okay, what is my prediction? Prediction is this. What is my truth? Truth is y-taste. And there you get the confusion matrix. Now if you don't know how to read it properly. Basically, this is your confusion matrix, but for better visualization, we can use a C1 library. We have used it in previous videos. I'm not going to go over this code in detail, but it's the same exact grid you are just having a little better visualization. Anything on the diagonal are your correct prediction. This means this zero is satoza, this zero is satoza, and this is truth, this is prediction. What this is saying is 11 times, 11 times, it was satoza and my model predicted it to be satoza. So this diagonal is all correct prediction. Anything that is not on this diagonal is wrong prediction. So one time what happened was truth was, was the color and it said it is virginica. 1 is how many instances? So we totally did. How many predictions did we do in total? 30. So 12 and 13, 13 and 6, 19, 19 and 11, 30. Because we have total 30 samples in our taste data set. And out of those 30, only one time it got the prediction wrong remaining 29 times it got it right and hence the score is so high. You can also use confusion matrix. So not confusion matrix, the classification report. So from a scalar matrix, you can import classification report and this classification report takes your white taste and why predicted. And you know, okay, the formatting is not great. So if you put for instant, men is going to show you a little better result. And this is showing you precision recall, reference score for each of the classes, this one class, two class and so on. So you have to get a fundamentals clear on precision recall. And if you go to YouTube search for code basics, precision recall, you'll find my video. If you watch it, you will get an idea on all these parameters. So that's all pretty much that I had for this video. KNN again is a very simple classifier. Very, very easy to understand. If I explain it to high school students, they will also understand it easily. The code you saw was so simple three lines. It was like fricking simple classifier. So after you want this video, you need to work on an exercise. So if you go to my GitHub page, I'm going to provide all these links in the video description below. exercise.md has this exercise and this exercise France is the most important part of my tutorials because learning coding or machine learning is like learning swimming until you practice, you're not going to get much benefit. So practice is extremely important. I want you to load digits data set from SK learn and use KNN classifier to classify digits from 0 to 9. You also need to plot confusion matrix classification report like how we did it in the current video. So for our main classifier, you need to find the optimal value of k that gives you the maximum score either by trial and error manually trying it out or by using grid search CV. A good student would try on their own and then only they will click on solution link. A bad student would directly click on solution link and try to look at the solution but the downside is that they will get a virus which I have embedded and crypted into the skid hub and your computer will start sneezing and your computer will have a fever. So if you don't want your computer to have a fever, please try the solution on your own and then only click on solution link. Alright, thanks very much for watching. If you have any question, post in a comment box below.",
    "Machine Learning Tutorial Python - 19_ Principal Component Analysis (PCA) with Python Code.wav": " Principle component analysis is a technique used in machine learning to reduce dimension. In this video we are going to look at what it is, we'll write some Python code and in the end there will be an interesting exercise for you to work on. Let's say you're working on a building a machine learning model to predict property prizes. Here all the columns on the left hand side are the features or the attributes of the property that decides the price. So the column in green is actually your target variable. You probably already know that the price of the home is mainly dependent on area which town it is in plot, etc. It depends on how many bathrooms you have in a property, but not as much. For example, 2600 square foot home with two bathrooms versus three bathrooms. Companies won't be that different, but if you go from 2600 to 3000 square feet, price will be significantly different. So clearly area, plot plays a more important role in determining the last price, bathroom plays, a little bit of a role and how about this particular column trees nearby. Whether you have two trees nearby your home or three trees nearby your home, you don't care that much. So that column probably doesn't impact the last price at all or it impacts it only little bit. When you're solving real life machine learning problems, you will have many columns, 100,000, 2000 columns or features and you need to do something to identify the features which are very important. You're working on let's say handwritten digit classification where you know you have digits which are written by hand and then you try to classify as one of the numbers from 0 to 9. Here this image is represented as pixel. Let's say this is a grid of 8 by 8 and every number presents the color. So 0 means black, 16 means highest white, you know. And since it's an 8 by 8 grid, there are 64 pixels or numbers that helps you determine what number it is. So these 64 pixels are called features and if you represent. Now if you think about some of the pixels in these images, you will find that those pixels don't play any role at all in figuring what digit is it. For example, these two pixels here are pretty much any pixel in this last column. No matter what number it is, these pixels are always black. So we can say that these pixels are not important features. And what if we get rid of these features? We get two benefits out of that. First, the training is faster. You know machine learning training takes a lot of time, a lot of computer resources. So you want to save the training, you want to make it a little lightweight and your inference can also be faster. And data visualization becomes easier. Let's say you have 100 features and somehow you reduce those 100 features into only two features or three features, then as a human, you can plot it on a 2D or 3D graph, you can visualize it. And data visualization helps a great data visualization helps you a lot in terms of your final decision making with regards to what kind of model you want to build. PCA is a process of figuring out the most important features or the principle component that has a maximum impact on a target variable. PCA will create in fact the new feature called principle component, you know PC1, PC2 and so on. So going back to the digits example. Let's say I have 64 features out of that I am plotting only two features, corner pixel and center pixel. Now here, these cluster represent different digits. You immediately notice that the corner pixel is not playing an important role. Maximum variation is on the y axis or maximum variance is on the y axis, which is a central pixel. So I asked you to reduce this two dimension into one dimension. You can easily do so by getting rid of corner pixel. So this graph that I have drawn on the right hand side, it is one dimension. I mean, I know the graph is not perfect, it still looks like two dimension, but you get my point to reduce this from two dimension to one dimension is easier. Let's look at the iris flower data set where sepal width and sepal height determine what kind of flower it is. If you are scatter plot like this, you can draw a line which covers the maximum variance. So this line covers the maximum variance or the maximum information in terms of features. And you can draw a perpendicular line which covers the second most variance, you know, and these are called principle component. So here PC one is this axis, which covers the most of the variance. PC 2 is the axis that covers the second most variance. So when you apply PCA, you get HR like this. And I know I have done only this graph for two dimension, but really if you have 100 dimension and if you apply PCA, you can figure out let's say 10 most important principle components. So for 100 features, you can actually create 100 principle components in the descending order of their impact on the target variable. So for digits, if I have to load this in a data frame, pandas data frame will look like this. These are little errors like pixels 6 to 3 because I start with pixel 0. But you already see pixel 0 and 1 has all values is 0. They are not kind of important. So when you use a scale on library and call PCA method where you say OK and components is 6, you're basically asking PCA to extract the 6th most important component. And that will look something like this. So what this is doing is calculating new features and these features are such that PC 1 covers the most variance in terms of features, you know, in terms of information extraction from your data set. PC 2 is the second most highest component that contains lot of information about the features of a data set. Here I give give 6, you can give it anything. You can have 2, 3, 4, 5. You know trial and error. You can also give a different parameter to this method which will be like 0.95 which means you know get me 95% of information in terms of features, you know, get me 95% of variance. So you can do that. We'll see that in the actual coding section. But few things to keep in mind before using PCA is you need to scale the features because if you don't scale it, it's not going to work okay. For example, here is a chart. And if this graph is in millions and this graph is in a normal value, you know, the graph might become skewed and PC and the PCA will not work as per your expectation. Accuracy might drop. So it's a trade off when you're trying to reduce, let's say you have 100 features and if all 100 features are important and they're trying to contribute to and target variable. If you reduce from 100 to 5, you're going to lose lot of information. So we'll see all of that in the coding section. But just to summarize, PCA is called a dimensionality reduction technique as it helps us reduce dimension. We already saw here in this case we had 64 dimension. This helps us to get to 6 dimension and when you have less columns, your computation becomes much faster. And it also helps you to address the dimensionality curves problem. In machine learning, dimensionality curves is a big problem. So many data sets having so many so many columns, you know, so many dimensions and that makes our model really complex how to visualize and PCA is an excellent technique that help you deal with dimensionality curves. Let's get to Python coding now. I opened my Jupyter notebook and imported some important libraries. I'm going to use the handwritten digits data set from a scale on data set module. Let's load the data set by calling load digits function. When you do that, it loads the data set and you can call data set dot keys to get an idea on, you know, what the data set is all about. You can see that data set dot data will contain all the pixels and these are the feature names and this is the target array. All right. Now let's look at data set dot data dot shape. So there are 17, almost 1700 or 1800 samples. Every sample has 64 columns. So if you look at the first sample, it's a flat one dimensional array of 64 pixels. And if you want to visualize this data using matrotolaryp library, you need to convert it into two dimensional array. And the way you do that is by calling reshape function because this is numpy array and when you do 8 by 8, it just converts one dimensional array into two dimensional array of pixels. Now you can import matrotolaryp and use the plot. You know, I'm going to just plot a gray image and when you do plot dot mat show, like a metric show, it will show that it will show a picture of that array. What is my array? This is my array, right? See two dimensional array. So I'm going to just put control C control B. And my first digit looks like this. This is clearly zero. If I look at my second data sample, see it is one, then two and so on. You know, like 50th sample will be, I think this is two. Okay. So this is how you visualize it. Now let's look at the target when you do data set dot target. It's a huge array, but it is your end class. If you do unique, you will find the way you do unique is by calling np dot unique. And you see the numbers are between 0 to 9. We already saw, let's say our 8 number here is which number it is. Okay. I can check it here. So if you do 8, this is the 8 number. I mean, I know it looks weird, but let's do the 10th number. So 10 number is 9. See 9. So this data set dot data is your feature and data set dot target is your class. Basically, we are classifying this in 10 classes 0 to 9. Okay. Let me create a data frame now. So I can do pd dot data frame and just supply data set dot data here. And that will create a data frame for you. But the column headers are, you know, randomly assigned. So I'm going to do columns. You can say data set dot feature names. The feature names are all the pixels, you know, like 0 through 0 pixel 0 through first pixel and so on. And I will just save that into a data frame object. Okay. My data frame object is ready. Now, I mean, I know from this data set, the values are between 0 to 16. But if you want to confirm, you can do df dot describe. And for each of the columns, if you look at min and max, I mean, this column is useless. But if you look at min and max for most of the good pixels, it will be between 0 and 16. Okay. So 0 is my minimum value 16 is my highest value. Now I'm going to store this in x and y. So df is my x and my y is my, you know, target set. So again, what is my x axis, my data frame? What is my y is my target classes, you know, the digit, which tells 0 to 9. Now I would like to scale these features before building my machine learning model. So I'm just going to use the standard scalar, you know, scaling is a common thing that I do before training my model. And as a simple, we're very convenient API to do that when you do scalar dot fit transform, it will just scale that X thing and you can store this into X scale. So when you print now X scale, you can see the values are standard, their scale, you know, using the standard scalar, I think negative one to one, you can use min max scalar as well. Okay. Now we are ready to train the model. So we are going to do a usual thing, which is train display, you already know this by this, by this point. And I'm going to call train display on my x scale. So X scale and Y and I will keep my taste size to be 20%. You can do 30% whatever. And I'm just keeping random state for reproducibility. If you don't want to keep it is fine. And in the result, what I get is X train X taste, Y train, Y taste. It should be in your muscle memory friends if you're doing machine learning for some time. Now I'm ready to train my model. Well, I'm going to use logistic regression. You can use random porous, this isn't really there are so many classification techniques, but I'll start with logistic regression. I will create a model which is an object of this class and then model dot fit again, pretty standard and model or score. So I do model or score to measure the accuracy. 97% see this is pretty amazing. I get 97% accuracy by training on all these features, but now let's do PCA. So to do PCA, you can import PCA from S scale on decomposition and then I will create a PCA variables. Let me increase my phone size. It's very simple PCA and here you supply two type of arguments. You can either say how many components you want. You can say two or five trial and error. You can figure it out, but what I'm going to do is I'm going to supply point 95. What this will tell PCA is that retain 95% of useful features and then create new dimensions. You're not explicitly saying I want only two dimension as an output. You're saying, you mean, retain 95% of information or retain 95% variation and then whatever features you come up with, I'm happy with that. And then PCA.fit transform. The API is quite simple. You supply your data frame here. You get a new data frame with new feature, new principle components and when you do xPCA.shape. See now you got only 29 columns. If you look at your original data frame, you had 64 columns. So 64 columns to 29, it probably got read off unnecessary things such as this guy for example. See, zero pick. So all values are zero. See, look at here. This feature is not needed at all. So it probably got read of that and it did not, by the way, just to make it clear, it's not like it will pick 29 columns out of it. It will calculate new columns. Okay. You do xPCA. You see, it actually calculated new columns for you. And if you do PCA variable dot explain variance ratio, it will tell you that my first column is capturing 14% of variation or 14% of useful information from my data set. PC2, second component is capturing 13% of information and so on. So this is how you print explain variation ratio. There is another thing called n components. It will tell you how many components you got, which is basically your columns, you know, 29. So here PCA help me reduce my columns from 64 to 29 all this 29 columns are computed column. They're the new feature. But good news is you can now use this new data frame to train your model. So I'm going to do train display once again, but this time I will supply xPCA. Y remains Y, then my test size is 0.2 and I'll just keep my random state to be this. And as a result, what you get is your train and test data sets. And I'm going to use again, same linear regression, logistic regression, sorry. And I will say logistic regression. This time I will fit on PCA and then model dot school and Y is say I got pretty good accuracy 97% but let's say this failed to converge because total number of iteration limit reached. So I need to supply some maximum iteration. So I will say max iter. So I need to increase my iteration basically to so that the gradient descent can converge. See almost same accuracy. So you are realizing that despite dropping lot of features, my accuracy is almost M here is 97.22 here is 96.99. So it's almost similar. Okay. Now you can create PCA by supplying the components explicitly. So I can say, okay, PCA, let's say my number of components is two. Now this is what is going to happen when you explicitly say my components are two. So I'm going to do PCA transform with my xxpca.shape. Now we are telling that we want to components. Okay. So it it will identify the two most important features. But here, see, let me see. So see, every data point is just two features. Now did it capture enough information? We are not sure. When you print explain variation variance ratio, you are capturing 14% and 30% around 2728% information. So naturally, when you use this data set to train your model, the accuracy is going to be low. So I'm just going to like same code, but I'm using the new PC and the new data for him. When you train the model, you see the accuracy is reduced to 60, 60.83. So you take a hit, but the benefit you got here is your features are only two. So your computation can be computation can be very fast. So you can decide what accuracy you want and then you can reduce your dimensions accordingly. So that's all I had for this video. Now let's move on to the most important part of this tutorial, which is an exercise. I'm going to provide a link of this exercise below, but it's very important friends that you work on exercise. And I have given a very good exercise. There is a data set, which you can download from KGLE or you can see if you click on here, there is this hard CSV. And so there is a data set for hard disease and using this data set, you need to predict if the person has hard disease or not and you're going to use PCA. So work on all these six aspects of the exercise and let me know how it goes in the comment post below your solution, your GitHub link, whatever. And if you have any question post in the comment below and if you like this video, share it with your friends so that they can learn everything is for free friends exercise explanation. All my YouTube videos are free so you can have a learning at almost no cost. So please set it with your friends and give it a thumbs up if you like this. Thank you.",
    "Deploy machine learning model to production AWS (Amazon EC2 instance).wav": " I'm going to show you how to deploy machine learning model to production today. We are going to deploy Bangler home price prediction website to Amazon EC2. On the same cloud EC2 instance, we will have our website as well as our python flasks are running. Eventually you will be able to load this kind of URL browser and the website will be fully functional. This is a temporary URL that Amazon gives but if you buy a domain, let's say ABC.com from GoDaddy then you can map that to this URL and then after that when you do ABC.com it will load this website and it will be fully functional. It is going to be so much fun deploying your code to production and running it end to end so please watch till the end. Here is the brief architecture of our application. In Amazon EC2 instance, I will be using engine X as my web server. Engine X is a light web server which can sell to HTTP request. When you load the URL into your web browser, it will go to our instance and from engine X web server, it will return all app our app.html.js and CSS file back to the server. After that when you click on a button to predict the home price from your JS file from your browser, it will again make a call to engine X server and will be using a reverse proxy setup on engine X to route all our requests, slash API request to our python flasks server which will be running on the same EC2 instance and using the saved ml model to solve the prediction request. So this is just a brief architecture. We are going to first setup engine X in development environment on windows and once the setup looks okay, we will do the same setup on Ubuntu server in our EC2 instance. First download engine X by googling this, you will get this link, go to that link, click on this particular link to download the zip file, unzip the zip file. So you go to download the file here, unzip it and then I copied that folder into C program files. If I go here and search for engine X, I have this folder. In this one, the configuration file is very important. So if you go to conf and click on engine X conf and edit that file. Here you will not a couple of things. But before we go into this file, let's test that this server is looking good. So for that, what I will do is I will go to here, double click on this. So that has started the web server. You can see that into process. You can see the web server is started. What it helps you is when you do local host like this, you get welcome engine X HTML page. So by default, now it is going to a specific directory and loading the HTML page when you say local host. So what is that? So that is specified into engine X.conf. So here when you look at it, it is saying that location is root is HTML. And in that, if you find either index.html or index.html, render that file. Okay. So let's see HTML folder. So here I have HTML folder. In that when you see index.html, it is rendering this file in the browser. So if you look at this file, you can see welcome to engine X. So this is the file that is being rendered here. So now I what I want to do is when I say local host, I want to render my Bangalore home price page. How can I do that? I have that folder into my see this directory. So all I have to do is instead of HTML directory, I have to make this my default directory so that it loads app.html. In order to do that, what you have to do here is instead of HTML, just specify that path. So I'm going to chain this to this. So now by default, when I say local host, it will go into this directory and it will look for either index.html or index.html. But our file name is app.html. So you can just add app.html here. So now the file is saved. You need to kill the old server and start the new one. So I'm going and the process and then go to engine X directory double click on it. So the engine X server is started again. You can see it here and when I go and here run local host, it is now loading my website. So you can see that now you don't have to load a specific HTML page. You can just say local host and it will load the website. What it is not doing however is it is not rendering and this estimate price is not working, etc. So we have to make that working. For that, you can just go to your app.js and in app.js, what I did is see previously I had this goal where I was making HTTP call on my local host on 5000 port, specific port. I want to change that and I want to just say less API get location name. I don't want it to be bounded by specific IP address and a port. It should be all dynamic. Again, this codebase is covered in our previous seven tutorials for data science project of predicting home prices in Bangalore. So if you have not seen those tutorials, I highly recommend watching them. Same way I have changed this URL as well for getting a location name. So you can see that now it's not hard coded to specific port and IP it is very dynamic. It is saying slash API slash predict home price. Now when you to slash API, how does it know which server to go to? Okay. So we are going to run our fast server on 5000 port only. We need to configure the reverse proxy in engine X web server so that whenever do local host slash API slash predict home price, it routes those requests to 5000 port. So we can do all of that in the same configuration file and the way to do that would be this. So we can copy this here. So let's see. So you can do that copy. So just add this particular section here in your comp file. What this is saying is whenever I get any HTTP call which has less API and after slash it could be anything. Okay. That request should be routed to the same host but the 5000 port. So you can see that we are dynamically routing slash API request to the server running on this specific port. And once you do that, you need to again terminate your engine X process and kind of restart your engine X once again. So I'm going to restart it now that my file is saved and my reverse proxy is set up. I am running it. So now my new server is running with the new configuration. I need to now run Python Flas server. So Python server is in the server directory. So I will just say Python server dot pi. So this party same. The same flash server is running on 5000 port. All right. Server is up and running. Let's test our website now. So when I refresh to you can see that it loaded all the locations. And when you estimate the price is working. So now you don't have to do specific port anything. Just say local host and your whole website is up and running. You can place F12 to see the incoming and outgoing traffic. If you don't know about how to debug JavaScript code in Chrome, you should learn it. It is going to be very, very beneficial. In the network request, when you press F fire again to refresh, you will see get location name request. And in the response, you see that this is the response returned by our Python Flas server. When you click on estimate price, again, you see new request predict home price where the estimated price is returned like this. You can go to sources to see the source code or you can also set a break point like if you click here, it will set a break point. If you want to debug it, you can just say F5 and F5 say it gets you here in a break point. So if you do like either do F10 or click on this step into next function call and it will show you all the state. So this debugging could be very useful. I have another break point here. And if I just click on this, it will come here and you can just see in the data that you got this location array back, you can see that data here. You see data locations. So you got all those locations back here. My local setup looks good. Now let's move on to cloud. You can go to AWS console, sign into the console using your Amazon account. You can create this account for free. Once you are here, type in EC2 Amazon AWS offers different services. EC2 is basically virtual machine in a cloud. They also have storage service, which is the three they have Amazon text track, Sage maker, different services. We are interested in EC2. Once you go here, we are going to create a virtual machine. Okay. So you'll click on launch instance. Here you can select a specific machine. You have different option. There is red head. These are like different versions of Linux. We are going to use Ubuntu. So you can just say select and we will be using free tire. So again, Amazon will charge you if your usage goes beyond certain points. So whatever we are doing is pretty much it requires less resources. So we are going to use a free tire. I will skip through all these steps and then because the default setting that Amazon is giving, they are good. They are giving me eight gigabyte storage on my virtual machine, which is okay. I will go to configure security step and create security group name. So I will just say Bangalore home prices. Okay. That's my security group name. Here I want to add a rule saying that for all HTTP. So click on HTTP. HTTP request are served on port 80. HTTPS. I is served on port 843. So if you do HTTPS, it is port 443. This is the standard. And I want to add these two so that from outside world, anyone can call my web server and they can make HTTP request. Okay. We are going to make HTTP request from your local Chrome browser. Hence these two rules are required. Now say review and launch. So this is the review. Okay. These are the rules. Instance details, storage details, C storage, 8 gigabyte or virtual machine is pretty small. Click on launch. Here you need to choose a key pair. So let's create a new key pair. So I will say create new pair and I will say Bangalore home prices, BHP and then download key pair. So this is downloading a PEM file which is Amazon ways of authentication so that when you connect to their EC2 cloud server, you don't have to specify password but you can specify this PEM file which will kind of serve as a password. So this file I am going to copy. So you can see control C and go to your user's directory. My user directory name is Vidal and here go to dot SSH directory and control V. So now your dot PEM file is saved here. You make sure you save this file because that is an important step. Now click on launch instances. So it will take few seconds or few minutes to launch your instance. You can go to view instances here and it will show you all the running instances or pending instances. So I have this pending instance. I have one more instance which is running but ignore that for you you will be seeing this as your pending instance. So we are waiting for that to be initialized. You can see that my new EC2 instance is up and running. The next step is to connect it using SSH client. So what we are doing is from your local Windows machine, you are connecting with the EC2 instance on cloud. For that you can use git bash. So if you have not installed git bash, so git bash, Windows just download and install it. It is again a free software on that type in SSH minus i. i is your key location. So if you remember we saved bhp.pem in this location and then the EC2 instance identifier. So here if you click on this and if you say connect it will give you the connect instruction. So the connect instructions is this. So this is user at public DNS for that instance. So I am going to copy paste here. Yes. And now I am connected to that instance. If you look at pwd see I am in that home we are going to directly now I am on cloud. Whatever I am doing on this Linux machine that is happening on Amazon AWS cloud. You can see it will have see different directory. This is a Linux operating system now. Now let's copy all our code into cloud machine using win scp. So win scp icon is here I am going to double click on it to launch it. And now here you want to connect to EC2 instance. So what is your host name? The host name is here. So just control c, control v, your user name is Ubuntu. The password is specified by that dot pem EC2 here is not using any password. We are using that dot pem authentication key. So here click on authentication and in private key go to that directly see users viral dot ssh here click on all private keys. So our private key was bhp dot pem when you say open it needs to convert that into ppk file because putty understands ppk hence you can just say okay and it will do auto conversion from dot pem to dot ppk file. So you can just say save okay now you got ppk okay login. So now it's connecting to our cloud server. So now connected successfully you can see home ubuntu there are no files here. So here you can click on this icon and go to different directories. So my code base is in cpy data science file. So see I have bangle of home prices where I have client models server all the files. So I'm not going to copy the entire folder. So you can just drag and drop. So this will copy the files from local machine to your cloud server. And once the copy is over you can come here and in your prison working directory when you do Ls you will see bangle or home prices here. Now we can install ngx on our ubuntu server. For that you can first run pseudo apt get update this will update all the packages that you have locally on this particular EC2 instance. Once that is done we can run the same command but this time let's install ngx ngionx and that will install ngionx web server on EC2. apt get is a way to install software on ubuntu. My ngionx is install perfectly fine. You can check the status of ngx by saying pseudo service ngx status. So this is saying it's running. So when we install it it not only install it it started the server as well. Now how can we confirm that this server is running fine. For that you can load the web URL into your browser. So for our server the web URL is this. When I copy paste that web URL see it is saying that it is running ngx on that machine. I just change the font size and background color to make it look little more better. Now here also ngx server is by default rendering some HTML. So when you go here and when you see welcome to ngx file where does it get this file from. We just need to replace this HTML with our Bangalore home price homepage. So let's check that. So the ngx folder structure is little different on ubuntu versus windows. The directory is this. It is in ETC. So this is the directory. If you do all the files these are the files it has. And if you look at the ngx comp file so let's check the ngx comp file. So this file is actually pointing us to this site enabled directory. Okay. And from site enabled directory it is rendering that HTML file. So now let's look at the sites enabled directory. So sites enabled directory has this default file. It has only one file which is a default. And when you look at the default file the default file root is var is var wwwhtml. And when you look at var wwwhtml index dot this HTML you can see that welcome to ngx. So this is the HTML page that is being rendered here. All we want to do is change this default page to Bangalore home price prediction home page which is in home ubuntu directory. Now let's go back to our sites enabled. So whatever is in the sites enabled directory it will use that as a home page for port 80 default HTML rendering. And here there is only one file if you do LL you will see that it is actually a simlink to sites available. So the general practice in ngx is that the configuration files are kept in sites are available and there is a simlink that you create into sites enabled. Okay. So first of all I want to disable the default configuration so I can just say pseudo unlink default. So now I don't have default file here because I don't want that. And I want to go to sites available. Now you can see that sites available has default file as well. And I want to create one more file here called bhp.conf. So I will do vim bhp.conf. And this configuration file will point me to my Bangalore home prices directory. Now if you go to my GitHub in the on this particular link which I'm going to provide in the video description below I have given the bhp.conf file just to save some typing time so you can just copy paste that. Let me explain. So what this is doing is you are running a server on port 80 which is a standard for HTTP request. You are just giving some server name and your route it is home Ubuntu directory. So remember on vim at cp in home Ubuntu we have Bangalore home prices, client and app.html is our home page. So that's what we are doing. We are saying go to this route and look for app.html file and it will load that file. This is our reverse proxy setup where it is saying that on the same server if you get less API, less calls then pass them to 5000 port. And on the 5000 port we will be running over python flash server. Okay. So let's save this file. So now I have two files bhp.conf and default and I will create a simlink in my sites enabled. The way you do simlink is by running this command. So using this command, okay let me just make it a small one. Okay using this command I am creating a simlink. When you do ll now see the simlink is created. You can see there is a simlink for bhp.conf and bhp.conf looks something like this. All right now let's restart our ngx server so that it can use this new configuration. Whatever is in this particular directory it is going to use that configuration. Okay. So we can run pseudo service ngx restart and now check the status. So the status is saying just five second ago I started this servers it is up and running. Now go to your browser, refresh the page and you can see that now it is loading your home page for the website. Okay. So this part looks good but still the call to the back end to the python flash server is not working. You can see choose location is not there estimate price is not working and if you do f12 if you refresh in the network you will see that this request is failing with fire to bad getaway. Well obviously because our python flash server is not running on that machine so we need to run it. So just go to get bash I'm going to now start my server for that you can go to home directory server and just start the server. Now before we start the server we have to install couple of things. We have this machine is blank it doesn't have all the modules which are required by our server. So if let's do Ls so this particular directory has a file called requirement.txt so what is requirement.txt let's check that requirement.txt has all the python modules that we need for our backend server. So you are going to install that by running first let's get pip 3. So first let's check if python 3 is there. So when I run python 3 I get it so python 3 is already installed let's see pip 3 is there so pip 3 is not there and they are saying okay install it using this command file just install it. So install pip 3 first it will take few seconds and then pip 3 gets installed now use pip 3 to install all the requirements. So what with what this will do is it will go to requirement.txt look at all the modules and install all of them. All my modules are installed. Now I can start my server python 3 server.py the server is up and running on this local host 5000 port and now refresh this page. When you refresh it you see voila all the locations are here and click on estimate price and now your code is working. See now you have fully functional website hosted on amazon easy to cloud. You can then go to go daddy or blue host by the domain banglore home prices bhp.com map it to here and then later on you can run bhp.com and it will load this website and it will be up and running. Whatever code we covered in this tutorial I have it available on my github I am going to provide the link into video description below. The github readme file has a useful instruction the kind of commands that we went over in this particular tutorial. So look at that readme file and try to deploy the model on your own. Practicing on your own is very important and amazon easy to free tire instances are you don't have to pay any money for it. So just go try it out. Let me know how it goes. If you have any question post in the comment I try to answer as many comments as possible. Also if you like the video give it a thumbs up.",
    "Machine Learning & Data Science Project - 1 _ Introduction (Real Estate Price Prediction Project).wav": " We're going to start working on a real-life data science project today. In this tutorial series, I will give you a glimpse of what kind of steps and challenges a data scientist working for a big company goes through in his day-to-day life. Assume that you are a data scientist working for a real estate company such as zelo.com here in US or magicbricks.com in India. Your business manager comes to you and asks you to build a model that can predict the property price based on certain features such as square-fade, bedroom bathroom, location, etc. On zelo.com, this feature is already available. They call it Zestimate. It shows you the Zelo-estimated price. Just to make this project more fun, we are also going to build a website using HTML, CSS, and JavaScript, which can do home price prediction for you. In terms of project architecture, first we are going to take a home price data set from kegel.com. This is for a Bangalore city in India and using that data set, we will build a machine learning model. While building the model, we will cover some of the cool data science concepts such as data cleaning, feature engineering, dimensionality reduction, outlier removal, etc. Once the model is built, we will export it to a pickle file and then we will write a Python Flashtra server which can consume this pickle file and do price prediction for you. This Python Flashtra server will expose HTTP endpoints for various requests and the UI written in HTML, CSS, and JavaScript will make HTTP get and post calls. In terms of tools and technology, we will use Python as a programming language, we will use pandas for data cleaning, matplot flip for data visualization, SK learn for model building, Python Flask for a back-and-servor, HTML, CSS, and JavaScript for our website. Overall, you will learn a lot and it will be a very interesting project for you. So without wasting any more time, let's get started.",
    "Machine Learning & Data Science Project - 7 _ Website or UI (Real Estate Price Prediction Project).wav": " In last video we build a Python Flask server which is going to be used as a back end for our website. Let's build that website in this video. We only have these three folders where server folder contains our server, the model folder contains the model and the Jupyter Notebook that we built, and the client folder is empty. Here we are going to build our website or our UI. It is going to be a simple HTML CSS JavaScript website. I am going to create three files here and let's call them app.html. And app.css. And the third one is app.js. In any simple web application you will have three files. HTML will contain the structure of your UI, your elements, your anatomy of the application. CSS is a cascaded style sheet. It will contain the style sheet like the color look and feel of the application. JavaScript will contain the dynamic code which will make HTTP calls to or back and retrieve the data. So this is more of a server communicator. You know it communicates with server, gets you the data and runs any dynamic code. If you don't know the fundamentals of HTML, CSS, and JavaScript then you should go and find some other online resources and get those fundamentals clear. You need just basic understanding of these three concepts in order to continue this tutorial. Okay. As I have these three simple blank files created, I'm going to open this into visual studio code. Visual studio code is an editor from Microsoft. It is a very nice fast lightweight IDE. So that's what I'm going to use for my UI code editing. Okay. So I have these three files. You can see these three files are blank right now and I'm going to start with my HTML. So for HTML I already have the HTML code ready so I'm just going to copy paste. Okay. So what I have here is any HTML will have two sections. One is head, another one is body. Okay. So this is my head section where I have a title of my application. I have I'm going to use jQuery to make HTTP calls for that I have imported this jQuery JS file. Also I'm going to include app app.js and app.css. This is more like you know in Python you import the files so this is similarly we are going to include JS and CSS here. From here my main application body starts. So in my main application as you know from our UI mock up we will have the area will have BHK. Okay. Area will be an input. So you can say this is an input entry. Then you have BHK which is a button bar. Okay. We have a bathroom option that is also a button bar and our options are one bathroom to five. Okay. You can add more if you want to. Then the last one is location. So location is and a drop down which we are going to dynamically populate in our JS file. All right. Then we have CSS. So the CSS are all these classes. See here I'm saying class location. So this location class I need to define into my CSS file. Again I'm copy pasting the code because this is not really an HTML and CSS tutorial. So just to save time I'm just copying and pasting here. Okay. So here are my CSS. For example we just looked at location class. So a location if you search here this is the class. Okay. And these are all the UI elements. You have background color. You have font which kind of font you want to use on application, et cetera. Okay. Now let's run the application when I double click on app.html it looks something like this. So it has a nice UI where all our UI elements are available and the locations are still hard coded because we have not written app.js file.js file is the one which makes HTTP calls and gets the data from the back end. So now that our UI looks pretty much good. We are going to write app.js file. So app.js file is here. So let's first include app.js file and this is how you include the app.js file. And in this first routine we are going to write is it will load the locations. You can see the location drop down has only hard coded values. These two which we hard coded in app.html you can see that we have these two hard coded locations. Okay. But if you want to get these locations from the back end and how do you do that? So in JavaScript there is this call call on load. So you can say window.on load is on page load function which we are going to define here. So I can write this function call on page load which will call certain routines when the page is loaded. When the HTML page is loaded the first thing we want to do is load the locations. Okay. And the way you load the locations is by writing this code. So here I'm making the same HTTP call which we tested using Postman. And you can use a dollar. This is for jQuery. When you say get it is making a get call to that same URL which we tested using Postman here. So you see this is the URL we call and we got this particular response back. So the same thing in JavaScript here in the data you will get that response back. And once you have the response you want to do data dot locations because you can see here we have data dot location that will be the list of locations. And then you iterate through those locations one by one and add those locations into our drop down. Okay. So let's test this code out. So here you can just refresh the page and when you refresh the page you can see that now this contains all the locations in Bangalore city. Once the locations are loaded now we can implement a function for estimate price click. Okay. And on estimate price click we have on on click event we have this function JavaScript function which is going to be run. So we can write that function here and that function name is on estimate price click. So what do you want to do on our estimate price click? Well you want to get the values of all these features area, BHK and bath. Okay. So let's first get the values of these UI elements. For that we are going to write two more functions which will be get bath value and get BHK value and this is how those functions look. And these functions are iterating through the button bar that we have and they are giving the value back. So if on your UI you have click let's say button number three then this particular function will return number three. And similarly get BHK value to return the BHK. So let's call these functions here in our main function we are going to call those functions here. I get the value of sqft, BHK bathroom location etc. Okay. And my URL for the price prediction and point is predict home price. And the way you call it is using jqaries a post call. So this is jqary dollar dot is a jqary object. You're making a post call because you saw that here in predict home price function. We made a post call with all these inputs and we got this as an output. So once we make this post call in this function we get an output back and in the output there is an element called estimated price. You can see estimated price here which contains the price in Indian lag rupees. So all we are doing is we are painting this EST price HTML element. So all we need to do is just refresh the page. So my page is refreshed and let's predict the price of Rajaj in Nagar. So when you click on estimated price it says that the price of this property is 2.25 Lakh rupees. If you have 3 bedroom, 3 bath it will be a little more. Then depends on the location of, okay. So if you have a different location let's say Nahrunagar, the same property price is much less because Nahrunagar, you know Rajaj in Nagar is probably more expensive area. If you have much bigger home like the double size then see 1000 square foot was 30 Lakh then the 2000 square foot is 100 in 10 Lakh. So our application is now ready. It is working fine locally. In production if you want to deploy there are some sets of steps and procedures that you need to follow. But in a typical work environment a data scientist goes through this process where he built model first then the Python Floss server and the HTML application. Sometimes there is a software engineering team which will be responsible for building this UI and backend application and data scientists will just build the model. That is the case in majority of the data science roles. But depending upon your job profile or the company you might also build these applications so that you can at least test them. So it's important to have some knowledge of web development or some UI development knowledge and some knowledge of Python Floss server etc. how to write HTTP backends etc. if you want to be a successful data scientist. Alright so I hope you enjoyed this tutorial series. This should have given you a very good understanding on how a data scientist working for a big firm builds an application N to N. So just to summarize we first built our model we did data cleaning we did dimensionality reduction feature engineering model building outlier detections all of that then we moved onto writing a Python Floss server which was serving HTTP get and post request and the third element was our UI or our website which will make the calls to the HTTP backend and it will return the it will get the response back. Okay so very well the whole application N to N. I hope you enjoyed this series and if you liked it please give the thumbs up to all the videos please share it with your friends anyone who is a new data science aspiring would find this series to be very very useful. Thank you very much for watching bye.",
    "Machine Learning Tutorial Python - 21_ Ensemble Learning - Bagging.wav": " Once I wanted to buy NIST thermostat and I wasn't sure if I should be buying that or not. I then called four of my friends who already have that device and then asked for their opinion. Three of them told me I should buy it, one guy told me no I shouldn't buy it, and I just took a majority vote and I went ahead and installed NIST thermostat in my home. We use Ensemble Learning in our real life where to make a decision, we take opinion from different people. Similarly in machine learning, sometimes what happens is if you have just one model and if you train that model using all your data set, that model might get overfit or it might suffer from a high variance problem. If you don't know about bias and variance, I have another video which you must check before continuing on this video but to tackle this high variance problem, we can use Ensemble Learning. In the case of my NIST thermostat, why didn't I call just one of my friends? Because if I call only one person, that person might be biased, so I wanted to make a good decision. Hence I talked to multiple people and took a majority vote. In Ensemble Learning, we train multiple models on the same data set and then when we do prediction, we do prediction on multiple models and then whatever output we get, we combine that result somehow to get our final output. Banging and boosting are the two main techniques used in Ensemble Learning and in this video we are going to talk about bagging, we will also write some Python code, let's get started. Let's say I have this data set of 100 samples and when I train a machine learning model, one of the problem I might encounter is overfitting. And overfitting happens due to the nature of the data set, your machine learning, methodology, etc. And usually overfitting model has a problem of high variance. To tackle this problem, one of the things you can do is out of 100 samples, create a small data set of 70 samples, let's say I'm just giving an example. And the way you create this subset is by using resampling with replacement. Now what is that exactly? Let's first understand that. Let's say I have this 10 samples out of which I want to create smaller data set with four samples. In resampling with replacement, we randomly pick any data point, let's say fourth. And then when we go and pick second data point, we again randomly pick any data point from one to 10 with equal probability. We doesn't look at what we already have in our subset. So second time, I will let's say get eight. Third time also, we randomly pick any data point from one to 10. And this time I might get same sample again. So this is resampling with replacement where in my subset, I can get same data sample multiple times. So here from my original data set, I created a subset of 70 sample. I might create a number of such smaller data sets from my original data set, using resampling with replacement. And then on individual data set, I can train my machine learning model. Let's say I'm trying to classify if a person should buy insurance or not. And I'm using logistic regression. So I will use logistic regression model. So here M1, M2, M3, they all are logistic regression. But they are trained on a different data set. And when they're trained and now I have to perform the prediction or inference, I will perform that prediction on all three models in parallel individually. And whatever result I get, I just take a majority word. So here M1 and M3 is saying persons should buy insurance. M2 is saying they should not majority word is clearly one. And that is my final outcome. The benefit you get here is these individual models are weak learners. Weak learners means they are trained on a subset of data set. And hence they will not overfit. You know, it is likely that they will generalize better. They will not overfit. And these individual weak learners, when you combine the result, you get overall a good quality result. This was the case of classification. Same thing applies for regression. Let's say you're doing housing price prediction. Here you take an average of whatever is the prediction by individual model. Now this technique is also called bootstrap aggregation because when you are creating this small subset of data set using resampling with replacement, that procedure is called bootstrap. And when you combine their results using either an average or majority word, that is called aggregation. So hence begging is also called bootstrap aggregation. Many times you hear all these terms and chargons and you get worded, worded is. But really these concepts are very, very easy. You know, you already understood what bootstrap aggregation means. Now random forest is one of the bagging technique with only one distinction which is here we not only sample your data rows, but you also samples your features. So basically you sample your rows as well as your columns. Let's look at our classical housing price prediction example where town area bedroom et cetera features and pricing, which is a green column is your target variable. Here you will sample or over the columns both. So here you can see I don't have a bedroom column or a plot column. I randomly re-sample out of see one, two, three, four, five, six out of seven columns. I got only four columns. In the second time again, I randomly sample this column. So I did not get for example here bedrooms in this particular data set. And in the third one, for example here, I did not get school rating. Okay. So you are re-sampling your randomly picking what rows as well as columns. Then on individual data set, you train a decision tree model and then you aggregate the results. Here I have decision tree regression. You can use it for classification problem as well. But the point is very simple random trees basically a bagging technique. But here we do one additional thing, which is we randomly pick features as well. And the difference between these terms bagging and back trees is that in bagging individual model can be SVM, KN and logistic regression, pretty much any model. Whereas in bagged trees, so random forest is a bagged tree example. Here the every model that you're training is a tree. Alright, so I hope the theory is clear less move on to Python coding using a scale on. I will be using this data set for the coding today. The data set is about Pima Indian diabetes where based on certain features, you have to predict whether the person has a diabetes or not. By clicking on this link, I have downloaded this CSV file, which looks something like this. Here the features are pregnancy glucose, blood pressure. These are all contributing factors for diabetes. Based on these, you're deciding whether the person has diabetes or not. I have loaded this CSV file in Pandas data frame, as you can see here. And as soon as I load data in my data frame, I like to do some basic exploration. So let's first start and find out if there is any column which has null values. So the way you find it out is you will do DF dot is null dot sum. This will tell you in this column, unless if the number is five, which means it has five null values. But we are lucky here. There are no null values. So no need to worry about it. Second thing that I do is DF dot describe. This tells me the basic statistics for each of the columns. For pregnancies, look at mean and max. You know, max I understand 17 is little high, but it's not unrealistic. When I examine min max values in all these columns, they look normal and I don't feel like we need to do any outlier detection or I outlier removal, etc. So I will just go ahead and assume that there are no outliers. The second thing I check is whether there is any imbalance in the data set. Because see, there is this outcome, right? So here, if you do value counts, what you will find is there are 500 samples, which says no diabetes 268 samples, which says yes, person has a diabetes. And if you look at the ratio, it's 0.53. So it looks like some imbalance, but it is not a major imbalance. Major imbalance would be like you know 10 to 1 or 100 to 1 ratio. This is more like 2 to 1 ratio. So I would say that this very slight imbalance, but it's not something that you should worry about. So I would go ahead and move on to the next stage, which is creating X and Y. So my X will be df dot drop because outcome is my target column. I need to drop that in the way you do that is by using drop function in pandas and in the axis, you will say columns and why would be df dot outcome? Okay, so my X and Y is ready. Now I will do scaling because the values are on a different scale here. You can see this particular number, you know, 0 to 17 versus 0 to 1, 99. I mean, it's not a huge difference in the scale, but still just to be on a safe side, I will use standard scaling. You can use min max scalar as well. So let's create our scalar object scalar. And if you've followed my previous videos, you know how to use scalar object. You can just say fit transform X and what you get in return is your X scale. This will be numpy array. Hence I will just print like first three rows out of it. You can see the values are scale. If you use min max scalar, you'll get different set of values, but both works okay. Now, this should be in your muscle memory. When you have X scale and why ready, you will do trained this split. And for that, I will import this method, intent train this bit. What do you supply X and Y? Right? X and Y. What is our X? X scale. Right, we want to use the scale value. And then in the output, you know, the standard output that I get is X train X test, Y train and Y test. Now here, I will use stratify argument because there is slight imbalance. So I want to make sure the test and train data set has equal number of samples, like equal proportion basically. And I will say why I mean, it won't be equal, but at least the, this ratio should be maintained. And random state I will set to 10 or maybe 20. Let's go 10. This is just a random number, by the way. And it allows you the reproducibility. Every time you run this method, you will get same X train Y train. So if you do the shape, okay, 5 76 samples in your train data set. And this data set has 192. And if you look at this. Oh, no, actually, you know what? I should be looking at Y train dot value count. And if you look at this. Around same kind of ratio that you saw here. Okay, now. We will use this is entry classifier to train a standalone model. You can use SVM, K, N, E, or S neighbor, any classification model. I'm using this is entry to demonstrate that this is entry is relatively imbalance classifier. It can overfit and it will, you know, it might generate an high variance model. So let's train this is entry first. And I will use cross validation score here to just write out on a different type of data set rather than just X train and X train. So cross validation score. If you don't know about cross validation score, I have done separate video on K fold cross validation. You should want that if you're not aware. Otherwise, you know, this thing will bounce off your head. So this isn't cross validation score expects a classifier, which is my decision tree classifier, then X and Y. And then CV is equal to 510. I'll give five. So what this will do is. If I have. If I have 768 samples, it will divide them into five folds. And it will try different set of X-taste. And you know why it is to X train and X-taste to try the model. You should watch my video on K fold cross validation. You will get a good understanding of it. And this will return. This will do like five time training. And all those training results would be inside the scores. So see the score that you got by running five iteration of training is this. And if you take up this is an empire. So just take a mean of it. You'll find your model is giving you 71% accuracy, which is okay. But now let me use bagging classifier. And first thing you can do is ask your friend. S-K learned bagging classifier. Your friend is Google and Google will tell you which API you need to use. So here, see, I will use the most important tool for any programmer, which is copy paste. And I create a bagging classifier. And bagging classifier, you can read all the arguments. But I'm just going to use couple of arguments. First of all, which estimator you are using. Well, I'm using this is entry. How many estimators, like how many subgroups of data set? 100. Trial and error, okay. You try 10, 20. Figure out which one is giving you best performance. And this 100 is nothing but this. See in my presentation, I said three models. I'm doing 100 models and 100 subset of data sets. And I'll be training them in parallel. And how many samples? See here, we used 70 out of 100. So for sampling, use 80% use 80% of my samples. There is another thing called OOB score. So OOB score is equal to two. Now, what is OOB score? Well, OOB means out of bag. When you are sampling, because you are doing random sampling by the law of probability, you're not going to cover all hundred samples in this subset. Let's say in this subset, all this subset, number 29 did not appear at all. Okay, so number 29, let's say number 29 is here, right? One, two hundred number. That 29 number sample did not appear in any of this subset. So now, all these models which are trained, they have not seen that data point 29. So you can use that 29 to test the accuracy of these models. So you are kind of treating that 29 sample as a test data set. Ideally, what you do is you take your data set, you split into train and test. So this diagram that I have shown here, this block is actually your X train. So your X test is separate already, which you will be using to test the performance of your final model before deploying that into a wild. But even within X train, because of our sampling strategy, you might miss some samples. Let's say you might, you have 20 data samples, which has not appeared in any of this subset. And those 20 samples, after these models are trained, you can use those 20 samples for prediction, take the majority word and figure out what was the accuracy. And that accuracy is your OOB score. So you realize that, okay, okay, let me first do random state here. Again, random state is for predictability and I will call this a bag model. And when I have a bag model, I will do OOB score. OOB score. OOB begging classifier. Actually, you know what? I have to fit. So I'm doing X, you know, X and Y train fit. And then I get this. You realize I did not try even X test and Y test. On training data set, when I did 80% sample, when I trained 100 classifier, it might have missed some of the samples from my training data set. And on them, I ran my model prediction and the accuracy I got is stored in this OOB score. Now I can do regular scoring X test, FITES, and you see improvement, right? 77% versus stand-alone model giving you 71%. Now, I agree. You will tell me here you use cross validation. Here, I did not use cross validation. So let me use cross validation. Then, so I'm going to do some copypissed magic and create the same bagging model. And then use cross validation score. Okay. In cross validation score, what do you supply? You supply first your model, then X, then Y, and how many folds? Well, five folds. You get scores back and those scores, you can just take a mean. You will find that the base model gives you 70% for one percent accuracy. Bagged model gives you 75% accuracy. So for unstable classifier like this is entry, bagging helps. If you have a classifier, you know, sometimes you have unstable classifier like this is entry. And sometimes your data set is such that there are so many null values. You know, your columns are such that your resulting model has high variance. And whenever you have this high variance, it makes sense to use bagging classifier. Now, we talked about random forest. So let me try random forest as well on this particular data set. So I will try random forest here. And I will say, okay, random forest classifier X, Y, C, V, equal to five. I get scores and pretty straightforward X mean random forest classifier gives me one little. Little better performance inside like underneath it will use bagging. It will not only sample your data rose, but it will sample your feature columns as well as we saw in the presentation. Now comes the most important part of this video, which is an exercise. Learning coding or data science is like learning swimming. By watching the swimming video, you are not going to learn swimming. Obviously, similarly, you need to work on this exercise. Otherwise, it will be hard to grasp the concepts, which I just taught you. Here I'm giving a CSV file, which I took from Kegel, by the way. And it is about hard. This is prediction. You have to load the data set, apply some outlier removal. I have given all the information here. So work on this exercise. And once you have put your sensor effort, then only click on solution link. Because I have an AI technology built into this video, where if you click on this link without trying out on your own, your laptop or computer will get a fever. And it will not recover for next 10 days. So you will miss all the fun. So better you try first on your own and then click on the solution link. I hope this you like this video. If you did, give it a thumbs up at least and share it with your friends. I wish you all the best. If you have any questions, there is a comment section below. Thank you.",
    "Machine Learning & Data Science Project - 3 _ Feature Engineering (Real Estate Price Prediction).wav": " In last video, we went through some data cleaning techniques. In this video, we will be looking at some feature engineering and dimensional reduction techniques. At this stage, my data frame looks something like this. Now, I'm going to copy the data frame into a new data frame. Let's call it DF5. And to do a deep copy, you can use copy function on a data frame. And I'm going to create now Price Per square fit column. Here, I have total square fit and the total price. We all know that in real estate market, the price per square fit is very important. And this feature will help us do some outlier cleaning in the later stage. So I am doing some feature engineering here and creating a new feature which can be helpful for outlier detection and removal in the later stage. So DF5, let's call this new column, Price Per square fit. And this column is nothing but a division of two columns. So DF5 price. And this price you are dividing it with your square foot area. Now, our price is in lakh rupees. So one lakh is, OK, so this is how it looks like. So one and then for five zeros actually. And then DF.head. Excellent. So now I have Price Per square fit column here. So let's keep this column. We'll look into this column later. But meanwhile, we'll now explore location column. I have these locations. I want to check how many locations are there. And how many rows are available in my data set for location? Location is a categorical feature here because it's just a category. It's a tax data. And if you have too many locations, it can create a problem. So let's first check how many locations I have in total. So if I do DF5.location.unique, it will show me the unique number of location. And if I do the count, you will find that I have 1,300 locations, which is a big number. Usually to handle the tax data, we convert it into dummy columns using one hot encoding. And if we keep all the locations, what's going to happen is we will have around 1,300 columns in our data frame, which is just too much. It's like too many feature. This is called a dimensionality curse. Or this is a high dimensionality problem. And there are techniques available to reduce the dimensions. One of the very effective techniques is to come up with this other category. Other category means when you have 1,304 locations, you will find that there will be many locations, which will have only one or two data points. So how do you find that? So let's do something to figure out how many data points are available for location. And before I do that, I want to strip any extra spaces from the location so that my data is clean. And you can do that by using this lambda function. So this will just remove the leading space or if there is space at the end of the location. Now I will create a location stats variable, which will give me the statistics on location. And I will say df.location or rather, let's say group by. So I want to group by my data frame by using location. And whatever I get as a result on that, give me a location column on which I will perform aggregation function, which will give me the count per location. I want to print it. OK, you can see that this one giri nagar location has only one data row. First phase jp nagar has less than 25 rows. So this is good. But I want to sort these guys by number of data points. And to do that, you can call sort values function where you want ascending to be false. So now you can see that the wide field location has a maximum number of rows in my CSV file, 535 data points. And you see many location, which has only one data point. So we can come up with some threshold and say that any location, which has less than, let's say, 10 data points is called other location. So let's just do that. So what I will do is I want to know how many locations have less than 10 data points. And since location stats is a series, you can apply this kind of condition. And it will tell you that there are 1,052 locations out of 1293, which has less than 10 data points. So this is pretty good. Now I can use this to qualify a location as other location. I'm just going to call that this. And let me see which locations have these less than 10 data points. So all these locations, you see. So I can put all of these locations into a general category called the other. And before I do that, I will just print total number of unique location in my data frames, my DF file here. You can see that I'm still at DF file. It has 1293 locations as unique locations. And now I'm going to apply a transformation into this data frame and call it dfi.location is equal to dfi.location.apply. Again, I am going to apply some lambda function here. And I will say that this location is other if x is in location stats, location stats less than 10. Else, whatever is the value you keep, after this transformation is done, what will happen is all these locations will be converted to other. And then when you print a unique location value, you will find that now I have only 242 locations. This is pretty good because when I later on convert this into one hot encoding, I will only have 242 columns. Let me just print some of the values from data frame. And you can see that this particular location is now converted to other. Alright, so that's all I had for this video. In the next video, we are going to look at outlier detection and outlier removal.",
    "Machine Learning Tutorial Python - 10  Support Vector Machine (SVM).wav": " Support vector machine is a very popular classification algorithm and that's what we're going to cover today We'll start with some theory first and then we will solve a classification problem for iris flowers using SVM In the end we'll have an interesting exercise for you to solve So it's gonna be a lot of fun today So please stay till the end in the picture You are seeing iris flower which has four features battle width and height Sepple width and height based on these four features you can determine the species of the iris flower There are like three different species this data set is available in escalon our data set module So you can easily import it on this cattle plot. I have two features Pattle length and petal width just to make things simple and Based on that you can determine whether the species is setosa or wassey color now When you draw a classification boundary to separate these two groups You will notice that there are many possible ways of drawing this boundary all these three are valid boundaries So how do you decide which boundary is the best for my classification problem? one way of looking at it is You can Take nearby data points and you can measure the distance from that line to the data point So here you can see the distance is smaller Here the distance is higher So this distance is called margin So which line is better the one with a lower margin or the one with a higher margin? If you think carefully You will realize that the line with a higher margin is better because it Classifies these two groups in a better way for example here if you have a data point in between these two lines Then this line will probably misclassified versus this line will classify it better And that's what support vector machine tries to do so it will try to maximize this margin here Between the nearby data points and the line itself and these nearby data points are called support vectors Hence the name support vector machine So in case of a 2D space where you have two features the boundary is a line in case of 3D the boundary is a plane What will it be if you have n dimensional space? Usually you have n number of features, right? so just Post this video for a moment try to visualize what that boundary will look like and you will realize that It's kind of impossible to visualize it but Theoretically or mathematically still possible and that boundary is called a hyperplane so hyperplane is a plane in an dimensional and Dimension that tries to separate out different classification groups and that's what support vector machine algorithms tries to do So we need to familiarize ourselves with certain technical terms such as gamma and regular Regularization and we'll go over these terms so on this graph you can see that This decision boundary is Only considering the data points which are very near to it Okay, so this is one way of drawing a decision boundary you can see that I have excluded these data points and these far away data points in Making my decision for the decision boundary The other approach of looking at same problem could be that you consider the far away data points as well So on the left-hand side I have a high gamma and right-hand side I have a low gamma and both the approaches are valid It's just that with low gamma sometimes you might get you might get Acquired a problem with accuracy, but that might be okay. You know, it might be computationally more efficient So both the approaches are right it depends on your individual situation The other example here is This is a separate data set where I try to draw my boundary very carefully To avoid any classification error you can see that this is almost overfitting the model So if you have a very complex data set this line might be might be very zigzag and wiggly and it might Try to overfit the model on the other hand I can take some errors. So here you can see this there is this classification error which might be okay And my line might look more smoother. So on the left-hand side what I have is a higher regularization On the right-hand side I have a low regularization with low regularization. You might get some error, but that might be okay That might be even better for your model When we'll use S-K-Learn library to train our model you'll see a parameter called C and C means regularization basically You might have a complex data set like this. So what do you do like here? It's not very easy to draw the boundary One approach is create a third dimension. So I have x and y here so what if you create Z dimension and the way you do it by doing x square plus y square. So you are doing some transformation on your basic features and creating this new feature here and With that you will be able to draw this this is in boundary So the y plane is perpendicular to your monitor right now. So that's why you are not able to see it Okay, so just try to visualize the situation. It's not very hard And once you have the boundary you can superimpose it on your y plane y and x the plane formed by x and y axis and you will get A boundary like this. So the Z here is called a kernel by kernel. What we mean is we are creating a transformation on your existing features So that we can draw the decision boundary easily As usual, I have my Jupyter Notebook open here and I have imported Iris data set from escalon library and when you look at the Iris data set the basic features you will see it has these values right and When I do feature names I'll see the four features that I showed you in our first picture All right, so what I'm going to do is create a data frame out of this because It's easy to visualize this data set with a data frame. So I will do something like DF is equal to pd.data frame And my data is in this iris.data And my columns is actually the feature names all right And when you do df.head You will see that I have nice data frame like this ready I also have target variable in this target parameter. So I will append One more column called target in my data frame So So now I have my target all right, so the target be the possible values of target is 0 1 and 2 So what does 0 1 2 means? So for that if you do Iris dot target names So 0 index means set us one means varsicle or three means varsginica So we have three type of iris flowers and the type is determined by these four features sample width and height and length the pattern width and height, okay? All right So Now I want to I want to do some exploration and see how many Of my data points have one in it. So what I'm going to do is I will say df target equal to 1 I want to see how many rows in my data frame has All right, I think I had a spelling mistake So you can see from row number 50 onwards my target value is one which means it is vorsicle or similarly if I do two You will notice that it starts from 100 and my total data points is 150 So 0 to 50 is set osa 50 to 100 is vorsicle and 100 to 150 is varsginica Let me add one more column call flower name so that is clear So flower name is equal to df dot target so from One column you're trying to generate another column and the way you do that in pandas is by using Apply function And here lambda is just a small function Or a transformation that you can apply on target column and you can generate this new column call flower name Okay, so iris dot Target names maybe So for each value in my target column It will so that each value is x And it will return the index of the the value in target names. So for example if it is two So in this array 0 1 to is varsginica so varsginica will be placed in flower name column And if you want to see it You can see that now I have a new column call flower name if you export this to csv file It will be even more clear on How this data set looks so you can visualize it better basically now Let's do some data visualization too and you know that to in order to do that you have to Use mat plot lib you can use some other libraries such as bokeh or There are like couple of libraries available for visualization right so from mat plot lib import plot splt And this is in line magic. This is a concept specific to Jupyter notebook, right now let's First create three data frames. Well, I want to separate these three species into three separate data frames. Okay So how do you do that? df dot target is equal to zero is first data frame The second one is this The third one is this so now I have a three Different data frame, right? And if you want to look at those data frames you realize that the first one is setter sir Second one is Varsicle the third one is Virginia now. Let's draw a scatter plot So you can do plt dot scatter and specify your x and y here, okay? So what is your x and y? So I'm going to plot the first two flowers so df zero Uh And it's a 2d plot so I will only use these two features Okay, so let's do this You You can specify color And marker Okay, so let's say my marker is this So my scatter plot looks like this, okay now I will do the same thing Actually, I made a mistake. I should have df Zero actually so df zero Is a sample width and df1 And let's say this color is blue So you can see that There is a clear classification. So if I use my SVM algorithm here it will probably draw About really like this, all right? You can also just to make this plot clear you can specify x label and y label also so what is my x label? x label is sample length x label And y label is my width You know when you're looking at the chart now it's very clear all right Let's plot Path of length and battle width also, okay? So I'll just copy paste this guy here All you need to do is replace apple with battle And this is just plotting two more features And you can see that this has even clear distinction So looks like our SVM is going to perform really well because it will be able to easily draw Very nice and clear boundary between these data points now here. I have plotted only two features each on on these scatter plots Actually when we train our algorithm we are going to use all four features and The classification will also be between all three species now. Let's train our model using ascalon Right, so the first step is as usual Use train display to split your data set into training and test data set remember you don't want to Taste your model on the same data set that you have performed training on because it might be biased, okay? So Let us do this Our df data frame has target column also so I want to remove those first all right and the way you remove them is you want to say I want to drop certain columns from my data frame And which are the which are the columns that you want to drop, okay? So let's look at our data frame Our data frame has all these okay out of this first four columns are your features that you want to use for Training your model second to columns are your target columns, okay? So I want to drop target and flower name You Perfect and my why Will be just df dot target Okay, so if you look at why it's gonna be your usual numbers between 0 1 and 2 Okay So now let's use train this split X and Y Your taste size let's say I want to use 20% of my samples as a test and 80% as a training All right, so X train the output of this train this split method is X train X Test Y train Y Test And if you look at length Of your X train and X test You will notice that this is thought 20% of your 150 samples, right? So it looks good now now let's From sqlarn Dot Svm import svc so the classifier is svc basically and This is how you create svm classifier And as usual you can call fit method to train your Model okay, I'm going to use X train Y train So my model is train you will notice some parameters here such as c if you remember from the presentation earlier This is a regularization parameter You have gamma also and you have kernel Okay All right now, let's look at the accuracy of our model You can Call score method And now this time I'm going to use X-taste and Y-taste So my model is train and I am Performing the accuracy of this model by supplying this X-taste and Y-taste What the score method is going to do is it will take X-taste it will predict the values So it will be called like Y predicted and it will compare Y predicted with Y-taste to measure how accurate the model is And you can see that it is 0.96 If you execute this again your X train and X-taste changes, you know the sample changes So I want to not train it on a different set of samples So again, it's like 0.96 you can Now use the separate parameters for example here by default c is 1.0 So what if I modify the c parameter Let's say you do c is equal to 10 And on the same data set if you train it See increasing regularization is actually decreasing my score. So You can use this parameters to tune your model and you can do this on like a Couple of data sets using cross validation techniques and you can figure out what parameters are Based suited for your given problem right you can use even gamma also So gamma is Let's see if I use gamma is equal to 1 what happens Okay, I get the same score gamma is equal to 10 Again the same score. So looks like this is not making much difference Oh, so gamma 100 is going to make the model performance worse all right You can also use kernel So by default the kernel was rbf Let's use linear Now how do you know which kernel to pass here right so if you Use shift tab it will show the help from the escalon api And you can say these are the possible kernels I have available to use right So let's do linear And with linear also you are getting a very high accuracy score We have realized that it never goes beyond point 96 so point 96 looks like is the Optimum score that you can achieve right let's move on to the most interesting part which is the exercise remember Learning programming is more about practicing it. It's short of like taking swimming lessons right if you see someone Giving instructions for swimming on video you're not probably going to learn it You have to jump into the pool all right So open your Jupyter notebook and let's start doing this exercise for this you are going to use SK learns Digits data set so the digits data set looks like this. It's like hand region Digits and you have to classify it into one of the numbers from 0 to 8 right and you're going to use different kernels different Regularization and gamma parameters to tune the model and you have to tell me Which parameters gives you the best accuracy for this classification? Also use 80% of the samples for training If you look at the video description below I have a Jupyter notebook used in this tutorial available on my GitHub repository Look at that Jupyter notebook Go towards the end and you will find the exercise section So I have mentioned all the criteria for the exercise So do this and post your answers in the comment section below Uh, one last thing is I didn't cover the mathematics behind This SVM model and different kernels because it is gonna make it will make probably this tutorial very long So that's something that we will uh cover in some of the future videos all right. Thank you very much for watching. Bye",
    "Data Science & Machine Learning Project - Part 4 Feature Engineering _ Image Classification.wav": " In the last video we looked at data cleaning where we created this crop folder which has faces crop from all the images. In this video we are going to talk about feature engineering. A technique called wavelet transform that can be used to extract the facial features such as eyes, nose and lips etc. At the end of this tutorial you will have X and Y data sets ready which you can use to train the model. In the last tutorial we wrote the code till this point where we created this crop folder which had all the cropped images. So just to show you once again see Eleonal Messies face similarly Mario Sarapua only faces and we also manually deleted bad images. So our images are now all clean. Now we are going to do a wavelet transformation on these images. Now wavelet transformation allows you to extract the important features from your image. There are other feature extraction techniques as well but if you read image processing literature wavelet transforms are often the most effective way of extracting features. Now I'm going to show you some functions. So this is the wavelet transformation where you input an image and it will perform the wavelet transformation on top of it using pywt pi wavelet transform library and it will return your new image that is your wavelet transform. So the way it's going to look like is this. So this is the original image. So let me show you the original image which is this. Mario Sarapua's image. This is a colorful image. When I do wavelet transformation it looks something like this. Now this for computer is a very important image because it shows a lot of features. You can see the area of eye is differentiated from the area of the forehead from the nose is visible etc. So this almost looks like a black and white image and those important features such as nose, lips, eyes are extracted using white and black color. See for a computer this kind of detail is very very important for doing face detection because when you have image like this which is colorful it can have variety of shades and variety of colors and it becomes really difficult task for a classifier to identify such an image. Now when I through in this function you might be thinking oh you are just showing a lot of complex code and we don't know what's going on. Look I understand that there is some math involved there is some signal processing involved and for that reason if you want to dig deeper you need to pause this video you need to get your concepts on signal processing clear you need to know what is frequency domain what is time domain what is Fourier transform for this I'm going to point you some resources. The first resource is Emons YouTube channel. Emons is a good friend of mine. On my request he created this tutorial where he explains how you can represent image as a frequency. When you talk about any signal let's say audio signal for example image can also be considered as a signal. It can be presented in two type of domain right so image can be presented in a special domain meaning like space you know X and Y or it can be represented as a frequency domain. If you're talking about audio signal it can be represented in a time domain or a frequency domain. So you can go through Emons channel he has some awesome tutorials he explains things in a very very simple way. So try that. Here is another tutorial what is signal processing I'm going to provide all this links in the video description below. Another thing you need to have good understanding on is Fourier transform. For that 3 blue 1 brown channel has an excellent tutorial. So if you watch this you will understand how Fourier transform works. Just to give you an idea Fourier transform will take a complex signal and it will return you the basic signals which makes that complex signal. Just to give you an idea let's say you are eating some dish let's say you are eating pizza and if you do reverse engineering on pizza what you find is a basic ingredients which is like cheese the the base which is made up of all purpose flour the vegetables whatever right. So similarly when let's say you have a complex signal let's say you have a signal where you have different instruments playing in and then you have also noise you know in we have many noise cancellation devices so how do they actually cancel the noise so that is something that you need to think about and that they do using Fourier transform because Fourier transform can separate out the voice off let's say your drums voice the the voice off the vocal voice the noise it can separate out all these signals into different frequencies and using the frequency filters you can suppress some frequency or you can amplify certain frequencies you might have seen in audio audio devices where you can increase like a treble or jazz you know all of this is possible because of Fourier transform and wavelength transform is kind of similar to Fourier transform it is little different I'm not going to go too much into detail because that's a huge subject on signal processing I might have to spend like six months you know just on that topic and for that reason we have Emons amazing channel and you can refer to his tutorials okay so let's come back so just assume if you don't want to just worry too much about this just assume that by creating this black and white image you are extracting the important facial features which is going to be very useful to our classifier because when you think about Maria Sarapua's face how to detect her face see has eyes at a certain location on her face her eyes size is different than other players so when you detect if this is a Maria Sarapua or this is Virat Kohli you use these features like how small or big the eyes are whether the person has a moustache or beard what is the size of the lips the the structure of the face and all of those fine details are given by this wavelength transform image to us okay so this I showed it for one image what we are going to do is now we are going to create this wavelength transform image for all the images in our crop folder now when we create these images we will vertically stack these two images so what we'll do is so just to give you an idea so we'll take this image Maria Sarapua's image okay so let me just visually kind of show you because that might become useful so let's say I take this image here okay and I use for example I'm a spaint in MSPaint I copy this image okay so I have this raw image now raw image also has important features I'm not saying it doesn't therefore we are using raw and the wavelength transform image both so this is the raw image and I have this wavelength transform image and just think that I am vertically stacking them so this will be the input of our classifier because raw image can give certain information to your classifier at the same time wavelength transform image will have a lot of meaningful important features and for that reason the next piece of code that I'm writing what I'm doing is I'm vertically stacking these two images and I'm going to do that for all the images in our crop folder okay so how do we do that so we saw that initially we created this particular dictionary so let's see what is this dictionary this dictionary is very important actually so this dictionary has a key which is a player name and the values are the crop images okay so similarly you will see Maria Sarapua and all the crop images of Maria Sarapua now I'm going to iterate through this dictionary how do I iterate through the dictionary it's a simple Python thing you can use dot items function so see if I do this I'm iterating to that and for each training files so training files is nothing but this list okay this list of images so I'm now going to go through all the training files okay so the first loop will iterate through my sports person second loop will iterate through every image for that particular sports person all right so what do I do now is I first read an image because training image will be the path of the image and here I'm reading the image using open CV all right now I need to do scaling because the images could be of different size and we need to have same size when we are training our classifier so I'm just resizing it using open CV and I get now scaled a raw image okay so just just think that this is this scale image okay at the top 32 by okay let me put it here so this is that first image 32 by 32 that is my scale image now the second thing I need is wavelet transform image but that has to be in a same dimension as well okay so all right so I'm this is how I get my wavelet transform image W2D is the function this is doing wavelet transformation I got this function from stack all flow I'm going to provide the credit thanks stack all flow you know like stack all flow we use all the time because we don't want to reinvent the wheel and we can just get ready made functions okay once I get this image I need to scale that image as well and now I scale this image call it scale image hard okay both are 32 by 32 how do you vertically stack them well numpy has a function to vertically stack them so here I'm saying numpy vertically stack those images my scale image and my scale wavelet transform image okay my scale image has 32 by 32 into 3 because it's a colorful image so RGB 3 channels this by 3 the wavelet transform image doesn't have colors it's like a gray image so I'm gonna combine this into combine image let's call it combine image fine I'm fine with that and now I need to create x and y's guys we are reaching an interesting stage because now we are creating x and y's which means we are kind of ready for the model training so in the x which is a python list simple python list nothing fancy I'm going to put my combine image alright what is y everyone so y is a name of the celebrity now I cannot put celebrity name here because x and y has to be number so somehow I need to generate a number some number for each of my celebrities so let's do that how do I do that okay so I'm gonna go through all my celebrities and generate a class did something like this so go through all celebrities and start with count just assign them some number you know just random it doesn't matter which number you assign you need just some some class number and once you have this dictionary now this dictionary can be used to a return a number for a celebrity name so now I'm getting a number for each of these celebrities I'm going to execute this so this error apparently it's happening because we had all these images in this dictionary but we manually deleted some images so some of the images are not available so one thing we can do is we can again iterate through all the folders and get the new images or we can just say if image so if image so if the image is not present the cv2 I am read will return a none so I just added if image is none then you continue and when I executed it I have my x and y ready and let's see what is x and y really so if you look at x shape okay x is a list so it doesn't have a shape really but if you look at the length of the x it is 162 if you go and count the images in your folder like all the images in this five folder it will be 162 so each element in x is an image and the size of each image is 4096 so 4096 is what so you see here 32 into 32 into 3 so that is our raw pixel like raw image the x and y is 30 to 32 3 is for RGB and then for wavelet transform image another 32 into 32 so you get 4096 and if you want to just look at like the first image for curiosity it's nothing but the you know it's some number which is representing the color or like the the shade of image because machine learning model can only understand numbers so that's why we are doing this and I'm gonna do one more thing to x I will convert things to float okay and I'm just reshaping it just to make sure the shape is 162 by 4096 okay and now these images are converted are now represented as a float number you see like dot after that and that float number later on when we train the model you know we might get some errors that's why I have converted that as a float if it is indeed your it will still work but the skilern APIs will give you lot of warnings so just to avoid that I'm converting it to float all right so that's I think all we had for this video now we have x and y ready to begin our model training in the next video we are going to do model training and then in the video after that we will do fine tuning or hyper parameter tuning using grid sir cv and we will select the best model of for our image classification thank you bye",
    "Machine Learning Tutorial Python - 9  Decision Tree.wav": " We are going to solve a classification problem using decision tree algorithm today. When you have a dataset like this, it's easier to draw a decision boundary using logistic regression. But if your dataset is a little complex like this, you cannot just draw a single line. You might have to split your dataset again and again to come up with the decision boundaries. And this is what decision tree algorithm does for you. We will use this particular dataset where you try to predict if person's salary is more than $100,000 based on the company, his job title and the degree that he has. Now when you look at the dataset and when you give it to any human being to solve this problem, you will naturally try to build a decision tree in your brain. So first you will split the dataset using the company. And here you can see what happened is if your company's Facebook, no matter what your degree or job title is, your answer is always yes. You are always getting $100,000 per annum. I mean, there are a lot of money right now and that stock is going up, revenue is going up, so they don't mind paying such a high salary. But in other two cases, you have mixed samples. So you need to ask further question. For example, for Google, I will ask, what is the job position? And based on that, I have further conclusions such as if it's a business manager, the answer is always yes, sales executive answer is no. Computer programmer, again, I need to split my decision tree. And you can do this iteratively to come up with a tree like this. Now this sounded very simple, but in real life, you will not have three attributes. You will have probably 50 attributes and it matters in which order you split the three. Right now we chose company first, then job title and then the degree. In which order you select these attributes is going to impact the performance of your algorithm. So the question arises, how do you exactly select the ordering of these features? So let's look at our example. So here we used company first, we might have used the degree instead of company, in which case our data set would be split like this. Now observe carefully. On the left hand side, what's happening is we are getting a little bit of a pure subset. What I mean by pure subset is in the case of Facebook, all the samples are green. Okay, so this has a very low entropy. Now if you remember the definition of entropy from your school days, it is basically the measure of randomness in your sample. Here there is no randomness, everything is green. So six green samples zero red hands low entropy. Here there is some entropy, but still majority of the samples are red. Okay, where on the right hand side, for this case, four red four green means there is total random randomness, it is 50, 50 hands, my entropy is one. Okay, here is little better entropy is little low. So overall I'm thinking if I use company as shown on the left hand side, I will have a high information gain. Okay, whereas on the right hand side, I have low information gain. Hence you should use an approach which gives you high information at every split, hence which shows company as the first attribute. And in the further split also, you can use high information gain criteria to divide it further. There is another term that you hear often when you're dealing with decision tree, which is genie impurity. Now this is nothing but an impurity in your data set. For example, when I split my sample like this at the bottom, most of the samples are red, whereas one is green. So this is almost pure, but there is little bit of impurity. It's sort of similar to entropy. I'm not going to go into mathematics too much. You can read articles on it. I will straight away jump into writing code. I launched my Jupyter Notebook and loaded the same data set into my data frame. You can see I have the same CSV file that I'm loading into my data frame. Now the first step is once I have my data frame ready, I want to divide it in between the target variable and the independent variable. So I will call the target variable, independent variable, data frame inputs. And I will just drop. See, this is my target column, okay target variable. So I'm just going to drop that and I will say x is equal to columns. Okay, so once I execute this, what's happening is my input looks like this. So it doesn't have the last column, which is my answer and my target looks like this, which is my last column. Now by this point, you all know that machine learning algorithms can only work on numbers. They cannot understand labels. So what we have to do is we have to convert these particular columns, these three columns into numbers. And one of the ways is to use the label encoder. So from sqln.pre processing, I will... All right, I hit tab so it was autocompleting, where it was slow. But see if you tab again, it's not working. This is some... See, not work. It's funny. All right, once I import label encoder, I am going to create the object of this class. And I have three columns. So I have to create like three different objects, okay. So first is Ali company. The second one is Ali job. And then degree. Once you have these three, what you do is, in your inputs, data frame, you are creating one more column. And this is how you create extra column in a data frame. You call fit and transform method on your company column. And you can do the same thing for your job and degree column also. So here you have job, your degree. Once you do that and then when you print head, this is how your data frame going, is going to look like it has three extra column and we have label encoded your label columns into numbers. Next step is to drop those label columns. So I'm going to create new data frame here. And just say drop and you can drop multiple columns at the same time. X is equal to columns. And when you look at your inputs and data frame, what it did is drop all the label columns. Now all you have is numbers. So Google encoded as number two. The second one was ABC Farmer, which was encoded as zero. And Facebook is encoded as one. And same thing for like job title and degree, just assigns different numbers to different labels. Now we are ready to train our classifier. So as usual, I am going to import some module. For decision tree, you import the tree from your askilon library. And then your model is nothing but tree.deceasing tree. This is entry classifier. And then you can now train your model. So you can call fit here. And I'm going to call inputs and my target variable. So train my model. Now I'm not using test train split here just to keep things simple, but ideally you should split your data set into training set and test it. 80, 20, 70, 30, whatever ratio you prefer. All right, but I'm just keeping it very simple. Here it used criteria as genie impurity by default. You can change it to entropy also. Again, for math, I'm not going to go into very much detail. You can google it to know the difference between genie and entropy. These details are abstracted by askilon library. So you're fine. Although knowing math always helps in terms of what kind of criteria you should choose for a given problem. So I still suggest going through that. All right. Now my model is ready to predict. So the first thing I'm going to do is predict my score. All right. And the way you predict your score is you supply your input and target data set. Now post this video for a moment and tell me what is going to be your score. The score is going to be one because I'm using the same data set which I use for training. And my data set was also very simple. So I was expecting that it will do very okay. It will be very accurate with my prediction. Hence the score is one. In real life, when you have complex data set, your score will be less than one. Okay. So now let's do some prediction. So I'm going to do predict. All right. What are we going to predict? So let me predict the salary of person working in Google sells executives his job and master's degree. Okay. So let's see. So that's number two. Okay. So two to one. Two to one. All right. It's expecting 2D area. Usually you supply data for him. So I'm just going to do this. And it says zero means the person who is working in Google sells executive his job master degree. His salary is not going to be more than 100,000 or. And by the way, just a disclaimer, I just made made up this data set in reality Google sells executive might be getting much more than 100,000 dollar. But I just made it up. All right. So that's a little disclaimer. How about business manager? So business managers number label and code number is zero. So his salary is one. All right. So we are doing perfect. Ray. All right. Here you can have this model and you can do further prediction using the train model and by calling a predicate method on that already. Now the most important part, which is the exercise. So I expect all of you to work on the exercise once you learn this concept because just by watching the tutorial, you're not going to learn anything. So you must do an exercise on your own. I have a Titanic data set. So this is showing the survival rate of passenger in a Titanic crash. This is the real data set. And you can get this CSV file by clicking on a link in the video description below. So that link contains the Jupyter Notebook that was used in this tutorial. And it has exercise subfolder within that you have Titanic dot CSV. Here you should ignore all the red columns and use the remaining columns to predict the survival rate. So here the survived column is your target and you have to predict the survival of a passenger based on the class, the sex age, and the fare that he paid before onboarding Titanic ship. Okay. So that's what you have to do. Come up with the score of your model and post your score as a comment in below. And I will verify your answer and we'll see how well you can do with it. All right. That's all I had for this tutorial. Thank you very much for watching. Bye."
}